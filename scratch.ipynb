{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaithi/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.5.5.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "import weaviate\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import RedisCache\n",
    "import redis\n",
    "\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "\n",
    "redis_client = redis.Redis.from_url(REDIS_URL)\n",
    "set_llm_cache(RedisCache(redis_client))\n",
    "\n",
    "\n",
    "client = weaviate.Client(\n",
    "url=\"http://localhost:8080\",\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate(client, \n",
    "                    \"GRP\", \n",
    "                    \"content\")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"You're an Friendly AI assistant, your name is Claro, you can make normal conversations in a friendly manner, and also provide Answer the question based on the following context make sure it sounds like human and official assistant:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RAG\n",
    "model = ChatOllama(model=\"openhermes:7b-mistral-v2-q8_0\")\n",
    "# model = ChatOllama(model=\"falcon:40b-instruct-q4_1\")\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Add typing for input\n",
    "class Question(BaseModel):\n",
    "    __root__: str\n",
    "\n",
    "\n",
    "chain = chain.with_types(input_type=Question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull openhermes:7b-mistral-v2-q8_0`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat is mediwave\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4511\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4507\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4508\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:154\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    151\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    153\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    163\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:554\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    548\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    552\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    553\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:415\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    414\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    416\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    417\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    419\u001b[0m ]\n\u001b[1;32m    420\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:405\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 405\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m         )\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:624\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 624\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:161\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    160\u001b[0m     }\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/llms/ollama.py:246\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code 404. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe your model is not found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand you should pull the model with `ollama pull \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull openhermes:7b-mistral-v2-q8_0`."
     ]
    }
   ],
   "source": [
    "result = chain.invoke('what is mediwave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "\n",
    "result = food_crew(input='how to make vanilla sponge cake give me the receipe only')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "\n",
    "result = food_crew(input='how to make vanilla sponge cake, give me the receipe')\n",
    "\n",
    "print(food_crew.usage_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5 \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set_verbose(True)\n",
    "# set_debug(True)\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# members = [\"Food_crew\", \"General_conversation\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "\n",
    "members = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "     following workers:  {members}. Given the following user request,\"\n",
    "     respond with the worker to act next. \n",
    "     \n",
    "     if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew',\n",
    "    if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag',    \n",
    "    if the user makes conversation, jokes and funny conversations then use 'General_conv',\n",
    "    if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other',\n",
    "    if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew'\n",
    "        \n",
    "    Each worker will perform a\n",
    "     task and respond with their results and status. When finished,\n",
    "    respond with FINISH.\"\"\"\n",
    ")\n",
    "\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role to act\",\n",
    "    \n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",        \n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": f\"{options}\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": \"route\",\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = OllamaFunctions(\n",
    "    model=os.environ['LLM'],\n",
    "    tool_system_prompt_template=DEFAULT_SYSTEM_TEMPLATE\n",
    "    )\n",
    "\n",
    "\n",
    "def supervisor_node(state):\n",
    "\n",
    "    print(state)\n",
    "    \n",
    "    supervisor_chain = (\n",
    "        prompt\n",
    "        | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )\n",
    "        \n",
    "    result = supervisor_chain.invoke(state)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "\n",
    "from grp_travel_crew_ai.grp_travel_crewai import travel_crew\n",
    "\n",
    "from grp_RAG1.grp_rag1_rag import mediwave_rag\n",
    "\n",
    "from grp_others.grp_others_graph import grp_other_def as gen_others\n",
    "\n",
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "from grp_Gen_Conv.grp_gen_conv_chain import general_conversation\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"Food_crew\", food_crew)\n",
    "workflow.add_node(\"General_conv\", general_conversation)\n",
    "workflow.add_node(\"General_other\", gen_others)\n",
    "workflow.add_node(\"Mediwave_rag\", mediwave_rag)\n",
    "workflow.add_node(\"Travel_crew\", travel_crew)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for member in members:\n",
    "    \n",
    "    if member == 'Mediwave_rag':\n",
    "        continue\n",
    "    if member == 'Travel_crew':\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "    \n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "\n",
    "\n",
    "\n",
    "conditional_map[\"FINISH\"] = END\n",
    "# conditional_map['supervisor'] ='supervisor'\n",
    "\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "workflow.set_finish_point('Mediwave_rag')\n",
    "workflow.set_finish_point('General_conv')\n",
    "workflow.set_finish_point('Travel_crew')\n",
    "\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"what is the current weather in pondicherry\"\n",
    "            )\n",
    "        ],\n",
    "    })\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub \n",
    "\n",
    "hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in kodaikanal and ooty give me a complete 7 day itenary with travel route and food, budget accomodation and other nearby scenic spots and tourist atractions, start from pondicherry and return pondicherry after last day.\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in ooty and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The user's travel needs in Ooty during summer can be met by visiting several top attractions over a 3-day itinerary, starting and ending in Pondicherry. Here is the detailed plan:\\n\\nDay 1:\\n- Visit Dodabetta Peak, the highest point in Ooty, offering breathtaking views of the surrounding mountains and valleys.\\n- Explore Mudumalai Wildlife Sanctuary, a large protected area known for its diverse wildlife population, including elephants, tigers, and various bird species.\\n\\nDay 2:\\n- Spend the morning at the Botanical Gardens, home to a vast collection of exotic plants, flowers, and trees.\\n- In the afternoon, visit Emerald Lake, a serene and picturesque lake nestled in the heart of Ooty.\\n- End the day by relaxing at Ooty Lake, enjoying a peaceful boat ride or taking a leisurely walk around the lake.\\n\\nDay 3:\\n- Begin the day with a visit to Pykara Falls, located approximately 20 km from Ooty, known for its stunning beauty and the surrounding lush greenery.\\n- In the afternoon, head towards Coonoor, a nearby hill station famous for its tea gardens. Take a tour of the gardens and learn about the tea-making process.\\n\\nBy following this itinerary, the user will get to experience the best of Ooty's natural beauty, cultural attractions, and local experiences during their summer vacation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in mysore and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Here's a detailed response summarizing key findings about the given context and information that could be relevant to it: The user is looking for travel suggestions in Mysore during summer and requires a three-day itinerary starting from Pondicherry. Based on your request, I have delegated this task to our Travel agency manager. Here's the suggested itinerary:\\n\\nDay 1:\\n- Depart from Pondicherry early morning by road, which is approximately a 4-hour drive.\\n- Visit the Chamundeshwari Temple located on Chamundi Hill, known for its religious significance and beautiful views of Mysore city.\\n- Head to Brindavan Gardens, located in the Krishnarajasagar Dam area, famous for its terrace gardens, fountains, and musical fountain show.\\n- Overnight stay at a hotel in Mysore.\\n\\nDay 2:\\n- Visit the Mysore Palace, a grand architectural marvel open to public viewing during summer months. Don't forget to check out the famous Dussehra durbar hall.\\n- Explore the Sri Chamarajendra Zoo and Museum located in the heart of Mysore city. The zoo is home to a wide variety of animals, and the museum exhibits artifacts related to the history and culture of Mysore.\\n- Visit the St. Philomena's Church, an impressive Roman Catholic basilica known for its neo-Gothic architecture.\\n- Overnight stay at a hotel in Mysore.\\n\\nDay 3:\\n- Depart from Mysore early morning and head back to Pondicherry by road.\\n- En route, stop at the Ranganathittu Bird Sanctuary located near Srirangapatna, famous for its diverse bird population and scenic beauty.\\n- Arrive in Pondicherry late afternoon/evening and complete your journey.\\n\\nPlease note that travel times may vary depending on traffic conditions, so it's always a good idea to leave early. Additionally, make sure to check the opening hours of each attraction before planning your visit. Let me know if you need any further assistance with this itinerary or if there are any modifications you would like me to make.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in mysore and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"what is the time now\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervisor - update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set_verbose(True)\n",
    "# set_debug(True)\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# members = [\"Food_crew\", \"General_conversation\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "\n",
    "members = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "     following workers:  {members}. Given the following user request,\"\n",
    "     respond with the worker to act next. \n",
    "     \n",
    "     if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew',\n",
    "    if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag',    \n",
    "    if the user makes conversation, jokes and funny conversations then use 'General_conv',\n",
    "    if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other',\n",
    "    if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew'\n",
    "        \n",
    "    Each worker will perform a\n",
    "     task and respond with their results and status. When finished,\n",
    "    respond with FINISH.\"\"\"\n",
    ")\n",
    "\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role to act\",\n",
    "    \n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",        \n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": f\"{options}\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": \"route\",\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = OllamaFunctions(\n",
    "    model=os.environ['LLM'],\n",
    "    tool_system_prompt_template=DEFAULT_SYSTEM_TEMPLATE\n",
    "    )\n",
    "\n",
    "\n",
    "def supervisor_node(state):\n",
    "\n",
    "    print(state)\n",
    "    \n",
    "    supervisor_chain = (\n",
    "        prompt\n",
    "        | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )\n",
    "        \n",
    "    result = supervisor_chain.invoke(state)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "\n",
    "from grp_travel_crew_ai.grp_travel_crewai import travel_crew\n",
    "\n",
    "from grp_RAG1.grp_rag1_rag import mediwave_rag\n",
    "\n",
    "from grp_others.grp_others_graph import grp_other_def as gen_others\n",
    "\n",
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "from grp_Gen_Conv.grp_gen_conv_chain import general_conversation\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"Food_crew\", food_crew)\n",
    "workflow.add_node(\"General_conv\", general_conversation)\n",
    "workflow.add_node(\"General_other\", gen_others)\n",
    "workflow.add_node(\"Mediwave_rag\", mediwave_rag)\n",
    "workflow.add_node(\"Travel_crew\", travel_crew)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for member in members:\n",
    "    \n",
    "    if member == 'Mediwave_rag':\n",
    "        continue\n",
    "    if member == 'Travel_crew':\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "    \n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "\n",
    "\n",
    "\n",
    "conditional_map[\"FINISH\"] = END\n",
    "# conditional_map['supervisor'] ='supervisor'\n",
    "\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "workflow.set_finish_point('Mediwave_rag')\n",
    "workflow.set_finish_point('General_conv')\n",
    "workflow.set_finish_point('Travel_crew')\n",
    "\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plan_and_execute.graph import graph \n",
    "\n",
    "graph.get_input_schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plan_and_execute.graph import graph \n",
    "\n",
    "\n",
    "\n",
    "graph.get_input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': [{'key': 'Mediwave_rag', 'value': 'explain about Mediwave'}, {'key': 'General_other', 'value': 'get current time'}]}\n",
      "explain about Mediwave\n",
      "{'messages': [HumanMessage(content='explain about Mediwave')], 'agent_outcome': None, 'next': 'Mediwave_rag'}\n",
      "explain about Mediwave\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaithi/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.5.5.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='explain about Mediwave'), HumanMessage(content='explain about Mediwave')], 'next': 'Mediwave_rag', 'agent_outcome': AgentFinish(return_values={'output': \" Claro: Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They are dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. Their services include designing not only digital products and services but also transformative experiences, from conceptualization through development, testing, and deployment. The team at Mediwave consists of talented full-stack developers, QA, DevOps programmers, and UI/UX designers who collaborate to create visually captivating interfaces with intuitively designed user experiences. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. Mediwave Digital has been instrumental in helping companies deliver delightful customer experiences, as testified by their clients.\"}, log=\" Claro: Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They are dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. Their services include designing not only digital products and services but also transformative experiences, from conceptualization through development, testing, and deployment. The team at Mediwave consists of talented full-stack developers, QA, DevOps programmers, and UI/UX designers who collaborate to create visually captivating interfaces with intuitively designed user experiences. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. Mediwave Digital has been instrumental in helping companies deliver delightful customer experiences, as testified by their clients.\")}\n",
      "explain about Mediwave\n",
      "{'messages': [HumanMessage(content='explain about Mediwave')], 'agent_outcome': None, 'next': 'Mediwave_rag'}\n",
      "explain about Mediwave\n",
      "{'messages': [HumanMessage(content='explain about Mediwave'), HumanMessage(content='explain about Mediwave')], 'next': 'Mediwave_rag', 'agent_outcome': AgentFinish(return_values={'output': \" Claro: Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They are dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. Their services include designing not only digital products and services but also transformative experiences, from conceptualization through development, testing, and deployment. The team at Mediwave consists of talented full-stack developers, QA, DevOps programmers, and UI/UX designers who collaborate to create visually captivating interfaces with intuitively designed user experiences. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. Mediwave Digital has been instrumental in helping companies deliver delightful customer experiences, as testified by their clients.\"}, log=\" Claro: Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They are dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. Their services include designing not only digital products and services but also transformative experiences, from conceptualization through development, testing, and deployment. The team at Mediwave consists of talented full-stack developers, QA, DevOps programmers, and UI/UX designers who collaborate to create visually captivating interfaces with intuitively designed user experiences. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. Mediwave Digital has been instrumental in helping companies deliver delightful customer experiences, as testified by their clients.\")}\n",
      "explain about Mediwave\n",
      "{'messages': [HumanMessage(content='explain about Mediwave')], 'agent_outcome': None, 'next': 'Mediwave_rag'}\n",
      "explain about Mediwave\n",
      "{'messages': [HumanMessage(content='explain about Mediwave'), HumanMessage(content='explain about Mediwave')], 'next': 'Mediwave_rag', 'agent_outcome': AgentFinish(return_values={'output': \" Claro: Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They are dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. Their services include designing not only digital products and services but also transformative experiences, from conceptualization through development, testing, and deployment. The team at Mediwave consists of talented full-stack developers, QA, DevOps programmers, and UI/UX designers who collaborate to create visually captivating interfaces with intuitively designed user experiences. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. Mediwave Digital has been instrumental in helping companies deliver delightful customer experiences, as testified by their clients.\"}, log=\" Claro: Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They are dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. Their services include designing not only digital products and services but also transformative experiences, from conceptualization through development, testing, and deployment. The team at Mediwave consists of talented full-stack developers, QA, DevOps programmers, and UI/UX designers who collaborate to create visually captivating interfaces with intuitively designed user experiences. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. Mediwave Digital has been instrumental in helping companies deliver delightful customer experiences, as testified by their clients.\")}\n",
      "explain about Mediwave\n",
      "{'messages': [HumanMessage(content='explain about Mediwave')], 'agent_outcome': None, 'next': 'Mediwave_rag'}\n",
      "explain about Mediwave\n",
      "{'messages': [HumanMessage(content='explain about Mediwave'), HumanMessage(content='explain about Mediwave')], 'next': 'Mediwave_rag', 'agent_outcome': AgentFinish(return_values={'output': \" Claro: Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They are dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. Their services include designing not only digital products and services but also transformative experiences, from conceptualization through development, testing, and deployment. The team at Mediwave consists of talented full-stack developers, QA, DevOps programmers, and UI/UX designers who collaborate to create visually captivating interfaces with intuitively designed user experiences. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. Mediwave Digital has been instrumental in helping companies deliver delightful customer experiences, as testified by their clients.\"}, log=\" Claro: Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They are dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. Their services include designing not only digital products and services but also transformative experiences, from conceptualization through development, testing, and deployment. The team at Mediwave consists of talented full-stack developers, QA, DevOps programmers, and UI/UX designers who collaborate to create visually captivating interfaces with intuitively designed user experiences. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. Mediwave Digital has been instrumental in helping companies deliver delightful customer experiences, as testified by their clients.\")}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from plan_and_execute.graph import graph \n",
    "\n",
    "\n",
    "set_verbose=True \n",
    "set_debug=True\n",
    "\n",
    "\n",
    "\n",
    "@chain\n",
    "async def custom_chain(input):\n",
    "    \n",
    "    result = await graph.ainvoke({\"input\": input})\n",
    "    \n",
    "    # print(result)\n",
    "    \n",
    "    return AIMessage(content=result['response'])\n",
    "\n",
    "res = await custom_chain.ainvoke(input='tell me about mediwave and also give the current time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. The current time is...')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'asyncio' has no attribute 'coroutine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplan_and_execute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/packages/rag-weaviate/plan_and_execute/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplan_and_execute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph\n\u001b[1;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/packages/rag-weaviate/plan_and_execute/graph.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m load_dotenv()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# from retry import retry\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mretrying_async\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m retry\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ---------------------------- STEP EXECUTOR --------------------------------\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_step\u001b[39m(super_state: PlanExecute):\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/retrying_async.py:40\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_exception\u001b[39m(obj):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m     36\u001b[0m             (inspect\u001b[38;5;241m.\u001b[39misclass(obj) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28missubclass\u001b[39m(obj, \u001b[38;5;167;01mException\u001b[39;00m)))\n\u001b[1;32m     37\u001b[0m     )\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;129m@asyncio\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoroutine\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcallback\u001b[39m(attempt, exc, args, kwargs, delay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m*\u001b[39m, loop):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(attempt \u001b[38;5;241m*\u001b[39m delay, loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retry\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'asyncio' has no attribute 'coroutine'"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from plan_and_execute.graph import graph \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"tell me about mediwave and along with tell me the current time\"}\n",
    "async for event in graph.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Response',\n",
       " 'description': 'Response to user.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'response': {'type': 'string'}},\n",
       "  'required': ['response']}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "convert_to_openai_function(Response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open ai function runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.output_parsers import (\n",
    "    BaseGenerationOutputParser,\n",
    "    BaseOutputParser,\n",
    "    JsonOutputParser,\n",
    ")\n",
    "from langchain_core.output_parsers.openai_functions import (\n",
    "    JsonOutputFunctionsParser,\n",
    "    PydanticAttrOutputFunctionsParser,\n",
    "    PydanticOutputFunctionsParser,\n",
    ")\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "from langchain.output_parsers import (\n",
    "    JsonOutputKeyToolsParser,\n",
    "    PydanticOutputParser,\n",
    "    PydanticToolsParser,\n",
    ")\n",
    "\n",
    "\n",
    "def create_openai_fn_runnable(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    enforce_single_function_usage: bool = True,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "   \n",
    "    # noqa: E501\n",
    "    if not functions:\n",
    "        raise ValueError(\"Need to pass in at least one function. Received zero.\")\n",
    "    openai_functions = [convert_to_openai_function(f) for f in functions]\n",
    "    llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions}\n",
    "    if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "        llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "    output_parser = output_parser or get_openai_output_parser(functions)\n",
    "    if prompt:\n",
    "        return prompt | llm.bind(functions=openai_functions) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs_) | output_parser\n",
    "\n",
    "\n",
    "def get_openai_output_parser(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    \"\"\"Get the appropriate function output parser given the user functions.\n",
    "\n",
    "    Args:\n",
    "        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\n",
    "            or a Python function. If a dictionary is passed in, it is assumed to\n",
    "            already be a valid OpenAI function.\n",
    "\n",
    "    Returns:\n",
    "        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\n",
    "            a JsonOutputFunctionsParser. If there's only one function and it is\n",
    "            not a Pydantic class, then the output parser will automatically extract\n",
    "            only the function arguments and not the function name.\n",
    "    \"\"\"\n",
    "    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\n",
    "        if len(functions) > 1:\n",
    "            pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "        else:\n",
    "            pydantic_schema = functions[0]\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    else:\n",
    "        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\n",
    "    return output_parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'input': 'tell me about mediwave and also give the current time', 'plan': [{'key': 'Mediwave_rag', 'value': 'Explain about Mediwave'}, {'key': 'General_other', 'value': 'Get current time'}], 'past_steps': ('Explain about Mediwave', \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"), 'response': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about mediwave and also give the current time',\n",
       " 'plan': ['Explain about Mediwave', 'Get current time'],\n",
       " 'past_steps': ('Explain about Mediwave',\n",
       "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
       " 'response': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_ = state.copy()\n",
    "\n",
    "plan_steps = []\n",
    "\n",
    "# m = [val for key, val in item.items() for item in g]\n",
    "\n",
    "for item in state['plan']:\n",
    "    # print(item['value'])\n",
    "    plan_steps.append(item['value'])\n",
    "    # for val in item.values():\n",
    "    #     # k.append(val)\n",
    "    #     print(val)\n",
    "        \n",
    "plan_steps\n",
    "\n",
    "state_ |= {'plan': plan_steps}\n",
    "state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "from langchain_experimental.pydantic_v1 import root_validator\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": <name of the selected tool>,\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "DEFAULT_RESPONSE_FUNCTION = {\n",
    "    \"name\": \"__conversational_response\",\n",
    "    \"description\": (\n",
    "        \"Respond conversationally if no other tools should be called for a given query.\"\n",
    "    ),\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"response\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Conversational response to the user.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"response\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class OllamaFunctions(BaseChatModel):\n",
    "    \"\"\"Function chat model that uses Ollama API.\"\"\"\n",
    "\n",
    "    llm: ChatOllama\n",
    "\n",
    "    tool_system_prompt_template: str\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        values[\"llm\"] = values.get(\"llm\") or ChatOllama(**values, format=\"json\")\n",
    "        values[\"tool_system_prompt_template\"] = (\n",
    "            values.get(\"tool_system_prompt_template\") or DEFAULT_SYSTEM_TEMPLATE\n",
    "        )\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def model(self) -> BaseChatModel:\n",
    "        \"\"\"For backwards compatibility.\"\"\"\n",
    "        return self.llm\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        functions = kwargs.get(\"functions\", [])\n",
    "        if \"function_call\" in kwargs:\n",
    "            functions = [\n",
    "                fn for fn in functions if fn[\"name\"] == kwargs[\"function_call\"][\"name\"]\n",
    "            ]\n",
    "            if not functions:\n",
    "                raise ValueError(\n",
    "                    'If \"function_call\" is specified, you must also pass a matching \\\n",
    "function in \"functions\".'\n",
    "                )\n",
    "            del kwargs[\"function_call\"]\n",
    "        elif not functions:\n",
    "            functions.append(DEFAULT_RESPONSE_FUNCTION)\n",
    "        system_message_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "            self.tool_system_prompt_template\n",
    "        )\n",
    "        system_message = system_message_prompt_template.format(\n",
    "            tools=json.dumps(functions, indent=2)\n",
    "        )\n",
    "        if \"functions\" in kwargs:\n",
    "            del kwargs[\"functions\"]\n",
    "        response_message = self.llm.predict_messages(\n",
    "            [system_message] + messages, stop=stop, callbacks=run_manager, **kwargs\n",
    "        )\n",
    "        chat_generation_content = response_message.content\n",
    "        if not isinstance(chat_generation_content, str):\n",
    "            raise ValueError(\"OllamaFunctions does not support non-string output.\")\n",
    "        try:\n",
    "            parsed_chat_result = json.loads(chat_generation_content)\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\n",
    "                f'\"{self.llm.model}\" did not respond with valid JSON. Please try again.'\n",
    "            )\n",
    "        called_tool_name = parsed_chat_result[\"tool\"]\n",
    "        called_tool_arguments = parsed_chat_result[\"tool_input\"]\n",
    "        called_tool = next(\n",
    "            (fn for fn in functions if fn[\"name\"] == called_tool_name), None\n",
    "        )\n",
    "        if called_tool is None:\n",
    "            raise ValueError(\n",
    "                f\"Failed to parse a function call from {self.llm.model} \\\n",
    "output: {chat_generation_content}\"\n",
    "            )\n",
    "        if called_tool[\"name\"] == DEFAULT_RESPONSE_FUNCTION[\"name\"]:\n",
    "            return ChatResult(\n",
    "                generations=[\n",
    "                    ChatGeneration(\n",
    "                        message=AIMessage(\n",
    "                            content=called_tool_arguments[\"response\"],\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        response_message_with_functions = AIMessage(\n",
    "            content=\"\",\n",
    "            additional_kwargs={\n",
    "                \"function_call\": {\n",
    "                    \"name\": called_tool_name,\n",
    "                    \"arguments\": json.dumps(called_tool_arguments)\n",
    "                    if called_tool_arguments\n",
    "                    else \"\",\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=response_message_with_functions)]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama_functions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.output_parsers import (BaseGenerationOutputParser, BaseOutputParser,)\n",
    "from langchain_core.output_parsers.openai_functions import (JsonOutputFunctionsParser, PydanticOutputFunctionsParser,)\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import (convert_to_openai_function)\n",
    "\n",
    "from langchain.output_parsers import (JsonOutputKeyToolsParser, PydanticOutputParser, PydanticToolsParser)\n",
    "\n",
    "\n",
    "def create_openai_fn_runnable(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    enforce_single_function_usage: bool = True,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "   \n",
    "    openai_functions = [convert_to_openai_function(f) for f in functions]\n",
    "    llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions, **llm_kwargs}\n",
    "    \n",
    "    if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "        llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "    output_parser = output_parser or get_openai_output_parser(functions)\n",
    "    if prompt:\n",
    "        return prompt | llm.bind(**llm_kwargs_) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs_) | output_parser\n",
    "\n",
    "\n",
    "def get_openai_output_parser(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    \"\"\"Get the appropriate function output parser given the user functions.\n",
    "\n",
    "    Args:\n",
    "        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\n",
    "            or a Python function. If a dictionary is passed in, it is assumed to\n",
    "            already be a valid OpenAI function.\n",
    "\n",
    "    Returns:\n",
    "        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\n",
    "            a JsonOutputFunctionsParser. If there's only one function and it is\n",
    "            not a Pydantic class, then the output parser will automatically extract\n",
    "            only the function arguments and not the function name.\n",
    "    \"\"\"\n",
    "    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\n",
    "        if len(functions) > 1:\n",
    "            pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "        else:\n",
    "            pydantic_schema = functions[0]\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    else:\n",
    "        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\n",
    "    return output_parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': Plan(steps=[Step(key='Mediwave_rag', value='Provide detailed information about Mediwave Digital, their focus areas, team composition, and notable achievements.')])}\n"
     ]
    }
   ],
   "source": [
    "state_ = {'input': 'tell me about mediwave',\n",
    " 'plan': ['Explain about Mediwave'],\n",
    " 'past_steps': ('Explain about Mediwave',\n",
    "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
    " 'response': None}\n",
    "\n",
    "from plan_and_execute.planner import crews\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing_extensions import List\n",
    "from typing import Literal\n",
    "\n",
    "class Step(BaseModel):\n",
    "    key: Literal[\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"] = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Step] \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from langchain.chains.openai_functions import create_openai_fn_runnable\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from plan_and_execute.planner import crews\n",
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "import os\n",
    "\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "\n",
    "\n",
    "set_verbose=True \n",
    "set_debug=True\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "response = {'name': 'Response',\n",
    " 'description': 'Response to user.',\n",
    " 'parameters': {'type': 'object',\n",
    "  'properties': {'response': {'type': 'string'}},\n",
    "  'required': ['response']}}\n",
    "\n",
    "\n",
    "function1 = {'name': 'plan',\n",
    " 'description': 'replanner',\n",
    " \n",
    " 'parameters': {\n",
    "   'type': 'array',\n",
    "   'properties': {\n",
    "     'key': {\n",
    "       \"enum\": f\"{crews}\",\n",
    "       'description': 'the worker gonna handle this task/step'\n",
    "       \n",
    ",\n",
    "        \n",
    "       'type': 'string'},\n",
    "     'value': {\n",
    "       'description': 'task/ step the worker need to do',\n",
    "      'type': 'string'}\n",
    "     },\n",
    "                    \n",
    "    'required': ['plan'],\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given user input, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    \n",
    "      if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' key,\n",
    "        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' key,    \n",
    "        if the user makes conversation, jokes and funny conversations then use 'General_conv' key,\n",
    "        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' key,\n",
    "        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' key.\n",
    "    \n",
    "    \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "user input was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly(remove the completed step). If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "replanner = create_openai_fn_runnable(\n",
    "    \n",
    "    [Plan, Response],\n",
    "    OllamaFunctions(model=os.environ['LLM']),\n",
    "    replanner_prompt,\n",
    ")\n",
    "\n",
    "# [function1, response],\n",
    "\n",
    "\n",
    "output = replanner.invoke(state_)\n",
    "\n",
    "\n",
    "if isinstance(output, Response):\n",
    "    print({\"response\": output})\n",
    "else:\n",
    "    print({\"plan\": output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step(key='Mediwave_rag', value='Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Plan(steps=[Step(key='Mediwave_rag', value='Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research'), Step(key='General_other', value='Determine and provide the current time')])\n",
    "\n",
    "g.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': 'Mediwave_rag',\n",
       "  'value': 'Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research'},\n",
       " {'key': 'General_other', 'value': 'Determine and provide the current time'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.dict()['steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromptTemplate(input_variables=['input', 'past_steps', 'plan'], template=\"For the given user input, come up with a simple step by step plan. This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.     \\n      if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' key,\\n        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' key,    \\n        if the user makes conversation, jokes and funny conversations then use 'General_conv' key,\\n        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' key,\\n        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' key.\\n    \\n    \\nThe result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\\n\\nuser input was this:\\n{input}\\n\\nYour original plan was this:\\n{plan}\\n\\nYou have currently done the follow steps:\\n{past_steps}\\n\\nUpdate your plan accordingly(remove the completed step). If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about mediwave and also give the current time',\n",
       " 'plan': ['Explain about Mediwave', 'Get current time'],\n",
       " 'past_steps': ('Explain about Mediwave',\n",
       "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
       " 'response': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'plan': [{'key': 'Mediwave_rag', 'value': 'Get information about Mediwave'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'name': 'Response', 'arguments': {'response': 'Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They have been instrumental in helping organizations achieve customer satisfaction throughout the customer journey. Some of their featured projects include Medichec, which helps identify medications that could potentially impact cognitive function or cause other adverse effects in older individuals, and Oxcare, a digital portal designed for supported self-management of physical health and mental wellbeing. Their tech stack includes various technologies such as Angular, Node.js, Mango DB, Cordova, Express.js, Apostrophe CMS, and PostgreSQL, among others. They take pride in collaborating with exceptional individuals and making a tangible impact with their work. For more information about their clients and testimonials, you can check out their website.'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from pydantic import BaseModel \n",
    "\n",
    "class Steps(BaseModel):\n",
    "    key: str = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Steps] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Plan': [{'key': 'Mediwave_rag',\n",
    "#    'value': 'Retrieve information about Mediwave'},\n",
    "#   {'key': 'General_other',\n",
    "#    'value': 'Format the retrieved information for user consumption'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the 2024 Australia open winner?\"}\n",
    "async for event in graph.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch - supervisor and agent planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "class PlanItem(BaseModel):\n",
    "    key: str = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    plan: List[PlanItem] = Field(description=\"different steps to follow, should be in sorted order, always make minimal steps\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "\n",
    "\n",
    "openai_function = convert_pydantic_to_openai_function(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crews = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = {'name': 'Plan',\n",
    " 'description': '',\n",
    " \n",
    " 'parameters': {\n",
    "   'type': 'array',\n",
    "   'properties': {\n",
    "     'key': {\n",
    "       \"enum\": f\"{crews}\",\n",
    "       'description': 'the worker gonna handle this task/step',\n",
    "       'type': 'string'},\n",
    "     'value': {\n",
    "       'description': 'task/ step the worker need to do',\n",
    "      'type': 'string'}\n",
    "     },\n",
    "                    \n",
    "    'required': ['plan'],\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict\n",
    "# from pydantic import BaseModel\n",
    "\n",
    "# class Parameter(BaseModel):\n",
    "#     key: str\n",
    "#     value: str\n",
    "\n",
    "# class FunctionParameter(BaseModel):\n",
    "#     type: str\n",
    "#     properties: Dict[str, Parameter]\n",
    "#     required: List[str]\n",
    "\n",
    "# class FunctionSchema(BaseModel):\n",
    "#     name: str\n",
    "#     description: str\n",
    "#     parameters: FunctionParameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict, Optional\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "# class Plan(BaseModel):\n",
    "#     name: str\n",
    "#     description: Optional[str] = Field(default='', description='Description of the plan')\n",
    "#     parameters: List[Dict[str, str]]\n",
    "\n",
    "#     class Config:\n",
    "#         json_schema_extra = {\n",
    "#             \"example\": {\n",
    "#                 \"name\": \"Plan\",\n",
    "#                 \"description\": \"Description of the plan\",\n",
    "#                 \"parameters\": [\n",
    "#                     {\n",
    "#                         \"key\": \"worker1\",\n",
    "#                         \"value\": \"Do task A\"\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"key\": \"worker2\",\n",
    "#                         \"value\": \"Do task B\"\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "Plan.schema_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[ Dict[\n",
    "                key: str = Field(description='the worker gonna handle this task/step')\n",
    "                value: str = Field(description='task/ step the worker need to do')\n",
    "    ]\n",
    "                ] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "Plan.schema_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plan.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "    \n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given user input, come up with a simple step by step plan but don't provide answer coz you have tools to figure out things. \\\n",
    "        \n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    \n",
    "    if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' worker,\n",
    "        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' worker,    \n",
    "        if the user makes conversation, jokes and funny conversations then use 'General_conv' worker,\n",
    "        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' worker,\n",
    "        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' worker.\n",
    "    \n",
    "    \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "if the given objective related to mediwave then give the objective as plan\n",
    "\n",
    "user input : {objective}\"\"\"\n",
    ")\n",
    "\n",
    "planner = create_structured_output_runnable(\n",
    "    function, \n",
    "    OllamaFunctions(model=os.environ['LLM']),\n",
    "    planner_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_step(input):\n",
    "    plan = planner.invoke({\"objective\": input})\n",
    "    # return {\"plan\": plan.steps}\n",
    "    h = {\"plan\": plan}\n",
    "    print(h)\n",
    "\n",
    "    return {\"plan\": plan['plan']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_step(input=\"Tell me about mediwave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner.invoke(\"tell me about mediwave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
