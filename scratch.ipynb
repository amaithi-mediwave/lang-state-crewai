{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaithi/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.5.5.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "import weaviate\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import RedisCache\n",
    "import redis\n",
    "\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "\n",
    "redis_client = redis.Redis.from_url(REDIS_URL)\n",
    "set_llm_cache(RedisCache(redis_client))\n",
    "\n",
    "\n",
    "client = weaviate.Client(\n",
    "url=\"http://localhost:8080\",\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate(client, \n",
    "                    \"GRP\", \n",
    "                    \"content\")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"You're an Friendly AI assistant, your name is Claro, you can make normal conversations in a friendly manner, and also provide Answer the question based on the following context make sure it sounds like human and official assistant:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RAG\n",
    "model = ChatOllama(model=\"openhermes:7b-mistral-v2-q8_0\")\n",
    "# model = ChatOllama(model=\"falcon:40b-instruct-q4_1\")\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Add typing for input\n",
    "class Question(BaseModel):\n",
    "    __root__: str\n",
    "\n",
    "\n",
    "chain = chain.with_types(input_type=Question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull openhermes:7b-mistral-v2-q8_0`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat is mediwave\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4511\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4507\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4508\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:154\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    151\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    153\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    163\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:554\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    548\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    552\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    553\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:415\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    414\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    416\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    417\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    419\u001b[0m ]\n\u001b[1;32m    420\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:405\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 405\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m         )\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:624\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 624\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:161\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    160\u001b[0m     }\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/llms/ollama.py:246\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code 404. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe your model is not found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand you should pull the model with `ollama pull \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull openhermes:7b-mistral-v2-q8_0`."
     ]
    }
   ],
   "source": [
    "result = chain.invoke('what is mediwave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "\n",
    "result = food_crew(input='how to make vanilla sponge cake give me the receipe only')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "\n",
    "result = food_crew(input='how to make vanilla sponge cake, give me the receipe')\n",
    "\n",
    "print(food_crew.usage_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5 \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set_verbose(True)\n",
    "# set_debug(True)\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# members = [\"Food_crew\", \"General_conversation\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "\n",
    "members = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "     following workers:  {members}. Given the following user request,\"\n",
    "     respond with the worker to act next. \n",
    "     \n",
    "     if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew',\n",
    "    if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag',    \n",
    "    if the user makes conversation, jokes and funny conversations then use 'General_conv',\n",
    "    if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other',\n",
    "    if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew'\n",
    "        \n",
    "    Each worker will perform a\n",
    "     task and respond with their results and status. When finished,\n",
    "    respond with FINISH.\"\"\"\n",
    ")\n",
    "\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role to act\",\n",
    "    \n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",        \n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": f\"{options}\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": \"route\",\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = OllamaFunctions(\n",
    "    model=os.environ['LLM'],\n",
    "    tool_system_prompt_template=DEFAULT_SYSTEM_TEMPLATE\n",
    "    )\n",
    "\n",
    "\n",
    "def supervisor_node(state):\n",
    "\n",
    "    print(state)\n",
    "    \n",
    "    supervisor_chain = (\n",
    "        prompt\n",
    "        | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )\n",
    "        \n",
    "    result = supervisor_chain.invoke(state)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "\n",
    "from grp_travel_crew_ai.grp_travel_crewai import travel_crew\n",
    "\n",
    "from grp_RAG1.grp_rag1_rag import mediwave_rag\n",
    "\n",
    "from grp_others.grp_others_graph import grp_other_def as gen_others\n",
    "\n",
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "from grp_Gen_Conv.grp_gen_conv_chain import general_conversation\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"Food_crew\", food_crew)\n",
    "workflow.add_node(\"General_conv\", general_conversation)\n",
    "workflow.add_node(\"General_other\", gen_others)\n",
    "workflow.add_node(\"Mediwave_rag\", mediwave_rag)\n",
    "workflow.add_node(\"Travel_crew\", travel_crew)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for member in members:\n",
    "    \n",
    "    if member == 'Mediwave_rag':\n",
    "        continue\n",
    "    if member == 'Travel_crew':\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "    \n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "\n",
    "\n",
    "\n",
    "conditional_map[\"FINISH\"] = END\n",
    "# conditional_map['supervisor'] ='supervisor'\n",
    "\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "workflow.set_finish_point('Mediwave_rag')\n",
    "workflow.set_finish_point('General_conv')\n",
    "workflow.set_finish_point('Travel_crew')\n",
    "\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"what is the current weather in pondicherry\"\n",
    "            )\n",
    "        ],\n",
    "    })\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub \n",
    "\n",
    "hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in kodaikanal and ooty give me a complete 7 day itenary with travel route and food, budget accomodation and other nearby scenic spots and tourist atractions, start from pondicherry and return pondicherry after last day.\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in ooty and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The user's travel needs in Ooty during summer can be met by visiting several top attractions over a 3-day itinerary, starting and ending in Pondicherry. Here is the detailed plan:\\n\\nDay 1:\\n- Visit Dodabetta Peak, the highest point in Ooty, offering breathtaking views of the surrounding mountains and valleys.\\n- Explore Mudumalai Wildlife Sanctuary, a large protected area known for its diverse wildlife population, including elephants, tigers, and various bird species.\\n\\nDay 2:\\n- Spend the morning at the Botanical Gardens, home to a vast collection of exotic plants, flowers, and trees.\\n- In the afternoon, visit Emerald Lake, a serene and picturesque lake nestled in the heart of Ooty.\\n- End the day by relaxing at Ooty Lake, enjoying a peaceful boat ride or taking a leisurely walk around the lake.\\n\\nDay 3:\\n- Begin the day with a visit to Pykara Falls, located approximately 20 km from Ooty, known for its stunning beauty and the surrounding lush greenery.\\n- In the afternoon, head towards Coonoor, a nearby hill station famous for its tea gardens. Take a tour of the gardens and learn about the tea-making process.\\n\\nBy following this itinerary, the user will get to experience the best of Ooty's natural beauty, cultural attractions, and local experiences during their summer vacation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in mysore and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Here's a detailed response summarizing key findings about the given context and information that could be relevant to it: The user is looking for travel suggestions in Mysore during summer and requires a three-day itinerary starting from Pondicherry. Based on your request, I have delegated this task to our Travel agency manager. Here's the suggested itinerary:\\n\\nDay 1:\\n- Depart from Pondicherry early morning by road, which is approximately a 4-hour drive.\\n- Visit the Chamundeshwari Temple located on Chamundi Hill, known for its religious significance and beautiful views of Mysore city.\\n- Head to Brindavan Gardens, located in the Krishnarajasagar Dam area, famous for its terrace gardens, fountains, and musical fountain show.\\n- Overnight stay at a hotel in Mysore.\\n\\nDay 2:\\n- Visit the Mysore Palace, a grand architectural marvel open to public viewing during summer months. Don't forget to check out the famous Dussehra durbar hall.\\n- Explore the Sri Chamarajendra Zoo and Museum located in the heart of Mysore city. The zoo is home to a wide variety of animals, and the museum exhibits artifacts related to the history and culture of Mysore.\\n- Visit the St. Philomena's Church, an impressive Roman Catholic basilica known for its neo-Gothic architecture.\\n- Overnight stay at a hotel in Mysore.\\n\\nDay 3:\\n- Depart from Mysore early morning and head back to Pondicherry by road.\\n- En route, stop at the Ranganathittu Bird Sanctuary located near Srirangapatna, famous for its diverse bird population and scenic beauty.\\n- Arrive in Pondicherry late afternoon/evening and complete your journey.\\n\\nPlease note that travel times may vary depending on traffic conditions, so it's always a good idea to leave early. Additionally, make sure to check the opening hours of each attraction before planning your visit. Let me know if you need any further assistance with this itinerary or if there are any modifications you would like me to make.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in mysore and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"what is the time now\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervisor - update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set_verbose(True)\n",
    "# set_debug(True)\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# members = [\"Food_crew\", \"General_conversation\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "\n",
    "members = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "     following workers:  {members}. Given the following user request,\"\n",
    "     respond with the worker to act next. \n",
    "     \n",
    "     if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew',\n",
    "    if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag',    \n",
    "    if the user makes conversation, jokes and funny conversations then use 'General_conv',\n",
    "    if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other',\n",
    "    if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew'\n",
    "        \n",
    "    Each worker will perform a\n",
    "     task and respond with their results and status. When finished,\n",
    "    respond with FINISH.\"\"\"\n",
    ")\n",
    "\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role to act\",\n",
    "    \n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",        \n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": f\"{options}\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": \"route\",\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = OllamaFunctions(\n",
    "    model=os.environ['LLM'],\n",
    "    tool_system_prompt_template=DEFAULT_SYSTEM_TEMPLATE\n",
    "    )\n",
    "\n",
    "\n",
    "def supervisor_node(state):\n",
    "\n",
    "    print(state)\n",
    "    \n",
    "    supervisor_chain = (\n",
    "        prompt\n",
    "        | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )\n",
    "        \n",
    "    result = supervisor_chain.invoke(state)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "\n",
    "from grp_travel_crew_ai.grp_travel_crewai import travel_crew\n",
    "\n",
    "from grp_RAG1.grp_rag1_rag import mediwave_rag\n",
    "\n",
    "from grp_others.grp_others_graph import grp_other_def as gen_others\n",
    "\n",
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "from grp_Gen_Conv.grp_gen_conv_chain import general_conversation\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"Food_crew\", food_crew)\n",
    "workflow.add_node(\"General_conv\", general_conversation)\n",
    "workflow.add_node(\"General_other\", gen_others)\n",
    "workflow.add_node(\"Mediwave_rag\", mediwave_rag)\n",
    "workflow.add_node(\"Travel_crew\", travel_crew)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for member in members:\n",
    "    \n",
    "    if member == 'Mediwave_rag':\n",
    "        continue\n",
    "    if member == 'Travel_crew':\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "    \n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "\n",
    "\n",
    "\n",
    "conditional_map[\"FINISH\"] = END\n",
    "# conditional_map['supervisor'] ='supervisor'\n",
    "\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "workflow.set_finish_point('Mediwave_rag')\n",
    "workflow.set_finish_point('General_conv')\n",
    "workflow.set_finish_point('Travel_crew')\n",
    "\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plan_and_execute.graph import graph \n",
    "\n",
    "graph.get_input_schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plan_and_execute.graph import graph \n",
    "\n",
    "\n",
    "\n",
    "graph.get_input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': [{'key': 'General_other', 'value': 'Get the current time and date in Pondicherry.'}, {'key': 'Travel_crew', 'value': 'Use the time and date to determine the weather in Pondicherry.'}]}\n",
      "Get the current time and date in Pondicherry.\n",
      "{'messages': [HumanMessage(content='Get the current time and date in Pondicherry.')], 'agent_outcome': None, 'next': 'General_other'}\n",
      "Get the current time and date in Pondicherry.\n",
      "Called `execute_tools`\n",
      "Calling tool: get_now(format: str = '%Y-%m-%d %H:%M:%S')\n",
      "Called `execute_tools`\n",
      "Calling tool: get\\_now\n",
      "Called `execute_tools`\n",
      "Calling tool: Weather(location)\n",
      "Called `execute_tools`\n",
      "Calling tool: Weather(location)\n",
      "Called `execute_tools`\n",
      "Calling tool: wikipedia_search\n",
      "{'next': 'General_other', 'messages': [HumanMessage(content='Get the current time and date in Pondicherry.'), HumanMessage(content='Get the current time and date in Pondicherry.')], 'agent_outcome': AgentFinish(return_values={'output': 'The current time and date in Pondicherry, India.'}, log='The current time and date in Pondicherry, India.')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get the current weather in Pondicherry\n",
      "{'messages': [HumanMessage(content='get the current weather in Pondicherry')], 'agent_outcome': None, 'next': 'Travel_crew'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Delegate work to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', task: 'get current weather in Pondicherry', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to check my input and make sure it's in the correct format.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', question: 'What is the correct format for input to the delegate work tool?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to check my input and make sure it's in the correct format.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', question: 'What is the correct format for input to the delegate work tool?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Thought: \n",
      "Action: Delegate work to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', task: 'get current weather in Pondicherry', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3mThought: \n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', question: 'What is the correct format for input to the delegate work tool?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Thought: I need to double-check the format of my input and make sure it's correct this time.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', question: 'What is the correct format for input to the delegate work tool?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3mThought: It seems like the format of my input is not correct. I need to make sure that the coworker name and task are both strings, while the context is a string as well. The task should also include all necessary information for the co-worker to complete it.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', question: 'What is the correct format for input to the delegate work tool?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to double-check the format of my input and make sure it's correct this time.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', question: 'What is the correct format for input to the delegate work tool?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to double-check the format of my input and make sure it's correct this time.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Expert Travel Agent', question: 'What is the correct format for input to the delegate work tool?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3mThought: I should ask my co-workers if they know how to find and provide the appropriate travel, exploration solution to the user needs.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'City Selection Expert', question: 'Do you know how to find and provide the appropriate travel, exploration solution to the user needs?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Thought: It seems like I need more information before I can effectively delegate work to my co-workers or ask the right questions.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Travel agency manager', question: 'What tools and resources do you have available to find and provide the appropriate travel, exploration solution to the user needs?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Thought: I should check with my co-workers and see if they have access to any other tools or resources that could help me find and provide the appropriate travel, exploration solution to the user needs.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Local Tour Guide', question: 'Do you have access to any other tools or resources that could help me find and provide the appropriate travel, exploration solution to the user needs?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need more information before I can effectively delegate work to my co-workers or ask the right questions.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {coworker: 'Travel agency manager', question: 'What tools and resources do you have available to find and provide the appropriate travel, exploration solution to the user needs?', context: 'The user needs information about the current weather in Pondicherry.'}\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'next': 'Travel_crew', 'messages': [HumanMessage(content='get the current weather in Pondicherry'), HumanMessage(content='get the current weather in Pondicherry')], 'agent_outcome': AgentFinish(return_values={'output': 'Agent stopped due to iteration limit or time limit.'}, log='Agent stopped due to iteration limit or time limit.')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get the current weather in Pondicherry\n",
      "{'messages': [HumanMessage(content='get the current weather in Pondicherry')], 'agent_outcome': None, 'next': 'Travel_crew'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Ask question to co-worker\n",
      "Action Input: {'coworker': 'Travel agency manager', 'question': 'What is the current weather in Pondicherry?', 'context': ''}\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Final Answer: The current weather in Pondicherry is partly cloudy with a temperature of 27C (80.6F) and humidity of 81%.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[93m \n",
      "\n",
      "The current weather in Pondicherry is partly cloudy with a temperature of 27C (80.6F) and humidity of 81%.\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should find out the appropriate travel solution to meet the user's need for the current weather in Pondicherry.\n",
      "Action: Delegate work to co-worker\n",
      "Action Input: {'coworker': 'Expert Travel Agent', 'task': 'Find a suitable travel solution for the user who wants the current weather in Pondicherry', 'context': 'The user is looking for an appropriate travel solution to meet their need for the current weather in Pondicherry.'}\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should search the internet for the current weather in Pondicherry.\n",
      "Action: Search the internet(query)\n",
      "Action Input: {\"query\": \"current weather in Pondicherry\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet(query)' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "Make a calculation: Make a calculation(operation) - Useful to perform any mathematical calculations,\n",
      "        like sum, minus, multiplication, division, etc.\n",
      "        The input to this tool should be a mathematical\n",
      "        expression, a couple examples are `200*7` or `5000/2*10`\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: The user wants to know the current weather in Pondicherry, I should use the \"Search the internet\" action to gather this information.\n",
      "Action: Search the internet(query)\n",
      "Action Input: {\"query\": \"current weather in Pondicherry\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet(query)' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "Make a calculation: Make a calculation(operation) - Useful to perform any mathematical calculations,\n",
      "        like sum, minus, multiplication, division, etc.\n",
      "        The input to this tool should be a mathematical\n",
      "        expression, a couple examples are `200*7` or `5000/2*10`\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: The user wants to know the current weather in Pondicherry. I should use the \"Search the internet\" action to gather this information.\n",
      "Action: Search the internet(query)\n",
      "Action Input: {\"query\": \"current weather in Pondicherry\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet(query)' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "Make a calculation: Make a calculation(operation) - Useful to perform any mathematical calculations,\n",
      "        like sum, minus, multiplication, division, etc.\n",
      "        The input to this tool should be a mathematical\n",
      "        expression, a couple examples are `200*7` or `5000/2*10`\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find out the current weather in Pondicherry, I should search the internet.\n",
      "Action: Search the internet(query)\n",
      "Action Input: {\"query\": \"current weather in Pondicherry\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet(query)' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "Make a calculation: Make a calculation(operation) - Useful to perform any mathematical calculations,\n",
      "        like sum, minus, multiplication, division, etc.\n",
      "        The input to this tool should be a mathematical\n",
      "        expression, a couple examples are `200*7` or `5000/2*10`\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: The user wants to know the current weather in Pondicherry.\n",
      "Action: Search the internet(query)\n",
      "Action Input: {\"query\": \"current weather in Pondicherry\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet(query)' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "Make a calculation: Make a calculation(operation) - Useful to perform any mathematical calculations,\n",
      "        like sum, minus, multiplication, division, etc.\n",
      "        The input to this tool should be a mathematical\n",
      "        expression, a couple examples are `200*7` or `5000/2*10`\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The user wants to know the current weather in Pondicherry. To gather this information, I should use the \"Search the internet\" action to search for relevant results on the internet. The input to this action is a query string that specifies the search terms, in this case, \"current weather in Pondicherry\". The result of the action will be a list of web pages that contain information about the current weather in Pondicherry. I can then use this information to give my final answer.\n",
      "\n",
      "Final Answer: The current weather in Pondicherry is [insert relevant information from the search results].\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[93m \n",
      "\n",
      "The current weather in Pondicherry is [insert relevant information from the search results].\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should find out the appropriate travel solution to meet the user's need for the current weather in Pondicherry.\n",
      "Action: Delegate work to co-worker\n",
      "Action Input: {'coworker': 'Expert Travel Agent', 'task': 'Find a suitable travel solution for the user who wants the current weather in Pondicherry', 'context': 'The user is looking for an appropriate travel solution to meet their need for the current weather in Pondicherry.'}\u001b[0m\u001b[93m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: In order to find the appropriate travel, exploration solution to the user's needs, we need to first gather more context about the user's preferences and requirements.\n",
      "Action: Ask question to co-worker\n",
      "Action Input: {'coworker': 'City Selection Expert', 'question': 'What kind of activities does the user prefer during their trip?', 'context': ''}\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mAction: Search the internet about travel activities.\n",
      "Action Input: {\n",
      "\"query\": \"Travel activities for tourists\",\n",
      "\"page_number\": 1,\n",
      "\"results_per_page\": 10,\n",
      "\"city_name\": None\n",
      "}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet about travel activities.' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: First, let's determine the context of the user's preferences for activities during their trip. This could include things like hobbies, interests, age group, fitness level, etc. Once we have a better understanding of what the user is looking for, I can provide them with a list of recommended cities and activities that would fit their preferences.\n",
      "\n",
      "Action: Search the internet about travel activities for tourists.\n",
      "Action Input: {\n",
      "\"query\": \"Travel activities for tourists\",\n",
      "\"page\\_number\": 1,\n",
      "\"results\\_per\\_page\": 10\n",
      "}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet about travel activities for tourists.' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: First, let's determine the context of the user's preferences for activities during their trip. This could include things like hobbies, interests, age group, fitness level, etc. Once we have a better understanding of what the user is looking for, I can provide them with a list of recommended cities and activities that would fit their preferences.\n",
      "Action: Search the internet about travel activities for tourists.\n",
      "Action Input: {\n",
      "\"query\": \"Travel activities for tourists\",\n",
      "\"page\\_number\": 1,\n",
      "\"results\\_per\\_page\": 10\n",
      "}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet about travel activities for tourists.' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: To determine the user's preferences for activities during their trip, I need more information about the user such as hobbies, interests, age group, fitness level, etc. Once I have this information, I can provide recommendations for cities and activities that match the user's interests.\n",
      "\n",
      "Action: Search the internet about travel activities based on the user's preferences.\n",
      "Action Input: {\n",
      "\"query\": None,\n",
      "\"page\\_number\": 1,\n",
      "\"results\\_per\\_page\": 10,\n",
      "\"city\\_name\": None\n",
      "}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet about travel activities based on the user's preferences.' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: To determine the user's preferences for activities during their trip, I need more information about the user such as hobbies, interests, age group, fitness level, etc. Once I have this information, I can provide recommendations for cities and activities that match the user's interests.\n",
      "Action: Search the internet about travel activities based on the user's preferences.\n",
      "Action Input: {\n",
      "\"query\": None,\n",
      "\"page\\_number\": 1,\n",
      "\"results\\_per\\_page\": 10,\n",
      "\"city\\_name\": None\n",
      "}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Search the internet about travel activities based on the user's preferences.' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: To provide recommendations for cities and activities that match the user's interests, I need more information about the user such as hobbies, interests, age group, fitness level, etc.\n",
      "Action: Ask the user for their preferences related to travel activities.\n",
      "Action Input: {\n",
      "\"query\": \"Please provide your preferences for travel activities such as hobbies, interests, age group, and fitness level.\",\n",
      "\"page\\_number\": 1,\n",
      "\"results\\_per\\_page\": 10,\n",
      "\"city\\_name\": None\n",
      "}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Ask the user for their preferences related to travel activities.' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Thought: To provide recommendations for cities and activities that match the user's interests, I need more information about the user such as hobbies, interests, age group, fitness level, etc.\n",
      "Action: Ask the user for their preferences related to travel activities.\n",
      "Action Input: {\n",
      "\"query\": \"Please provide your preferences for travel activities such as hobbies, interests, age group, and fitness level.\",\n",
      "\"page\\_number\": 1,\n",
      "\"results\\_per\\_page\": 10,\n",
      "\"city\\_name\": None\n",
      "}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Ask the user for their preferences related to travel activities.' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "I apologize for the confusion earlier. It seems that I missed some important steps in the process. Here's an updated version of the conversation based on the correct format:\n",
      "\n",
      "Thought: To determine the user's preferences for activities during their trip, I need more information about the user such as hobbies, interests, age group, fitness level, etc. Once I have this information, I can provide recommendations for cities and activities that match the user's interests.\n",
      "\n",
      "Action: Ask the user for their preferences related to travel activities.\n",
      "Action Input: {\n",
      "\"query\": \"Please provide your preferences for travel activities such as hobbies, interests, age group, and fitness level.\",\n",
      "\"page\\_number\": 1,\n",
      "\"results\\_per\\_page\": 10,\n",
      "\"city\\_name\": None\n",
      "}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'Ask the user for their preferences related to travel activities.' don't exist, these are the only available Actions: Search the internet: Search the internet(query) - Useful to search the internet\n",
      "        about a a given topic and return relevant results\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from plan_and_execute.graph import graph \n",
    "\n",
    "\n",
    "set_verbose=True \n",
    "set_debug=True\n",
    "\n",
    "\n",
    "\n",
    "@chain\n",
    "async def custom_chain(input):\n",
    "    \n",
    "    result = await graph.ainvoke({\"input\": input})\n",
    "    \n",
    "    # print(result)\n",
    "    \n",
    "    return AIMessage(content=result['response'])\n",
    "\n",
    "res = await custom_chain.ainvoke(input='suggest some good spots in pondicherry')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. The current time is...')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"tool\": \"output_formatter\",\n",
    "  \"tool_input\": {\n",
    "    \"plan\": [\n",
    "      {\n",
    "        \"key\": \"Mediwave_rag\",\n",
    "        \"value\": \"Describe Mediwave\"\n",
    "      },\n",
    "      {\n",
    "        \"key\": \"General_other\",\n",
    "        \"value\": \"Get current time\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to parse a function call from mistral:7b-instruct-v0.2-q8_0 output: {\n  \"tool\": \"General_other\",\n  \"tool_input\": {\n    \"output\": {\n      \"steps\": [\n        {\n          \"key\": \"General_other\",\n          \"value\": \"Get current time\"\n        },\n        {\n          \"key\": \"_OutputFormatter\",\n          \"value\": {\n            \"steps\": [\n              {\n                \"key\": \"General_other\",\n                \"value\": \"Get current time\"\n              },\n              {\n                \"key\": \"Food_crew\",\n                \"value\": \"\"\n              },\n              {\n                \"key\": \"_OutputFormatter\",\n                \"value\": {\n                  \"steps\": [\n                    {\n                      \"key\": \"General_other\",\n                      \"value\": \"Get current time\"\n                    },\n                    {\n                      \"key\": \"Food_crew\",\n                      \"value\": \"\"\n                    },\n                    {\n                      \"key\": \"_OutputFormatter.output.steps[0].key\",\n                      \"value\": \"The current time is: {current_time}\"\n                    }\n                  ],\n                  \"current_time\": \"\"\n                }\n              }\n            ]\n          }\n        }\n      ]\n    },\n    \"current_time\": \"\"\n  }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m}\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhats the time now\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mastream(inputs, config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4698\u001b[0m, in \u001b[0;36mRunnableBindingBase.astream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4692\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastream\u001b[39m(\n\u001b[1;32m   4693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4694\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4695\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4696\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4697\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[0;32m-> 4698\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   4699\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4701\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4702\u001b[0m     ):\n\u001b[1;32m   4703\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m/usr/lib/python3.11/contextlib.py:222\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    220\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mathrow(typ, value, traceback)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/channels/base.py:118\u001b[0m, in \u001b[0;36mAsyncChannelsManager\u001b[0;34m(channels, checkpoint)\u001b[0m\n\u001b[1;32m    113\u001b[0m empty \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    114\u001b[0m     k: v\u001b[38;5;241m.\u001b[39mafrom_checkpoint(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(k))\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m channels\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    116\u001b[0m }\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m {k: \u001b[38;5;28;01mawait\u001b[39;00m v\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m empty\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m empty\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:900\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug)\u001b[0m\n\u001b[1;32m    893\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    894\u001b[0m     futures,\n\u001b[1;32m    895\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    896\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    903\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1033\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2536\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2536\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2537\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2538\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m             patch_config(\n\u001b[1;32m   2540\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2541\u001b[0m             ),\n\u001b[1;32m   2542\u001b[0m         )\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3981\u001b[0m, in \u001b[0;36mRunnableLambda.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable asynchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m the_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafunc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m-> 3981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\n\u001b[1;32m   3982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke,\n\u001b[1;32m   3983\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3984\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, the_func),\n\u001b[1;32m   3985\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3986\u001b[0m )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1675\u001b[0m, in \u001b[0;36mRunnable._acall_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1671\u001b[0m coro \u001b[38;5;241m=\u001b[39m acall_func_with_variable_args(\n\u001b[1;32m   1672\u001b[0m     func, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1673\u001b[0m )\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[0;32m-> 1675\u001b[0m     output: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1677\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3928\u001b[0m, in \u001b[0;36mRunnableLambda._ainvoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3926\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3928\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m acall_func_with_variable_args(\n\u001b[1;32m   3929\u001b[0m         cast(Callable, afunc), \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   3930\u001b[0m     )\n\u001b[1;32m   3931\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/packages/rag-weaviate/plan_and_execute/graph.py:45\u001b[0m, in \u001b[0;36mplan_step\u001b[0;34m(super_state)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplan_step\u001b[39m(super_state: PlanExecute):\n\u001b[0;32m---> 45\u001b[0m     plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m planner\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: super_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# return {\"plan\": plan.steps}\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     h \u001b[38;5;241m=\u001b[39m plan\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2536\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2536\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2537\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2538\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m             patch_config(\n\u001b[1;32m   2540\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2541\u001b[0m             ),\n\u001b[1;32m   2542\u001b[0m         )\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4523\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4517\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   4518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4519\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4520\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   4524\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4526\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4527\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:174\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    173\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 174\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    175\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    176\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    177\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    178\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    179\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    180\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:564\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    558\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    562\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    563\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    565\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:524\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    514\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m             ]\n\u001b[1;32m    523\u001b[0m         )\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    525\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    526\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    528\u001b[0m ]\n\u001b[1;32m    529\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:705\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 705\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    706\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    707\u001b[0m         )\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:745\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_agenerate\u001b[39m(\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    739\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    743\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    744\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Top Level call\"\"\"\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[1;32m    746\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    747\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate,\n\u001b[1;32m    748\u001b[0m         messages,\n\u001b[1;32m    749\u001b[0m         stop,\n\u001b[1;32m    750\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mget_sync() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    752\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:514\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run a function in an executor.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    Output: The output of the function.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    516\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)),\n\u001b[1;32m    517\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    520\u001b[0m     executor_or_config, partial(func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs\n\u001b[1;32m    521\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_experimental/llms/ollama_functions.py:110\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         called_tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m    107\u001b[0m             (fn \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions \u001b[38;5;28;01mif\u001b[39;00m fn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m called_tool_name), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m called_tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse a function call from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124moutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_generation_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m             )\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m called_tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m DEFAULT_RESPONSE_FUNCTION[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(\n\u001b[1;32m    116\u001b[0m                 generations\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    117\u001b[0m                     ChatGeneration(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 ]\n\u001b[1;32m    123\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to parse a function call from mistral:7b-instruct-v0.2-q8_0 output: {\n  \"tool\": \"General_other\",\n  \"tool_input\": {\n    \"output\": {\n      \"steps\": [\n        {\n          \"key\": \"General_other\",\n          \"value\": \"Get current time\"\n        },\n        {\n          \"key\": \"_OutputFormatter\",\n          \"value\": {\n            \"steps\": [\n              {\n                \"key\": \"General_other\",\n                \"value\": \"Get current time\"\n              },\n              {\n                \"key\": \"Food_crew\",\n                \"value\": \"\"\n              },\n              {\n                \"key\": \"_OutputFormatter\",\n                \"value\": {\n                  \"steps\": [\n                    {\n                      \"key\": \"General_other\",\n                      \"value\": \"Get current time\"\n                    },\n                    {\n                      \"key\": \"Food_crew\",\n                      \"value\": \"\"\n                    },\n                    {\n                      \"key\": \"_OutputFormatter.output.steps[0].key\",\n                      \"value\": \"The current time is: {current_time}\"\n                    }\n                  ],\n                  \"current_time\": \"\"\n                }\n              }\n            ]\n          }\n        }\n      ]\n    },\n    \"current_time\": \"\"\n  }\n}"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from plan_and_execute.graph import graph \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"whats the time now\"}\n",
    "async for event in graph.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Response',\n",
       " 'description': 'Response to user.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'response': {'type': 'string'}},\n",
       "  'required': ['response']}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "convert_to_openai_function(Response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open ai function runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.output_parsers import (\n",
    "    BaseGenerationOutputParser,\n",
    "    BaseOutputParser,\n",
    "    JsonOutputParser,\n",
    ")\n",
    "from langchain_core.output_parsers.openai_functions import (\n",
    "    JsonOutputFunctionsParser,\n",
    "    PydanticAttrOutputFunctionsParser,\n",
    "    PydanticOutputFunctionsParser,\n",
    ")\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "from langchain.output_parsers import (\n",
    "    JsonOutputKeyToolsParser,\n",
    "    PydanticOutputParser,\n",
    "    PydanticToolsParser,\n",
    ")\n",
    "\n",
    "\n",
    "def create_openai_fn_runnable(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    enforce_single_function_usage: bool = True,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "   \n",
    "    # noqa: E501\n",
    "    if not functions:\n",
    "        raise ValueError(\"Need to pass in at least one function. Received zero.\")\n",
    "    openai_functions = [convert_to_openai_function(f) for f in functions]\n",
    "    llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions}\n",
    "    if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "        llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "    output_parser = output_parser or get_openai_output_parser(functions)\n",
    "    if prompt:\n",
    "        return prompt | llm.bind(functions=openai_functions) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs_) | output_parser\n",
    "\n",
    "\n",
    "def get_openai_output_parser(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    \"\"\"Get the appropriate function output parser given the user functions.\n",
    "\n",
    "    Args:\n",
    "        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\n",
    "            or a Python function. If a dictionary is passed in, it is assumed to\n",
    "            already be a valid OpenAI function.\n",
    "\n",
    "    Returns:\n",
    "        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\n",
    "            a JsonOutputFunctionsParser. If there's only one function and it is\n",
    "            not a Pydantic class, then the output parser will automatically extract\n",
    "            only the function arguments and not the function name.\n",
    "    \"\"\"\n",
    "    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\n",
    "        if len(functions) > 1:\n",
    "            pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "        else:\n",
    "            pydantic_schema = functions[0]\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    else:\n",
    "        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\n",
    "    return output_parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'input': 'tell me about mediwave and also give the current time', 'plan': [{'key': 'Mediwave_rag', 'value': 'Explain about Mediwave'}, {'key': 'General_other', 'value': 'Get current time'}], 'past_steps': ('Explain about Mediwave', \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"), 'response': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about mediwave and also give the current time',\n",
       " 'plan': ['Explain about Mediwave', 'Get current time'],\n",
       " 'past_steps': ('Explain about Mediwave',\n",
       "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
       " 'response': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_ = state.copy()\n",
    "\n",
    "plan_steps = []\n",
    "\n",
    "# m = [val for key, val in item.items() for item in g]\n",
    "\n",
    "for item in state['plan']:\n",
    "    # print(item['value'])\n",
    "    plan_steps.append(item['value'])\n",
    "    # for val in item.values():\n",
    "    #     # k.append(val)\n",
    "    #     print(val)\n",
    "        \n",
    "plan_steps\n",
    "\n",
    "state_ |= {'plan': plan_steps}\n",
    "state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "from langchain_experimental.pydantic_v1 import root_validator\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": <name of the selected tool>,\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "DEFAULT_RESPONSE_FUNCTION = {\n",
    "    \"name\": \"__conversational_response\",\n",
    "    \"description\": (\n",
    "        \"Respond conversationally if no other tools should be called for a given query.\"\n",
    "    ),\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"response\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Conversational response to the user.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"response\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class OllamaFunctions(BaseChatModel):\n",
    "    \"\"\"Function chat model that uses Ollama API.\"\"\"\n",
    "\n",
    "    llm: ChatOllama\n",
    "\n",
    "    tool_system_prompt_template: str\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        values[\"llm\"] = values.get(\"llm\") or ChatOllama(**values, format=\"json\")\n",
    "        values[\"tool_system_prompt_template\"] = (\n",
    "            values.get(\"tool_system_prompt_template\") or DEFAULT_SYSTEM_TEMPLATE\n",
    "        )\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def model(self) -> BaseChatModel:\n",
    "        \"\"\"For backwards compatibility.\"\"\"\n",
    "        return self.llm\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        functions = kwargs.get(\"functions\", [])\n",
    "        if \"function_call\" in kwargs:\n",
    "            functions = [\n",
    "                fn for fn in functions if fn[\"name\"] == kwargs[\"function_call\"][\"name\"]\n",
    "            ]\n",
    "            if not functions:\n",
    "                raise ValueError(\n",
    "                    'If \"function_call\" is specified, you must also pass a matching \\\n",
    "function in \"functions\".'\n",
    "                )\n",
    "            del kwargs[\"function_call\"]\n",
    "        elif not functions:\n",
    "            functions.append(DEFAULT_RESPONSE_FUNCTION)\n",
    "        system_message_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "            self.tool_system_prompt_template\n",
    "        )\n",
    "        system_message = system_message_prompt_template.format(\n",
    "            tools=json.dumps(functions, indent=2)\n",
    "        )\n",
    "        if \"functions\" in kwargs:\n",
    "            del kwargs[\"functions\"]\n",
    "        response_message = self.llm.predict_messages(\n",
    "            [system_message] + messages, stop=stop, callbacks=run_manager, **kwargs\n",
    "        )\n",
    "        chat_generation_content = response_message.content\n",
    "        if not isinstance(chat_generation_content, str):\n",
    "            raise ValueError(\"OllamaFunctions does not support non-string output.\")\n",
    "        try:\n",
    "            parsed_chat_result = json.loads(chat_generation_content)\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\n",
    "                f'\"{self.llm.model}\" did not respond with valid JSON. Please try again.'\n",
    "            )\n",
    "        called_tool_name = parsed_chat_result[\"tool\"]\n",
    "        called_tool_arguments = parsed_chat_result[\"tool_input\"]\n",
    "        called_tool = next(\n",
    "            (fn for fn in functions if fn[\"name\"] == called_tool_name), None\n",
    "        )\n",
    "        if called_tool is None:\n",
    "            raise ValueError(\n",
    "                f\"Failed to parse a function call from {self.llm.model} \\\n",
    "output: {chat_generation_content}\"\n",
    "            )\n",
    "        if called_tool[\"name\"] == DEFAULT_RESPONSE_FUNCTION[\"name\"]:\n",
    "            return ChatResult(\n",
    "                generations=[\n",
    "                    ChatGeneration(\n",
    "                        message=AIMessage(\n",
    "                            content=called_tool_arguments[\"response\"],\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        response_message_with_functions = AIMessage(\n",
    "            content=\"\",\n",
    "            additional_kwargs={\n",
    "                \"function_call\": {\n",
    "                    \"name\": called_tool_name,\n",
    "                    \"arguments\": json.dumps(called_tool_arguments)\n",
    "                    if called_tool_arguments\n",
    "                    else \"\",\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=response_message_with_functions)]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama_functions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.output_parsers import (BaseGenerationOutputParser, BaseOutputParser,)\n",
    "from langchain_core.output_parsers.openai_functions import (JsonOutputFunctionsParser, PydanticOutputFunctionsParser,)\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import (convert_to_openai_function)\n",
    "\n",
    "from langchain.output_parsers import (JsonOutputKeyToolsParser, PydanticOutputParser, PydanticToolsParser)\n",
    "\n",
    "\n",
    "def create_openai_fn_runnable(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    enforce_single_function_usage: bool = True,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "   \n",
    "    openai_functions = [convert_to_openai_function(f) for f in functions]\n",
    "    llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions, **llm_kwargs}\n",
    "    \n",
    "    if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "        llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "    output_parser = output_parser or get_openai_output_parser(functions)\n",
    "    if prompt:\n",
    "        return prompt | llm.bind(**llm_kwargs_) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs_) | output_parser\n",
    "\n",
    "\n",
    "def get_openai_output_parser(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    \"\"\"Get the appropriate function output parser given the user functions.\n",
    "\n",
    "    Args:\n",
    "        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\n",
    "            or a Python function. If a dictionary is passed in, it is assumed to\n",
    "            already be a valid OpenAI function.\n",
    "\n",
    "    Returns:\n",
    "        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\n",
    "            a JsonOutputFunctionsParser. If there's only one function and it is\n",
    "            not a Pydantic class, then the output parser will automatically extract\n",
    "            only the function arguments and not the function name.\n",
    "    \"\"\"\n",
    "    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\n",
    "        if len(functions) > 1:\n",
    "            pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "        else:\n",
    "            pydantic_schema = functions[0]\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    else:\n",
    "        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\n",
    "    return output_parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': Plan(steps=[Step(key='Mediwave_rag', value='Provide detailed information about Mediwave Digital, their focus areas, team composition, and notable achievements.')])}\n"
     ]
    }
   ],
   "source": [
    "state_ = {'input': 'tell me about mediwave',\n",
    " 'plan': ['Explain about Mediwave'],\n",
    " 'past_steps': ('Explain about Mediwave',\n",
    "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
    " 'response': None}\n",
    "\n",
    "from plan_and_execute.planner import crews\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing_extensions import List\n",
    "from typing import Literal\n",
    "\n",
    "class Step(BaseModel):\n",
    "    key: Literal[\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"] = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Step] \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from langchain.chains.openai_functions import create_openai_fn_runnable\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from plan_and_execute.planner import crews\n",
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "import os\n",
    "\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "\n",
    "\n",
    "set_verbose=True \n",
    "set_debug=True\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "response = {'name': 'Response',\n",
    " 'description': 'Response to user.',\n",
    " 'parameters': {'type': 'object',\n",
    "  'properties': {'response': {'type': 'string'}},\n",
    "  'required': ['response']}}\n",
    "\n",
    "\n",
    "function1 = {'name': 'plan',\n",
    " 'description': 'replanner',\n",
    " \n",
    " 'parameters': {\n",
    "   'type': 'array',\n",
    "   'properties': {\n",
    "     'key': {\n",
    "       \"enum\": f\"{crews}\",\n",
    "       'description': 'the worker gonna handle this task/step'\n",
    "       \n",
    ",\n",
    "        \n",
    "       'type': 'string'},\n",
    "     'value': {\n",
    "       'description': 'task/ step the worker need to do',\n",
    "      'type': 'string'}\n",
    "     },\n",
    "                    \n",
    "    'required': ['plan'],\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given user input, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    \n",
    "      if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' key,\n",
    "        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' key,    \n",
    "        if the user makes conversation, jokes and funny conversations then use 'General_conv' key,\n",
    "        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' key,\n",
    "        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' key.\n",
    "    \n",
    "    \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "user input was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly(remove the completed step). If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "replanner = create_openai_fn_runnable(\n",
    "    \n",
    "    [Plan, Response],\n",
    "    OllamaFunctions(model=os.environ['LLM']),\n",
    "    replanner_prompt,\n",
    ")\n",
    "\n",
    "# [function1, response],\n",
    "\n",
    "\n",
    "output = replanner.invoke(state_)\n",
    "\n",
    "\n",
    "if isinstance(output, Response):\n",
    "    print({\"response\": output})\n",
    "else:\n",
    "    print({\"plan\": output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step(key='Mediwave_rag', value='Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Plan(steps=[Step(key='Mediwave_rag', value='Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research'), Step(key='General_other', value='Determine and provide the current time')])\n",
    "\n",
    "g.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': 'Mediwave_rag',\n",
       "  'value': 'Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research'},\n",
       " {'key': 'General_other', 'value': 'Determine and provide the current time'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.dict()['steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromptTemplate(input_variables=['input', 'past_steps', 'plan'], template=\"For the given user input, come up with a simple step by step plan. This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.     \\n      if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' key,\\n        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' key,    \\n        if the user makes conversation, jokes and funny conversations then use 'General_conv' key,\\n        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' key,\\n        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' key.\\n    \\n    \\nThe result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\\n\\nuser input was this:\\n{input}\\n\\nYour original plan was this:\\n{plan}\\n\\nYou have currently done the follow steps:\\n{past_steps}\\n\\nUpdate your plan accordingly(remove the completed step). If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about mediwave and also give the current time',\n",
       " 'plan': ['Explain about Mediwave', 'Get current time'],\n",
       " 'past_steps': ('Explain about Mediwave',\n",
       "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
       " 'response': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'plan': [{'key': 'Mediwave_rag', 'value': 'Get information about Mediwave'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'name': 'Response', 'arguments': {'response': 'Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They have been instrumental in helping organizations achieve customer satisfaction throughout the customer journey. Some of their featured projects include Medichec, which helps identify medications that could potentially impact cognitive function or cause other adverse effects in older individuals, and Oxcare, a digital portal designed for supported self-management of physical health and mental wellbeing. Their tech stack includes various technologies such as Angular, Node.js, Mango DB, Cordova, Express.js, Apostrophe CMS, and PostgreSQL, among others. They take pride in collaborating with exceptional individuals and making a tangible impact with their work. For more information about their clients and testimonials, you can check out their website.'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from pydantic import BaseModel \n",
    "\n",
    "class Steps(BaseModel):\n",
    "    key: str = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Steps] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Plan': [{'key': 'Mediwave_rag',\n",
    "#    'value': 'Retrieve information about Mediwave'},\n",
    "#   {'key': 'General_other',\n",
    "#    'value': 'Format the retrieved information for user consumption'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the 2024 Australia open winner?\"}\n",
    "async for event in graph.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch - supervisor and agent planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "class PlanItem(BaseModel):\n",
    "    key: str = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    plan: List[PlanItem] = Field(description=\"different steps to follow, should be in sorted order, always make minimal steps\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "\n",
    "\n",
    "openai_function = convert_pydantic_to_openai_function(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crews = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = {'name': 'Plan',\n",
    " 'description': '',\n",
    " \n",
    " 'parameters': {\n",
    "   'type': 'array',\n",
    "   'properties': {\n",
    "     'key': {\n",
    "       \"enum\": f\"{crews}\",\n",
    "       'description': 'the worker gonna handle this task/step',\n",
    "       'type': 'string'},\n",
    "     'value': {\n",
    "       'description': 'task/ step the worker need to do',\n",
    "      'type': 'string'}\n",
    "     },\n",
    "                    \n",
    "    'required': ['plan'],\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict\n",
    "# from pydantic import BaseModel\n",
    "\n",
    "# class Parameter(BaseModel):\n",
    "#     key: str\n",
    "#     value: str\n",
    "\n",
    "# class FunctionParameter(BaseModel):\n",
    "#     type: str\n",
    "#     properties: Dict[str, Parameter]\n",
    "#     required: List[str]\n",
    "\n",
    "# class FunctionSchema(BaseModel):\n",
    "#     name: str\n",
    "#     description: str\n",
    "#     parameters: FunctionParameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict, Optional\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "# class Plan(BaseModel):\n",
    "#     name: str\n",
    "#     description: Optional[str] = Field(default='', description='Description of the plan')\n",
    "#     parameters: List[Dict[str, str]]\n",
    "\n",
    "#     class Config:\n",
    "#         json_schema_extra = {\n",
    "#             \"example\": {\n",
    "#                 \"name\": \"Plan\",\n",
    "#                 \"description\": \"Description of the plan\",\n",
    "#                 \"parameters\": [\n",
    "#                     {\n",
    "#                         \"key\": \"worker1\",\n",
    "#                         \"value\": \"Do task A\"\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"key\": \"worker2\",\n",
    "#                         \"value\": \"Do task B\"\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "Plan.schema_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[ Dict[\n",
    "                key: str = Field(description='the worker gonna handle this task/step')\n",
    "                value: str = Field(description='task/ step the worker need to do')\n",
    "    ]\n",
    "                ] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "Plan.schema_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plan.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "    \n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given user input, come up with a simple step by step plan but don't provide answer coz you have tools to figure out things. \\\n",
    "        \n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    \n",
    "    if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' worker,\n",
    "        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' worker,    \n",
    "        if the user makes conversation, jokes and funny conversations then use 'General_conv' worker,\n",
    "        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' worker,\n",
    "        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' worker.\n",
    "    \n",
    "    \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "if the given objective related to mediwave then give the objective as plan\n",
    "\n",
    "user input : {objective}\"\"\"\n",
    ")\n",
    "\n",
    "planner = create_structured_output_runnable(\n",
    "    function, \n",
    "    OllamaFunctions(model=os.environ['LLM']),\n",
    "    planner_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_step(input):\n",
    "    plan = planner.invoke({\"objective\": input})\n",
    "    # return {\"plan\": plan.steps}\n",
    "    h = {\"plan\": plan}\n",
    "    print(h)\n",
    "\n",
    "    return {\"plan\": plan['plan']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_step(input=\"Tell me about mediwave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner.invoke(\"tell me about mediwave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
