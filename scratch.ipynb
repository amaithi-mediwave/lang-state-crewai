{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaithi/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.5.5.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "import weaviate\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import RedisCache\n",
    "import redis\n",
    "\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n",
    "\n",
    "redis_client = redis.Redis.from_url(REDIS_URL)\n",
    "set_llm_cache(RedisCache(redis_client))\n",
    "\n",
    "\n",
    "client = weaviate.Client(\n",
    "url=\"http://localhost:8080\",\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate(client, \n",
    "                    \"GRP\", \n",
    "                    \"content\")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"You're an Friendly AI assistant, your name is Claro, you can make normal conversations in a friendly manner, and also provide Answer the question based on the following context make sure it sounds like human and official assistant:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RAG\n",
    "model = ChatOllama(model=\"openhermes:7b-mistral-v2-q8_0\")\n",
    "# model = ChatOllama(model=\"falcon:40b-instruct-q4_1\")\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Add typing for input\n",
    "class Question(BaseModel):\n",
    "    __root__: str\n",
    "\n",
    "\n",
    "chain = chain.with_types(input_type=Question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull openhermes:7b-mistral-v2-q8_0`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat is mediwave\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4511\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4507\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4508\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:154\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    151\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    153\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    163\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:554\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    548\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    552\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    553\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:415\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    414\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    416\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    417\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    419\u001b[0m ]\n\u001b[1;32m    420\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:405\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 405\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m         )\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:624\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 624\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:161\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    160\u001b[0m     }\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_community/llms/ollama.py:246\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code 404. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe your model is not found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand you should pull the model with `ollama pull \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull openhermes:7b-mistral-v2-q8_0`."
     ]
    }
   ],
   "source": [
    "result = chain.invoke('what is mediwave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "\n",
    "result = food_crew(input='how to make vanilla sponge cake give me the receipe only')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "\n",
    "result = food_crew(input='how to make vanilla sponge cake, give me the receipe')\n",
    "\n",
    "print(food_crew.usage_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5 \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set_verbose(True)\n",
    "# set_debug(True)\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# members = [\"Food_crew\", \"General_conversation\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "\n",
    "members = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "     following workers:  {members}. Given the following user request,\"\n",
    "     respond with the worker to act next. \n",
    "     \n",
    "     if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew',\n",
    "    if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag',    \n",
    "    if the user makes conversation, jokes and funny conversations then use 'General_conv',\n",
    "    if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other',\n",
    "    if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew'\n",
    "        \n",
    "    Each worker will perform a\n",
    "     task and respond with their results and status. When finished,\n",
    "    respond with FINISH.\"\"\"\n",
    ")\n",
    "\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role to act\",\n",
    "    \n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",        \n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": f\"{options}\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": \"route\",\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = OllamaFunctions(\n",
    "    model=os.environ['LLM'],\n",
    "    tool_system_prompt_template=DEFAULT_SYSTEM_TEMPLATE\n",
    "    )\n",
    "\n",
    "\n",
    "def supervisor_node(state):\n",
    "\n",
    "    print(state)\n",
    "    \n",
    "    supervisor_chain = (\n",
    "        prompt\n",
    "        | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )\n",
    "        \n",
    "    result = supervisor_chain.invoke(state)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "\n",
    "from grp_travel_crew_ai.grp_travel_crewai import travel_crew\n",
    "\n",
    "from grp_RAG1.grp_rag1_rag import mediwave_rag\n",
    "\n",
    "from grp_others.grp_others_graph import grp_other_def as gen_others\n",
    "\n",
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "from grp_Gen_Conv.grp_gen_conv_chain import general_conversation\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"Food_crew\", food_crew)\n",
    "workflow.add_node(\"General_conv\", general_conversation)\n",
    "workflow.add_node(\"General_other\", gen_others)\n",
    "workflow.add_node(\"Mediwave_rag\", mediwave_rag)\n",
    "workflow.add_node(\"Travel_crew\", travel_crew)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for member in members:\n",
    "    \n",
    "    if member == 'Mediwave_rag':\n",
    "        continue\n",
    "    if member == 'Travel_crew':\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "    \n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "\n",
    "\n",
    "\n",
    "conditional_map[\"FINISH\"] = END\n",
    "# conditional_map['supervisor'] ='supervisor'\n",
    "\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "workflow.set_finish_point('Mediwave_rag')\n",
    "workflow.set_finish_point('General_conv')\n",
    "workflow.set_finish_point('Travel_crew')\n",
    "\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"what is the current weather in pondicherry\"\n",
    "            )\n",
    "        ],\n",
    "    })\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub \n",
    "\n",
    "hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in kodaikanal and ooty give me a complete 7 day itenary with travel route and food, budget accomodation and other nearby scenic spots and tourist atractions, start from pondicherry and return pondicherry after last day.\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in ooty and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The user's travel needs in Ooty during summer can be met by visiting several top attractions over a 3-day itinerary, starting and ending in Pondicherry. Here is the detailed plan:\\n\\nDay 1:\\n- Visit Dodabetta Peak, the highest point in Ooty, offering breathtaking views of the surrounding mountains and valleys.\\n- Explore Mudumalai Wildlife Sanctuary, a large protected area known for its diverse wildlife population, including elephants, tigers, and various bird species.\\n\\nDay 2:\\n- Spend the morning at the Botanical Gardens, home to a vast collection of exotic plants, flowers, and trees.\\n- In the afternoon, visit Emerald Lake, a serene and picturesque lake nestled in the heart of Ooty.\\n- End the day by relaxing at Ooty Lake, enjoying a peaceful boat ride or taking a leisurely walk around the lake.\\n\\nDay 3:\\n- Begin the day with a visit to Pykara Falls, located approximately 20 km from Ooty, known for its stunning beauty and the surrounding lush greenery.\\n- In the afternoon, head towards Coonoor, a nearby hill station famous for its tea gardens. Take a tour of the gardens and learn about the tea-making process.\\n\\nBy following this itinerary, the user will get to experience the best of Ooty's natural beauty, cultural attractions, and local experiences during their summer vacation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in mysore and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Here's a detailed response summarizing key findings about the given context and information that could be relevant to it: The user is looking for travel suggestions in Mysore during summer and requires a three-day itinerary starting from Pondicherry. Based on your request, I have delegated this task to our Travel agency manager. Here's the suggested itinerary:\\n\\nDay 1:\\n- Depart from Pondicherry early morning by road, which is approximately a 4-hour drive.\\n- Visit the Chamundeshwari Temple located on Chamundi Hill, known for its religious significance and beautiful views of Mysore city.\\n- Head to Brindavan Gardens, located in the Krishnarajasagar Dam area, famous for its terrace gardens, fountains, and musical fountain show.\\n- Overnight stay at a hotel in Mysore.\\n\\nDay 2:\\n- Visit the Mysore Palace, a grand architectural marvel open to public viewing during summer months. Don't forget to check out the famous Dussehra durbar hall.\\n- Explore the Sri Chamarajendra Zoo and Museum located in the heart of Mysore city. The zoo is home to a wide variety of animals, and the museum exhibits artifacts related to the history and culture of Mysore.\\n- Visit the St. Philomena's Church, an impressive Roman Catholic basilica known for its neo-Gothic architecture.\\n- Overnight stay at a hotel in Mysore.\\n\\nDay 3:\\n- Depart from Mysore early morning and head back to Pondicherry by road.\\n- En route, stop at the Ranganathittu Bird Sanctuary located near Srirangapatna, famous for its diverse bird population and scenic beauty.\\n- Arrive in Pondicherry late afternoon/evening and complete your journey.\\n\\nPlease note that travel times may vary depending on traffic conditions, so it's always a good idea to leave early. Additionally, make sure to check the opening hours of each attraction before planning your visit. Let me know if you need any further assistance with this itinerary or if there are any modifications you would like me to make.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in mysore and give me a 3 day itenary, start from pondicherry and return pondicherry after third day\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(input=\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    config= {\"recursion_limit\": 100},\n",
    "    \n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"suggest some good spots to visit during summer in pondicherry and give me a 2 day itenary\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"what is the time now\"\n",
    "            )\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervisor - update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# set_verbose(True)\n",
    "# set_debug(True)\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# members = [\"Food_crew\", \"General_conversation\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "\n",
    "members = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "     following workers:  {members}. Given the following user request,\"\n",
    "     respond with the worker to act next. \n",
    "     \n",
    "     if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew',\n",
    "    if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag',    \n",
    "    if the user makes conversation, jokes and funny conversations then use 'General_conv',\n",
    "    if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other',\n",
    "    if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew'\n",
    "        \n",
    "    Each worker will perform a\n",
    "     task and respond with their results and status. When finished,\n",
    "    respond with FINISH.\"\"\"\n",
    ")\n",
    "\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role to act\",\n",
    "    \n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",        \n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": f\"{options}\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": \"route\",\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = OllamaFunctions(\n",
    "    model=os.environ['LLM'],\n",
    "    tool_system_prompt_template=DEFAULT_SYSTEM_TEMPLATE\n",
    "    )\n",
    "\n",
    "\n",
    "def supervisor_node(state):\n",
    "\n",
    "    print(state)\n",
    "    \n",
    "    supervisor_chain = (\n",
    "        prompt\n",
    "        | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )\n",
    "        \n",
    "    result = supervisor_chain.invoke(state)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "\n",
    "from grp_travel_crew_ai.grp_travel_crewai import travel_crew\n",
    "\n",
    "from grp_RAG1.grp_rag1_rag import mediwave_rag\n",
    "\n",
    "from grp_others.grp_others_graph import grp_other_def as gen_others\n",
    "\n",
    "from grp_food_crew_ai.grp_food_crewai import food_crew\n",
    "\n",
    "from grp_Gen_Conv.grp_gen_conv_chain import general_conversation\n",
    "\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"Food_crew\", food_crew)\n",
    "workflow.add_node(\"General_conv\", general_conversation)\n",
    "workflow.add_node(\"General_other\", gen_others)\n",
    "workflow.add_node(\"Mediwave_rag\", mediwave_rag)\n",
    "workflow.add_node(\"Travel_crew\", travel_crew)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for member in members:\n",
    "    \n",
    "    if member == 'Mediwave_rag':\n",
    "        continue\n",
    "    if member == 'Travel_crew':\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "    \n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "\n",
    "\n",
    "\n",
    "conditional_map[\"FINISH\"] = END\n",
    "# conditional_map['supervisor'] ='supervisor'\n",
    "\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "workflow.set_finish_point('Mediwave_rag')\n",
    "workflow.set_finish_point('General_conv')\n",
    "workflow.set_finish_point('Travel_crew')\n",
    "\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plan_and_execute.graph import graph \n",
    "\n",
    "graph.get_input_schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plan_and_execute.graph import graph \n",
    "\n",
    "\n",
    "\n",
    "graph.get_input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(result)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AIMessage(content\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m custom_chain\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuggest some good spots in pondicherry\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3981\u001b[0m, in \u001b[0;36mRunnableLambda.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable asynchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m the_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafunc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m-> 3981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\n\u001b[1;32m   3982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke,\n\u001b[1;32m   3983\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3984\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, the_func),\n\u001b[1;32m   3985\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3986\u001b[0m )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1675\u001b[0m, in \u001b[0;36mRunnable._acall_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1671\u001b[0m coro \u001b[38;5;241m=\u001b[39m acall_func_with_variable_args(\n\u001b[1;32m   1672\u001b[0m     func, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1673\u001b[0m )\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[0;32m-> 1675\u001b[0m     output: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1677\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3928\u001b[0m, in \u001b[0;36mRunnableLambda._ainvoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3926\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3928\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m acall_func_with_variable_args(\n\u001b[1;32m   3929\u001b[0m         cast(Callable, afunc), \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   3930\u001b[0m     )\n\u001b[1;32m   3931\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mcustom_chain\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@chain\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_chain\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m})\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(result)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AIMessage(content\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4523\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4517\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   4518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4519\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4520\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   4524\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4526\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4527\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1006\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[0;34m(self, input, config, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m output_is_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_keys, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   1005\u001b[0m latest: Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any] \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m output_is_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1006\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1008\u001b[0m     config,\n\u001b[1;32m   1009\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1010\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1011\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m   1012\u001b[0m     interrupt_before_nodes\u001b[38;5;241m=\u001b[39minterrupt_before_nodes,\n\u001b[1;32m   1013\u001b[0m     interrupt_after_nodes\u001b[38;5;241m=\u001b[39minterrupt_after_nodes,\n\u001b[1;32m   1014\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1016\u001b[0m ):\n\u001b[1;32m   1017\u001b[0m     latest \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlatest, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchunk} \u001b[38;5;28;01mif\u001b[39;00m output_is_dict \u001b[38;5;28;01melse\u001b[39;00m chunk\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m latest\n",
      "File \u001b[0;32m/usr/lib/python3.11/contextlib.py:222\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    220\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mathrow(typ, value, traceback)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/channels/base.py:118\u001b[0m, in \u001b[0;36mAsyncChannelsManager\u001b[0;34m(channels, checkpoint)\u001b[0m\n\u001b[1;32m    113\u001b[0m empty \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    114\u001b[0m     k: v\u001b[38;5;241m.\u001b[39mafrom_checkpoint(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(k))\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m channels\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    116\u001b[0m }\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m {k: \u001b[38;5;28;01mawait\u001b[39;00m v\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m empty\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m empty\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:893\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug)\u001b[0m\n\u001b[1;32m    879\u001b[0m futures \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    880\u001b[0m     [\n\u001b[1;32m    881\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mcreate_task(_aconsume(proc\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;28minput\u001b[39m, config)))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    888\u001b[0m     ]\n\u001b[1;32m    889\u001b[0m )\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m# execute tasks, and wait for one to fail or all to finish.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# each task is independent from all other concurrent tasks\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    894\u001b[0m     futures,\n\u001b[1;32m    895\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    896\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    900\u001b[0m _panic_or_proceed(done, inflight, step)\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py:423\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing coroutines is forbidden, use tasks explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    422\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py:530\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    527\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from plan_and_execute.graph import graph \n",
    "\n",
    "\n",
    "set_verbose=True \n",
    "set_debug=True\n",
    "\n",
    "\n",
    "\n",
    "@chain\n",
    "async def custom_chain(input):\n",
    "    \n",
    "    result = await graph.ainvoke({\"input\": input})\n",
    "    \n",
    "    # print(result)\n",
    "    \n",
    "    return AIMessage(content=result['response'])\n",
    "\n",
    "res = await custom_chain.ainvoke(input='suggest some good spots in pondicherry')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They have worked on several projects, one of which is the Sefton & Liverpool CAMHS platform built using Node.js, MongoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize. The current time is...')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"tool\": \"output_formatter\",\n",
    "  \"tool_input\": {\n",
    "    \"plan\": [\n",
    "      {\n",
    "        \"key\": \"Mediwave_rag\",\n",
    "        \"value\": \"Describe Mediwave\"\n",
    "      },\n",
    "      {\n",
    "        \"key\": \"General_other\",\n",
    "        \"value\": \"Get current time\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to parse a function call from mistral:7b-instruct-q6_K output: {\n  \"tool\": \"Travel_crew\",\n  \"tool_input\": {\n    \"plan\": [\n      {\n        \"key\": \"Food_crew\",\n        \"value\": \"suggesting food spots\"\n      },\n      {\n        \"key\": \"General_other\",\n        \"value\": \"suggesting tourist spots\"\n      }\n    ]\n  }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m}\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggest some good tourist spots in pondicherry\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mastream(inputs, config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4698\u001b[0m, in \u001b[0;36mRunnableBindingBase.astream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4692\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastream\u001b[39m(\n\u001b[1;32m   4693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4694\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4695\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4696\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4697\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[0;32m-> 4698\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   4699\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4701\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4702\u001b[0m     ):\n\u001b[1;32m   4703\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m/usr/lib/python3.11/contextlib.py:222\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    220\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mathrow(typ, value, traceback)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/channels/base.py:118\u001b[0m, in \u001b[0;36mAsyncChannelsManager\u001b[0;34m(channels, checkpoint)\u001b[0m\n\u001b[1;32m    113\u001b[0m empty \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    114\u001b[0m     k: v\u001b[38;5;241m.\u001b[39mafrom_checkpoint(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(k))\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m channels\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    116\u001b[0m }\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m {k: \u001b[38;5;28;01mawait\u001b[39;00m v\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m empty\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m empty\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:900\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug)\u001b[0m\n\u001b[1;32m    893\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    894\u001b[0m     futures,\n\u001b[1;32m    895\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    896\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    903\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1033\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2536\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2536\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2537\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2538\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m             patch_config(\n\u001b[1;32m   2540\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2541\u001b[0m             ),\n\u001b[1;32m   2542\u001b[0m         )\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3981\u001b[0m, in \u001b[0;36mRunnableLambda.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable asynchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m the_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafunc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m-> 3981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\n\u001b[1;32m   3982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke,\n\u001b[1;32m   3983\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3984\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, the_func),\n\u001b[1;32m   3985\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3986\u001b[0m )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1675\u001b[0m, in \u001b[0;36mRunnable._acall_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1671\u001b[0m coro \u001b[38;5;241m=\u001b[39m acall_func_with_variable_args(\n\u001b[1;32m   1672\u001b[0m     func, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1673\u001b[0m )\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[0;32m-> 1675\u001b[0m     output: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1677\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3928\u001b[0m, in \u001b[0;36mRunnableLambda._ainvoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3926\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3928\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m acall_func_with_variable_args(\n\u001b[1;32m   3929\u001b[0m         cast(Callable, afunc), \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   3930\u001b[0m     )\n\u001b[1;32m   3931\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/packages/rag-weaviate/plan_and_execute/graph.py:45\u001b[0m, in \u001b[0;36mplan_step\u001b[0;34m(super_state)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplan_step\u001b[39m(super_state: PlanExecute):\n\u001b[0;32m---> 45\u001b[0m     plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m planner\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: super_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# return {\"plan\": plan.steps}\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     h \u001b[38;5;241m=\u001b[39m plan\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2536\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2536\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2537\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2538\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m             patch_config(\n\u001b[1;32m   2540\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2541\u001b[0m             ),\n\u001b[1;32m   2542\u001b[0m         )\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4523\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4517\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   4518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4519\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4520\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   4524\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4526\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4527\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:174\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    173\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 174\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    175\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    176\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    177\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    178\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    179\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    180\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:564\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    558\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    562\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    563\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    565\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:524\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    514\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m             ]\n\u001b[1;32m    523\u001b[0m         )\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    525\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    526\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    528\u001b[0m ]\n\u001b[1;32m    529\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:705\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 705\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    706\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    707\u001b[0m         )\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:745\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_agenerate\u001b[39m(\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    739\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    743\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    744\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Top Level call\"\"\"\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[1;32m    746\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    747\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate,\n\u001b[1;32m    748\u001b[0m         messages,\n\u001b[1;32m    749\u001b[0m         stop,\n\u001b[1;32m    750\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mget_sync() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    752\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:514\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run a function in an executor.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    Output: The output of the function.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    516\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)),\n\u001b[1;32m    517\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    520\u001b[0m     executor_or_config, partial(func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs\n\u001b[1;32m    521\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_experimental/llms/ollama_functions.py:110\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         called_tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m    107\u001b[0m             (fn \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions \u001b[38;5;28;01mif\u001b[39;00m fn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m called_tool_name), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m called_tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse a function call from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124moutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_generation_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m             )\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m called_tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m DEFAULT_RESPONSE_FUNCTION[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(\n\u001b[1;32m    116\u001b[0m                 generations\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    117\u001b[0m                     ChatGeneration(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 ]\n\u001b[1;32m    123\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to parse a function call from mistral:7b-instruct-q6_K output: {\n  \"tool\": \"Travel_crew\",\n  \"tool_input\": {\n    \"plan\": [\n      {\n        \"key\": \"Food_crew\",\n        \"value\": \"suggesting food spots\"\n      },\n      {\n        \"key\": \"General_other\",\n        \"value\": \"suggesting tourist spots\"\n      }\n    ]\n  }\n}"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from plan_and_execute.graph import graph \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"suggest some good tourist spots in pondicherry\"}\n",
    "async for event in graph.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Response',\n",
       " 'description': 'Response to user.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'response': {'type': 'string'}},\n",
       "  'required': ['response']}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "convert_to_openai_function(Response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open ai function runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.output_parsers import (\n",
    "    BaseGenerationOutputParser,\n",
    "    BaseOutputParser,\n",
    "    JsonOutputParser,\n",
    ")\n",
    "from langchain_core.output_parsers.openai_functions import (\n",
    "    JsonOutputFunctionsParser,\n",
    "    PydanticAttrOutputFunctionsParser,\n",
    "    PydanticOutputFunctionsParser,\n",
    ")\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "from langchain.output_parsers import (\n",
    "    JsonOutputKeyToolsParser,\n",
    "    PydanticOutputParser,\n",
    "    PydanticToolsParser,\n",
    ")\n",
    "\n",
    "\n",
    "def create_openai_fn_runnable(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    enforce_single_function_usage: bool = True,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "   \n",
    "    # noqa: E501\n",
    "    if not functions:\n",
    "        raise ValueError(\"Need to pass in at least one function. Received zero.\")\n",
    "    openai_functions = [convert_to_openai_function(f) for f in functions]\n",
    "    llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions}\n",
    "    if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "        llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "    output_parser = output_parser or get_openai_output_parser(functions)\n",
    "    if prompt:\n",
    "        return prompt | llm.bind(functions=openai_functions) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs_) | output_parser\n",
    "\n",
    "\n",
    "def get_openai_output_parser(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    \"\"\"Get the appropriate function output parser given the user functions.\n",
    "\n",
    "    Args:\n",
    "        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\n",
    "            or a Python function. If a dictionary is passed in, it is assumed to\n",
    "            already be a valid OpenAI function.\n",
    "\n",
    "    Returns:\n",
    "        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\n",
    "            a JsonOutputFunctionsParser. If there's only one function and it is\n",
    "            not a Pydantic class, then the output parser will automatically extract\n",
    "            only the function arguments and not the function name.\n",
    "    \"\"\"\n",
    "    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\n",
    "        if len(functions) > 1:\n",
    "            pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "        else:\n",
    "            pydantic_schema = functions[0]\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    else:\n",
    "        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\n",
    "    return output_parser\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'input': 'tell me about mediwave and also give the current time', 'plan': [{'key': 'Mediwave_rag', 'value': 'Explain about Mediwave'}, {'key': 'General_other', 'value': 'Get current time'}], 'past_steps': ('Explain about Mediwave', \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"), 'response': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about mediwave and also give the current time',\n",
       " 'plan': ['Explain about Mediwave', 'Get current time'],\n",
       " 'past_steps': ('Explain about Mediwave',\n",
       "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
       " 'response': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_ = state.copy()\n",
    "\n",
    "plan_steps = []\n",
    "\n",
    "# m = [val for key, val in item.items() for item in g]\n",
    "\n",
    "for item in state['plan']:\n",
    "    # print(item['value'])\n",
    "    plan_steps.append(item['value'])\n",
    "    # for val in item.values():\n",
    "    #     # k.append(val)\n",
    "    #     print(val)\n",
    "        \n",
    "plan_steps\n",
    "\n",
    "state_ |= {'plan': plan_steps}\n",
    "state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "from langchain_experimental.pydantic_v1 import root_validator\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": <name of the selected tool>,\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "DEFAULT_RESPONSE_FUNCTION = {\n",
    "    \"name\": \"__conversational_response\",\n",
    "    \"description\": (\n",
    "        \"Respond conversationally if no other tools should be called for a given query.\"\n",
    "    ),\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"response\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Conversational response to the user.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"response\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class OllamaFunctions(BaseChatModel):\n",
    "    \"\"\"Function chat model that uses Ollama API.\"\"\"\n",
    "\n",
    "    llm: ChatOllama\n",
    "\n",
    "    tool_system_prompt_template: str\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        values[\"llm\"] = values.get(\"llm\") or ChatOllama(**values, format=\"json\")\n",
    "        values[\"tool_system_prompt_template\"] = (\n",
    "            values.get(\"tool_system_prompt_template\") or DEFAULT_SYSTEM_TEMPLATE\n",
    "        )\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def model(self) -> BaseChatModel:\n",
    "        \"\"\"For backwards compatibility.\"\"\"\n",
    "        return self.llm\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        functions = kwargs.get(\"functions\", [])\n",
    "        if \"function_call\" in kwargs:\n",
    "            functions = [\n",
    "                fn for fn in functions if fn[\"name\"] == kwargs[\"function_call\"][\"name\"]\n",
    "            ]\n",
    "            if not functions:\n",
    "                raise ValueError(\n",
    "                    'If \"function_call\" is specified, you must also pass a matching \\\n",
    "function in \"functions\".'\n",
    "                )\n",
    "            del kwargs[\"function_call\"]\n",
    "        elif not functions:\n",
    "            functions.append(DEFAULT_RESPONSE_FUNCTION)\n",
    "        system_message_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "            self.tool_system_prompt_template\n",
    "        )\n",
    "        system_message = system_message_prompt_template.format(\n",
    "            tools=json.dumps(functions, indent=2)\n",
    "        )\n",
    "        if \"functions\" in kwargs:\n",
    "            del kwargs[\"functions\"]\n",
    "        response_message = self.llm.predict_messages(\n",
    "            [system_message] + messages, stop=stop, callbacks=run_manager, **kwargs\n",
    "        )\n",
    "        chat_generation_content = response_message.content\n",
    "        if not isinstance(chat_generation_content, str):\n",
    "            raise ValueError(\"OllamaFunctions does not support non-string output.\")\n",
    "        try:\n",
    "            parsed_chat_result = json.loads(chat_generation_content)\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\n",
    "                f'\"{self.llm.model}\" did not respond with valid JSON. Please try again.'\n",
    "            )\n",
    "        called_tool_name = parsed_chat_result[\"tool\"]\n",
    "        called_tool_arguments = parsed_chat_result[\"tool_input\"]\n",
    "        called_tool = next(\n",
    "            (fn for fn in functions if fn[\"name\"] == called_tool_name), None\n",
    "        )\n",
    "        if called_tool is None:\n",
    "            raise ValueError(\n",
    "                f\"Failed to parse a function call from {self.llm.model} \\\n",
    "output: {chat_generation_content}\"\n",
    "            )\n",
    "        if called_tool[\"name\"] == DEFAULT_RESPONSE_FUNCTION[\"name\"]:\n",
    "            return ChatResult(\n",
    "                generations=[\n",
    "                    ChatGeneration(\n",
    "                        message=AIMessage(\n",
    "                            content=called_tool_arguments[\"response\"],\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        response_message_with_functions = AIMessage(\n",
    "            content=\"\",\n",
    "            additional_kwargs={\n",
    "                \"function_call\": {\n",
    "                    \"name\": called_tool_name,\n",
    "                    \"arguments\": json.dumps(called_tool_arguments)\n",
    "                    if called_tool_arguments\n",
    "                    else \"\",\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=response_message_with_functions)]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama_functions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.output_parsers import (BaseGenerationOutputParser, BaseOutputParser,)\n",
    "from langchain_core.output_parsers.openai_functions import (JsonOutputFunctionsParser, PydanticOutputFunctionsParser,)\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import (convert_to_openai_function)\n",
    "\n",
    "from langchain.output_parsers import (JsonOutputKeyToolsParser, PydanticOutputParser, PydanticToolsParser)\n",
    "\n",
    "\n",
    "def create_openai_fn_runnable(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    enforce_single_function_usage: bool = True,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "   \n",
    "    openai_functions = [convert_to_openai_function(f) for f in functions]\n",
    "    llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions, **llm_kwargs}\n",
    "    \n",
    "    if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "        llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "    output_parser = output_parser or get_openai_output_parser(functions)\n",
    "    if prompt:\n",
    "        return prompt | llm.bind(**llm_kwargs_) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs_) | output_parser\n",
    "\n",
    "\n",
    "def get_openai_output_parser(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    \"\"\"Get the appropriate function output parser given the user functions.\n",
    "\n",
    "    Args:\n",
    "        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\n",
    "            or a Python function. If a dictionary is passed in, it is assumed to\n",
    "            already be a valid OpenAI function.\n",
    "\n",
    "    Returns:\n",
    "        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\n",
    "            a JsonOutputFunctionsParser. If there's only one function and it is\n",
    "            not a Pydantic class, then the output parser will automatically extract\n",
    "            only the function arguments and not the function name.\n",
    "    \"\"\"\n",
    "    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\n",
    "        if len(functions) > 1:\n",
    "            pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "        else:\n",
    "            pydantic_schema = functions[0]\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    else:\n",
    "        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\n",
    "    return output_parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['path', 'query'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['path', 'query'] Ignoring optional parameter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': Plan(steps=[Step(key='Mediwave_rag', value='Provide detailed information about Mediwave Digital, their focus areas, team composition, and notable achievements.')])}\n"
     ]
    }
   ],
   "source": [
    "state_ = {'input': 'tell me about mediwave',\n",
    " 'plan': ['Explain about Mediwave'],\n",
    " 'past_steps': ('Explain about Mediwave',\n",
    "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
    " 'response': None}\n",
    "\n",
    "from plan_and_execute.planner import crews\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing_extensions import List\n",
    "from typing import Literal\n",
    "\n",
    "class Step(BaseModel):\n",
    "    key: Literal[\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"] = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Step] \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from langchain.chains.openai_functions import create_openai_fn_runnable\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from plan_and_execute.planner import crews\n",
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "import os\n",
    "\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "\n",
    "\n",
    "set_verbose=True \n",
    "set_debug=True\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "response = {'name': 'Response',\n",
    " 'description': 'Response to user.',\n",
    " 'parameters': {'type': 'object',\n",
    "  'properties': {'response': {'type': 'string'}},\n",
    "  'required': ['response']}}\n",
    "\n",
    "\n",
    "function1 = {'name': 'plan',\n",
    " 'description': 'replanner',\n",
    " \n",
    " 'parameters': {\n",
    "   'type': 'array',\n",
    "   'properties': {\n",
    "     'key': {\n",
    "       \"enum\": f\"{crews}\",\n",
    "       'description': 'the worker gonna handle this task/step'\n",
    "       \n",
    ",\n",
    "        \n",
    "       'type': 'string'},\n",
    "     'value': {\n",
    "       'description': 'task/ step the worker need to do',\n",
    "      'type': 'string'}\n",
    "     },\n",
    "                    \n",
    "    'required': ['plan'],\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given user input, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    \n",
    "      if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' key,\n",
    "        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' key,    \n",
    "        if the user makes conversation, jokes and funny conversations then use 'General_conv' key,\n",
    "        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' key,\n",
    "        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' key.\n",
    "    \n",
    "    \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "user input was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly(remove the completed step). If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "replanner = create_openai_fn_runnable(\n",
    "    \n",
    "    [Plan, Response],\n",
    "    OllamaFunctions(model=os.environ['LLM']),\n",
    "    replanner_prompt,\n",
    ")\n",
    "\n",
    "# [function1, response],\n",
    "\n",
    "\n",
    "output = replanner.invoke(state_)\n",
    "\n",
    "\n",
    "if isinstance(output, Response):\n",
    "    print({\"response\": output})\n",
    "else:\n",
    "    print({\"plan\": output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step(key='Mediwave_rag', value='Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Plan(steps=[Step(key='Mediwave_rag', value='Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research'), Step(key='General_other', value='Determine and provide the current time')])\n",
    "\n",
    "g.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': 'Mediwave_rag',\n",
       "  'value': 'Give detailed information about Mediwave Digital and their focus on healthcare technology and clinical research'},\n",
       " {'key': 'General_other', 'value': 'Determine and provide the current time'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.dict()['steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromptTemplate(input_variables=['input', 'past_steps', 'plan'], template=\"For the given user input, come up with a simple step by step plan. This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.     \\n      if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' key,\\n        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' key,    \\n        if the user makes conversation, jokes and funny conversations then use 'General_conv' key,\\n        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' key,\\n        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' key.\\n    \\n    \\nThe result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\\n\\nuser input was this:\\n{input}\\n\\nYour original plan was this:\\n{plan}\\n\\nYou have currently done the follow steps:\\n{past_steps}\\n\\nUpdate your plan accordingly(remove the completed step). If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Accept. Valid values are ['query', 'path'] Ignoring optional parameter\n",
      "Unsupported APIPropertyLocation \"ParameterLocation.HEADER\" for parameter Content-Type. Valid values are ['query', 'path'] Ignoring optional parameter\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing_extensions import List\n",
    "from typing import Literal\n",
    "\n",
    "class Step(BaseModel):\n",
    "    key: Literal[\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"] = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Step] \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chains.openai_functions import create_openai_fn_runnable\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from plan_and_execute.planner import crews\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "import os\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "\n",
    "\n",
    "set_verbose=True \n",
    "set_debug=True\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "response = {'name': 'Response',\n",
    " 'description': 'Response to user.',\n",
    " 'parameters': {'type': 'object',\n",
    "  'properties': {'response': {'type': 'string'}},\n",
    "  'required': ['response']}}\n",
    "\n",
    "\n",
    "function1 = {'name': 'plan',\n",
    " 'description': 'replanner',\n",
    " \n",
    " 'parameters': {\n",
    "   'type': 'array',\n",
    "   'properties': {\n",
    "     'key': {\n",
    "       \"enum\": f\"{crews}\",\n",
    "       'description': 'the worker gonna handle this task/step'\n",
    "       \n",
    ",\n",
    "        \n",
    "       'type': 'string'},\n",
    "     'value': {\n",
    "       'description': 'task/ step the worker need to do',\n",
    "      'type': 'string'}\n",
    "     },\n",
    "                    \n",
    "    'required': ['plan'],\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm = ChatOllama(model=os.environ['LLM'], stop= [\n",
    "#         os.environ['LLM_START_PARAM'],\n",
    "#         os.environ['LLM_STOP_PARAM']\n",
    "#     ]\n",
    "#                  )\n",
    "\n",
    "\n",
    "# replanner = create_openai_fn_runnable(\n",
    "    \n",
    "#     [Plan, Response],\n",
    "#     OllamaFunctions(llm=llm),\n",
    "#     replanner_prompt,\n",
    "# ).with_retry(\n",
    "#   retry_if_exception_type=(ValueError,KeyError),\n",
    "#   stop_after_attempt=4,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# output = replanner.invoke(state_)\n",
    "\n",
    "\n",
    "# if isinstance(output, Response):\n",
    "#     print({\"response\": output})\n",
    "# else:\n",
    "#     print({\"plan\": output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'input': 'tell me about mediwave and also give the current time',\n",
    " 'plan': ['Explain about Mediwave', 'Get current time'],\n",
    " 'past_steps': ('Explain about Mediwave',\n",
    "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
    " 'response': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, TypeVar\n",
    "\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.prompts import BasePromptTemplate, PromptTemplate\n",
    "\n",
    "NAIVE_COMPLETION_RETRY = \"\"\"Prompt:\n",
    "{prompt}\n",
    "Completion:\n",
    "{completion}\n",
    "\n",
    "Above, the Completion did not satisfy the constraints given in the Prompt.\n",
    "Please try again:\"\"\"\n",
    "\n",
    "NAIVE_COMPLETION_RETRY_WITH_ERROR = \"\"\"Prompt:\n",
    "{prompt}\n",
    "Completion:\n",
    "{completion}\n",
    "\n",
    "Above, the Completion did not satisfy the constraints given in the Prompt.\n",
    "Details: {error}\n",
    "Please try again:\"\"\"\n",
    "\n",
    "NAIVE_RETRY_PROMPT = PromptTemplate.from_template(NAIVE_COMPLETION_RETRY)\n",
    "NAIVE_RETRY_WITH_ERROR_PROMPT = PromptTemplate.from_template(\n",
    "    NAIVE_COMPLETION_RETRY_WITH_ERROR\n",
    ")\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class RetryWithErrorOutputParser(BaseOutputParser[T]):\n",
    "    \"\"\"Wraps a parser and tries to fix parsing errors.\n",
    "\n",
    "    Does this by passing the original prompt, the completion, AND the error\n",
    "    that was raised to another language model and telling it that the completion\n",
    "    did not work, and raised the given error. Differs from RetryOutputParser\n",
    "    in that this implementation provides the error that was raised back to the\n",
    "    LLM, which in theory should give it more information on how to fix it.\n",
    "    \"\"\"\n",
    "\n",
    "    parser: BaseOutputParser[T]\n",
    "    \"\"\"The parser to use to parse the output.\"\"\"\n",
    "    # Should be an LLMChain but we want to avoid top-level imports from langchain.chains\n",
    "    retry_chain: Any\n",
    "    \"\"\"The LLMChain to use to retry the completion.\"\"\"\n",
    "    max_retries: int = 1\n",
    "    \"\"\"The maximum number of times to retry the parse.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        parser: BaseOutputParser[T],\n",
    "        prompt: BasePromptTemplate = NAIVE_RETRY_WITH_ERROR_PROMPT,\n",
    "        max_retries: int = 1,\n",
    "    ) -> RetryWithErrorOutputParser[T]:\n",
    "        \"\"\"Create a RetryWithErrorOutputParser from an LLM.\n",
    "\n",
    "        Args:\n",
    "            llm: The LLM to use to retry the completion.\n",
    "            parser: The parser to use to parse the output.\n",
    "            prompt: The prompt to use to retry the completion.\n",
    "            max_retries: The maximum number of times to retry the completion.\n",
    "\n",
    "        Returns:\n",
    "            A RetryWithErrorOutputParser.\n",
    "        \"\"\"\n",
    "        from langchain.chains.llm import LLMChain\n",
    "\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        return cls(parser=parser, retry_chain=chain, max_retries=max_retries)\n",
    "\n",
    "    def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:\n",
    "        retries = 0\n",
    "\n",
    "        while retries <= self.max_retries:\n",
    "            try:\n",
    "                return self.parser.parse(completion)\n",
    "            except OutputParserException as e:\n",
    "                if retries == self.max_retries:\n",
    "                    raise e\n",
    "                else:\n",
    "                    retries += 1\n",
    "                    completion = self.retry_chain.run(\n",
    "                        prompt=prompt_value.to_string(),\n",
    "                        completion=completion,\n",
    "                        error=repr(e),\n",
    "                    )\n",
    "\n",
    "        raise OutputParserException(\"Failed to parse\")\n",
    "\n",
    "    async def aparse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:\n",
    "        retries = 0\n",
    "\n",
    "        while retries <= self.max_retries:\n",
    "            try:\n",
    "                return await self.parser.aparse(completion)\n",
    "            except OutputParserException as e:\n",
    "                if retries == self.max_retries:\n",
    "                    raise e\n",
    "                else:\n",
    "                    retries += 1\n",
    "                    completion = await self.retry_chain.arun(\n",
    "                        prompt=prompt_value.to_string(),\n",
    "                        completion=completion,\n",
    "                        error=repr(e),\n",
    "                    )\n",
    "\n",
    "        raise OutputParserException(\"Failed to parse\")\n",
    "\n",
    "    def parse(self, completion: str) -> T:\n",
    "        raise NotImplementedError(\n",
    "            \"This OutputParser can only be called by the `parse_with_prompt` method.\"\n",
    "        )\n",
    "\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return self.parser.get_format_instructions()\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"retry_with_error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RetryWithErrorOutputParser\nparser\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 85\u001b[0m\n\u001b[1;32m     76\u001b[0m replanner \u001b[38;5;241m=\u001b[39m create_openai_fn_runnable(\n\u001b[1;32m     77\u001b[0m     \n\u001b[1;32m     78\u001b[0m     functions,\n\u001b[1;32m     79\u001b[0m     OllamaFunctions(llm\u001b[38;5;241m=\u001b[39mllm),\n\u001b[1;32m     80\u001b[0m     replanner_prompt,\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     83\u001b[0m chain_response \u001b[38;5;241m=\u001b[39m replanner\u001b[38;5;241m.\u001b[39minvoke(state)\n\u001b[0;32m---> 85\u001b[0m retry_parser \u001b[38;5;241m=\u001b[39m \u001b[43mRetryWithErrorOutputParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# retry_parser = RetryWithErrorOutputParser(parser=output_parser, retry_chain=replanner, max_retries=2)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m output \u001b[38;5;241m=\u001b[39m retry_parser\u001b[38;5;241m.\u001b[39mparse_with_prompt(chain_response, prompt_value\u001b[38;5;241m=\u001b[39mreplanner_prompt\u001b[38;5;241m.\u001b[39mformat_prompt(state))\n",
      "Cell \u001b[0;32mIn[3], line 180\u001b[0m, in \u001b[0;36mRetryWithErrorOutputParser.from_llm\u001b[0;34m(cls, llm, parser, prompt, max_retries)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[1;32m    179\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for RetryWithErrorOutputParser\nparser\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "# from langchain.output_parsers import PydanticOutputParser, RetryWithErrorOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers.openai_functions import (\n",
    "    JsonOutputFunctionsParser,\n",
    "    PydanticAttrOutputFunctionsParser,\n",
    "    PydanticOutputFunctionsParser,\n",
    ")\n",
    "from typing import Union, Dict, Type\n",
    "\n",
    "functions = [Plan, Response]\n",
    "\n",
    "pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "\n",
    "\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def output_parser(response) -> BaseOutputParser:\n",
    "    output_parser = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    return output_parser.invoke(response)\n",
    "\n",
    "template =  \"\"\"For the given user input, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    \n",
    "      if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' key,\n",
    "        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' key,    \n",
    "        if the user makes conversation, jokes and funny conversations then use 'General_conv' key,\n",
    "        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' key,\n",
    "        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' key.\n",
    "    \n",
    "    \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "user input was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly(remove the completed step). If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\n",
    "\n",
    "only provide the final answer after make sure the user requirement has been satisfied completely.\n",
    "\n",
    "make sure the tool name is either 'Plan' or 'Response'\n",
    "\n",
    "while providing response make sure the user input is satisfied with the response refer the follow steps to gather the necessary informations for the final response.\n",
    "\"\"\"\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \n",
    "\n",
    "    template=template,\n",
    "#    partial_variables={'format_instructions': output_parser.}\n",
    "   )\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=os.environ['LLM'], stop= [\n",
    "        os.environ['LLM_START_PARAM'],\n",
    "        os.environ['LLM_STOP_PARAM']\n",
    "    ]\n",
    "                 )\n",
    "\n",
    "\n",
    "replanner = create_openai_fn_runnable(\n",
    "    \n",
    "    functions,\n",
    "    OllamaFunctions(llm=llm),\n",
    "    replanner_prompt,\n",
    ")\n",
    "\n",
    "chain_response = replanner.invoke(state)\n",
    "\n",
    "retry_parser = RetryWithErrorOutputParser.from_llm(llm = llm, parser=output_parser, max_retries=3)\n",
    "\n",
    "# retry_parser = RetryWithErrorOutputParser(parser=output_parser, retry_chain=replanner, max_retries=2)\n",
    "\n",
    "output = retry_parser.parse_with_prompt(chain_response, prompt_value=replanner_prompt.format_prompt(state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Generation\ntext\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3961\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3969\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3970\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3971\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1625\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1622\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1623\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1624\u001b[0m         Output,\n\u001b[0;32m-> 1625\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1635\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3835\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3833\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3835\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36moutput_parser\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129m@chain\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutput_parser\u001b[39m(response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseOutputParser:\n\u001b[1;32m     28\u001b[0m     output_parser \u001b[38;5;241m=\u001b[39m PydanticOutputFunctionsParser(pydantic_schema\u001b[38;5;241m=\u001b[39mpydantic_schema)\n\u001b[0;32m---> 29\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:89\u001b[0m, in \u001b[0;36mBaseGenerationOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m     82\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1625\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1622\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1623\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1624\u001b[0m         Output,\n\u001b[0;32m-> 1625\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1635\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:90\u001b[0m, in \u001b[0;36mBaseGenerationOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m     82\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m]),\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     92\u001b[0m         config,\n\u001b[1;32m     93\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Generation\ntext\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "output_parser.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.output_parsers import RetryWithErrorOutputParser\n",
    "     \n",
    "\n",
    "retry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=OpenAI(temperature=0))\n",
    "     \n",
    "\n",
    "retry_parser.parse_with_prompt(bad_response, prompt_value)\n",
    "     \n",
    "\n",
    "Action(action='search', action_input='who is leo di caprios gf?')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Plan(steps=[Step(key='Mediwave_rag', value='Explain about Mediwave (remaining part)'), Step(key='General_other', value='Get current time')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about mediwave and also give the current time',\n",
       " 'plan': ['Explain about Mediwave', 'Get current time'],\n",
       " 'past_steps': ('Explain about Mediwave',\n",
       "  \" Claro: I'd be happy to help explain a bit more about Mediwave Digital! They are a company dedicated to addressing the world's needs by empowering over 50 brands to thrive and make a meaningful impact. As a design-thinking company, they focus on creating not only digital products and services but also transformative experiences.\\n\\nTheir team consists of talented UI/UX designers, full-stack developers, QA engineers, and DevOps programmers who collaborate from conceptualization through development, testing, and deployment. They emphasize a pursuit of excellence and innovation in all their projects.\\n\\nMediwave Digital has a strong focus on healthcare technology and clinical research, combining user-centered design with agile development. One of their notable achievements is their collaboration with Sefton & Liverpool CAMHS services to promote mental health and well-being for children, young people, families, and carers through a dedicated platform. Their tech stack includes Node.js, MangoDB, Apostrophe CMS + Node.js, PostgreSQL, and Sequelize.\\n\\nThey pride themselves on their collaborative efforts and the impact they make with their work. Matt Rigby, Head of Digital at Brook, expressed his gratitude for Mediwave's role in their team and project DFD. Overall, Mediwave is a culture-driven design company that thrives on tackling big challenges and making a tangible difference with their work. If you are interested in joining them on their mission to assist companies in delivering delightful customer experiences, they offer opportunities for Full Stack Developers.\"),\n",
       " 'response': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"plan\": [\n",
    "    {\n",
    "      \"key\": \"General_conv\",\n",
    "      \"value\": \"respond with 'good morning' to user\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'input': 'tell me about mediwave and also give the current time',\n",
    " 'plan': [\n",
    "    {\n",
    "      \"key\": \"General_conv\",\n",
    "      \"value\": \"respond with 'good morning' to user\"\n",
    "    }\n",
    "  ],\n",
    " 'past_steps': [],\n",
    " 'response': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'plan': [{'key': 'Mediwave_rag', 'value': 'Get information about Mediwave'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'name': 'Response', 'arguments': {'response': 'Mediwave Digital is a healthcare technology and clinical research company that combines user-centered design with agile development. They have been instrumental in helping organizations achieve customer satisfaction throughout the customer journey. Some of their featured projects include Medichec, which helps identify medications that could potentially impact cognitive function or cause other adverse effects in older individuals, and Oxcare, a digital portal designed for supported self-management of physical health and mental wellbeing. Their tech stack includes various technologies such as Angular, Node.js, Mango DB, Cordova, Express.js, Apostrophe CMS, and PostgreSQL, among others. They take pride in collaborating with exceptional individuals and making a tangible impact with their work. For more information about their clients and testimonials, you can check out their website.'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from pydantic import BaseModel \n",
    "\n",
    "class Steps(BaseModel):\n",
    "    key: str = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Steps] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Plan': [{'key': 'Mediwave_rag',\n",
    "#    'value': 'Retrieve information about Mediwave'},\n",
    "#   {'key': 'General_other',\n",
    "#    'value': 'Format the retrieved information for user consumption'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the 2024 Australia open winner?\"}\n",
    "async for event in graph.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch - supervisor and agent planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "class PlanItem(BaseModel):\n",
    "    key: str = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    plan: List[PlanItem] = Field(description=\"different steps to follow, should be in sorted order, always make minimal steps\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "\n",
    "\n",
    "openai_function = convert_pydantic_to_openai_function(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crews = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = {'name': 'Plan',\n",
    " 'description': '',\n",
    " \n",
    " 'parameters': {\n",
    "   'type': 'array',\n",
    "   'properties': {\n",
    "     'key': {\n",
    "       \"enum\": f\"{crews}\",\n",
    "       'description': 'the worker gonna handle this task/step',\n",
    "       'type': 'string'},\n",
    "     'value': {\n",
    "       'description': 'task/ step the worker need to do',\n",
    "      'type': 'string'}\n",
    "     },\n",
    "                    \n",
    "    'required': ['plan'],\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict\n",
    "# from pydantic import BaseModel\n",
    "\n",
    "# class Parameter(BaseModel):\n",
    "#     key: str\n",
    "#     value: str\n",
    "\n",
    "# class FunctionParameter(BaseModel):\n",
    "#     type: str\n",
    "#     properties: Dict[str, Parameter]\n",
    "#     required: List[str]\n",
    "\n",
    "# class FunctionSchema(BaseModel):\n",
    "#     name: str\n",
    "#     description: str\n",
    "#     parameters: FunctionParameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict, Optional\n",
    "# from pydantic import BaseModel, Field\n",
    "\n",
    "# class Plan(BaseModel):\n",
    "#     name: str\n",
    "#     description: Optional[str] = Field(default='', description='Description of the plan')\n",
    "#     parameters: List[Dict[str, str]]\n",
    "\n",
    "#     class Config:\n",
    "#         json_schema_extra = {\n",
    "#             \"example\": {\n",
    "#                 \"name\": \"Plan\",\n",
    "#                 \"description\": \"Description of the plan\",\n",
    "#                 \"parameters\": [\n",
    "#                     {\n",
    "#                         \"key\": \"worker1\",\n",
    "#                         \"value\": \"Do task A\"\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"key\": \"worker2\",\n",
    "#                         \"value\": \"Do task B\"\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "Plan.schema_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[ Dict[\n",
    "                key: str = Field(description='the worker gonna handle this task/step')\n",
    "                value: str = Field(description='task/ step the worker need to do')\n",
    "    ]\n",
    "                ] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "Plan.schema_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plan.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "    \n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given user input, come up with a simple step by step plan but don't provide answer coz you have tools to figure out things. \\\n",
    "        \n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    \n",
    "    if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' worker,\n",
    "        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' worker,    \n",
    "        if the user makes conversation, jokes and funny conversations then use 'General_conv' worker,\n",
    "        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' worker,\n",
    "        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' worker.\n",
    "    \n",
    "    \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "if the given objective related to mediwave then give the objective as plan\n",
    "\n",
    "user input : {objective}\"\"\"\n",
    ")\n",
    "\n",
    "planner = create_structured_output_runnable(\n",
    "    function, \n",
    "    OllamaFunctions(model=os.environ['LLM']),\n",
    "    planner_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_step(input):\n",
    "    plan = planner.invoke({\"objective\": input})\n",
    "    # return {\"plan\": plan.steps}\n",
    "    h = {\"plan\": plan}\n",
    "    print(h)\n",
    "\n",
    "    return {\"plan\": plan['plan']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_step(input=\"Tell me about mediwave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner.invoke(\"tell me about mediwave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apr 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rag_weaviate.plan_and_execute.graph import graph1\n",
    "\n",
    "from plan_and_execute.graph import graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS8AAAMhCAIAAAA2KaDWAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdd1QU59cH8Dtb6U16b4IUBQUVBDugYuyigoI1zRJjjzEmJib5GWOMedM0GluiEXsHARVsgIoFUTpSpXcW2P7+sRtQxKiwuzO73M+ZwxmG4Zm7zH6Z2SnPEGKxGBBCFEAjuwCEkBSmESGqwDQiRBUMsgtASqy2tra5uZnD4TQ2NgqFwoaGhrYfcTgcHo/X9q2Ojg6dTpeMM5lMLS0tBoOhra2to6Ojqamprq6u6NIpCdOIOhIIBKWlpYWFhRUVFZWVlZWVlVVS1RWVFTU1NQ0NjS0tzS3NzbJaIkEQurp6Gpoa2trahoZGxkaGhoaGxsbGhoaGvXr1MjQ0tLKysrS01NPTk9USqYnAY6o9lkgkKiwszM7OzsrKys/PLy4uLigsLCgoKC8rEwqFknk0tXX0ehnq6Bto6Rto6+lr6/fS1tPX0NJiq6uz1NS1dPTY6uostpqGtjYAaOroAkFIfpHFYrPU1NqW1dRQ3zbOa23lc1uFAkFLM6e5sYHb2sJraeE0NrQ2N7dymhpqaxpqaxprqxtqqhvrautravg8ruQXtbS0La0sbaxtrK2trKysHBwcnJycnJycdHR0FPQnkzNMY08hEAgyMjLu37//5MmTrOzszMzMnJwcbmsrAOjoG5hYWBmYmhmaWRiaW/QyMTc0Mzcyt9Tt1YvOYJJdODQ3NdaUlVaWllSXlVaVPassKa6tKK0ufVZeXMTn8wDAyNjY2blPH2cnJycnDw8PT09PY2NjsqvuCkyjyuLxeCkpKffv33/w4EHKvXuPHz/mtrYyWWwrB0dTG3szGztzO3tzOwczG3ttPX2yi+0KoVBQWVL8LD+vJC+ntOBpWUFeSV5OVVkpAJiamQ3oP6B/f09PT8/BgwdbWVmRXewbwTSqlMbGxuTk5Bs3bly/cePWrVutLS2aWtrWzi72rn2tejtbOjg5unsw2Wyyy5QjTkNDYXZG3uPU3Mep+U8eFeXliIRCE1Ozgd5e/v7+fn5+gwcPZjLJ3+B3CtOo9Hg83vXr16Ojo6Oio588fgwAVg69nfp7u3gNcu4/0MzGjuwCycRtac5Je5h+Nznr/t2M+3c5jQ3a2jrDhw8fN27s2LFj7e3tyS7wBZhGZVVSUnLu3LmLUVFXrlzhNDVZOzp5DB3pPmiIc39vJd3zlDexSFSYk5l+Nzn11rVHSTeam5oce/ceHxwcHBw8cuRIKmwwMY1Kpqam5vz585FHj166dInJZDr3H+g1ImBQwFgjc0uyS1MmIqHwacbju1dj78fH5TxO1dXVmzDhnZCQkHHjxjEYpJ32wzQqBx6Pd/z48b1798XHX2Wy2d4jg/zGTew/dKRqfwhUjIriwpsXz96KOpuXnmZsYjJr5swPPvjAxcVF8ZVgGqmuqKho165df+zeU1NT7T0iwH/8ZK+RgWw1vHhF9p49zb158cy1M8dLiwpGjBixZMmSSZMmKXJTiWmkrkePHn351VenT53SNeg1OmR20MxwAxNTsotSfWKR6P6N+Jh/DqQkXDYxNV2zevUHH3ygmGv3MI1UlJGR8cWmTcePHbN1dp20aIlPUDCDAscYepqK4sKoQ/tjIg/q6epu+PTTd999ly3nzwWYRmqpra1ds2bN/v37Le0dZyxbPTgwmPj3WjNEirrqylN//BJz5KCxsclPO36cOnWq/JaFaaSQkydPLl68RCAWh6/93D94EkHD+92ooqa87J//++7qyaOTJ0/59ddfzMzM5LEUTCMlNDU1LVi48PixY6OnhUas/VxTVS6DVjGpt67/8cXalsaG3bv/mD59uszbx/++5CsqKvLz8798Nf6LvZEffr1N6aL405ql0/qY342PpWBrstVvyNAfzl4ePG7ijBkzvv32W5m3j/c3kiwlJeWddyao6+n/7+gFQzMLsst5a3XVlbeiz1GzNXlgq2u898X/rHs7f/7F55lZWXv//LPtLuruwz1VMj158mTo0GE2bn1X7fhDXVOL7HI6l3n/7tm9O/OePKqtLNfU0bNxdhkfvtBrRAAAfB4+7fGdxOdnXr/zgPeIQAB4mv749J5fsx6k1FZV6BsaO/f3Dl2+1sTKRjLbtuXvJV46z2AyD97J+GX9ivvXroQuX5scG/Wq1ijowY34rUsXLFm8+IcffpBVm7inSpqmpqbJU6aa2Nqv/XkvZaOYFHtxQ9ikpNiLDbU1RuaWfF7rw5sJ334QEXVoHwAYmJjq6BtI5tQ3MjG1tlVT1wCA7If3NoROvHHhdH11lbmtfW1VxfXzp1ZNCSorzJfMzFbXAAABn3/stx9vRZ1t4TS1cJpe1Ro1efqPWPLtjh9//HHfvn2yahPTSJoNGzZUVFWt3PHH8/fIU83ZP38Xi8UO7h4Hkp/8HH1jf+LjYROn6fYyvH05WiwWf7zt1xlLV0nm/GDz1l9jbrkP9gOAo79u57a2AMD/Is9vP3P5q4PHAaClqfHc/j8kM9MZ0r272Mi/Jy1cvHL7zv5DR76qNcryC5447YPlH3z4YW5urkwaxM+N5Hj69Olvv/323qbvDIxNyK7lv3AaGwCA01BfVVpiam1Lo9OXb/35tb81/9OvZixdxeO22ji7iEUie9d+dDpDKBQUZmd0mHPYhKkRaz6TS+kKMWPpyjtxUZ9t3PjP4cPdbw3TSI49e/boGxqPmBJCdiGv4TFkWHFudllh/pKgIabWtn0GDPTwGz5wVNB/71rrGxlfO3cyOeZieVGBZCMpwX+uFzmJQQFj5VK3otDpjMnvLv11w8qfduzofvcfuKdKjqjo6EFBwXQ61f8bzlm1YdS0WZI6ywrz408f+2nN0vdHej+8mfCqXxGLRF/On3ns1+2F2RnuPn7T3v9o1rLVr3ql+kaU3jV4E4MDx4EYrly50v2mMI3kyMjIsHNxI7uK12OpqS35ZvufNx+u3L5zfPhC2z6uAMBpaNiyZH5DTXWnv/L4TmJ26n0A8B8/+dOdB8NWfDL1/WUikbDTmVXg+lu2uoaFnX16enr3m8I0kkAkErW2tKhpaJJdyJvS1tP3C564YMPmH07HLfzsawDgtbYWZL7w/hOLpKfKKkqKJSPWTn0kI4+Sbr7tibS21pSCuqZWU1NT99vBNJKARqPpG/Sqq6oku5DXqKuq2BA2aYFfv4t/722b2Na7qb6RMQC03WmZ+eCuZKTXv7d9PUq8IRTwy4sK/vxmo+Sa2/rXveSXW1MKtZUVMukzEtNIjoHe3ul3k8mu4jX0DI0NjE3rq6v+/PqzhX4eH78zcqG/58GtmwHAL3iipaMTANi5uEtmPvXHL/N83KIP73fxGizpFuRR0o3ZXk6LA30BYPKixQBQUVL08Tsjc9MevmqJL7cm31coC5XPistLigYOHNj9pjCN5AgJmX7nyqX66iqyC3mNFT/8Fr76Mwd3D25rS8nTHIIg3Ab6Lt/688ff/yqZwc7Vfc6qDXqGxgwmU11Ty9DMgqWm9tnuQx5+wzW0ddTUNUZMDvnm0OlJCz4cMHy0nqGxGMQMFutVi3u5NUW90K6LO3bY2MTU39+/+03hlXHk4HA4jr17ewwPfG/TFrJrQV1XW1n+0bihG9av//TTT7vfGm4byaGpqfnDtm0xkX/duyaDI+OIFCKh8Od1y01NTFasWCGTBnHbSKbwiIhTp8989ddJyZkDGXr2NPfXDSv/e4bGulrn/t7/MU/Emo3/PYO8UfxV7P5y/dVTkTdv3PDy8pJJg5hGMnG53HHjglPu31/98x63gb5kl4PelFDA/2PTJ1dPHT1+/PjkyZNl1SzuqZKJzWZHRV0cHzxu84LQ+NNHyS4HvZEWTtN3i+cnRp07deqUDKMIeJ0q6dhs9qG//7a1sdmyfkXGvTsRazZqaCvZvf89SlryzZ0b1xBCwc2bNzw8PGTbOO6pUsWJEycWL14iJIhFn/9v0OgxZJeDOuI0NBz8/qvLx/+ZMGHirl07TU1l37ctppFCampqVqxcefDAAe+RgbM+WqsUF7L2BAI+P/bo3yd3/sSg0X75+f9CQuR15w2mkXLi4uI++WT9vXspPkHBM5ettnJ0Jruinkso4F89dezE7zvqa6o+eP/9L774Ql9fjs//wjRSkVgsPnv27GcbNz55/HjgyKAxs+f18x2K3RwrUkNtzZUTR2KOHKipKF+0cOGnn35qaSn3p4BhGqlLJBKdOnXqp//7v+vXrlnaOQTOihg5ZabS9e+odLIf3ov+58CtqLPqaurz5s39+OOPbW1tFbNoTKMSyMzM3Ldv386du5pbWjz8hvmOeWdw4DjKdmylpCpKim5ePHvtzNHCnOw+fVw++OD9RYsWaWoq9K43TKPSaGhoOH78+D//HLl69YrkEY6+Y9/xGDIMY9kdxbnZd67E3Lp4Ji89zcTUdOaMGaGhoT4+PqQUg2lUPhUVFcePHz9yJPLmzRs0Gt3Fa6CH/8j+Q0faOLvgZ8s30cJpepR44/71qw9vxJeXFBkY9Jo+fdqsWbOGDRsmw66KuwDTqMSqqqpiYmKio6OjoqOrKit7GZu4Dh7i7Ont6u1j5ehEI/WNRTVNDfWZ9+5k3LuTee925oN7IpFwgJd38LixY8eOHTRoELkhbINpVAVisfjevXsxMTHXrl2/detWQ0O9pra2c/+BTp5eDu4edi5uKtAZ1NsSCvhFOdn5GWlZD+5l3LtdlJMlFot7OzsP8/cfNWpUYGCgoaEh2TV2hGlUNSKRKC0t7fr167du3bp+40ZRYSEA6Bsa2bq42bq42/Vxt3ZyNrW2Y776ll8lVV9dVZyXnZ/xJD89rTAzvSArg8/nsdXU+vcfMNTfz9/ff8iQIRRM4PMwjSqupqbmwb/u3b+fmZEhEAhodLqJhaWZrb2ZrYOFnYOZjb2xpVUvU3NliWhDbU116bPSwqel+XklT3PL8nNLnuY1NdQDgIFBL09Pj/79+3t6enp6evbp04fBUJqLsTGNPQuXy83MzMz615P09JzsnNraGslP9Q2NDE3N9U3MjMwtepma6Rkaa+sb6Ogb6BkaaevpsxX1VAyxWNxYW9NQWyP5WlddWVtZUVlSXFtRWlNWWvGsmNvaCgB0Ot3axsbZydnFpY+Tk5OTk5Ozs7OFhRJ03vEqmEYEVVVVBQUFxcXFBQUFRUVFxcXFBYVFhYUFlZWVPC63bTa2mpquQS8dfQO2hgZLTV1dU1tNU5Otpq6moaGprQsEwWAy1TSkiaXR6Opa0lMvQoGgtZkjGReJRM2NjQDA47ZyW1qaGxtamzm81pbW5uaWpobW5uaG2uqG2lqRSNS2XD09fTNzMxtrGysrS0tLSxsbG0tLSysrK1tbW5aSbMzfEKYR/ZfGxsbKysrKysqq53A4nObm5oaGhoaGxiYOp7mZU1tbCwAtLS3cVml6eXxeM0eaQBqNpqOj29amjq4OjUZTU1PT0NDQ19PT1NLS0tTU0tLS1dXV1NQ0NDQ0MjIyNjY2NDTs1auXoaEhU/l7QH5DmEYkR1ZWVitWrFi58r9600Bt8N5/hKgC04gQVWAaEaIKTCNCVIFpRIgqMI0IUQWmESGqwDQiRBWYRoSoAtOIEFVgGhGiCkwjQlSBaUSIKjCNCFEFphEhqsA0IkQVmEaEqALTiBBVYBoRogpMI0JUgWlEiCowjQhRBaYRIarANCJEFZhGhKgC04gQVWAaEaIKTCNCVIFpRIgqMI0IUQWmESGqwDQiRBWYRoSoAtOIEFVgGhGiCkwjQlSBaUSIKjCNCFEFphEhqsA0IkQVmEaEqALTiBBVYBoRogpMI0JUgWlEiCowjQhRBaYRIarANCJEFZhGhKgC04gQVWAaEaIKTCNCVIFpRIgqMI0IUQWmESGqwDQiRBWYRoSoAtOIEFVgGhGiCkwjQlSBaUSIKjCNCFEFphEhqsA0IkQVmEaEqALTiBBVYBoRogpMI0JUgWlEiCowjQhRBaYRIarANCJEFZhGhKgC04gQVWAaEaIKTCNCVIFpRIgqMI0IUQWmESGqwDQiRBWYRoSogkF2AUil7NmzJycnp+1bsVgcFxdXUVHRNmX+/PnOzs5klKYECLFYTHYNSHV89913n3zyCYvFIgiiw48EAgGdTq+srNTR0SGlNurDNCJZKiwstLW17fRNxWAwJk6ceOLECcVXpSzwcyOSJWtr64EDB9JonbyvhELhnDlzFF+SEsE0IhmLiIh4eTcVANTV1ceOHav4epQIphHJ2IwZM16eyGQyZ86cqa6urvh6lAimEcmYkZHRqFGj6HT68xP5fH5YWBhZJSkLTCOSvTlz5nQ4kKOvrz9ixAiSylEamEYke1OmTGEw2k9ls1is8PDw56egTmEakexpa2tPmDCByWRKvuXxeKGhoeSWpBQwjUguZs+eLRAIJOOWlpaDBw8mtx6lgGlEchEcHKypqQkALBZr7ty5nZ7zQB1gGpFcsNnskJAQGo3G4/FmzZpFdjnKAdOI5CUsLEwkEvXp08fd3Z3sWpQDXqeK3giHw3n27Fl5eXl5eXlpaWlFRUVdXV1DQ0NDQ31jY21tbXVDQ4NQKKyra5C8o0QicX09R/K7TCZdS0t63l9Nja2urqahoaGjo6ujo6+jo6+np6erq6uvr29mZmZsbGxubm5iYmJsbNzhjGVPgGlEL+Dz+bm5uXl5ef9+zc7Ly8zPL+ZwWtvmMTZmGhvT9PXF2tpCHR2hjg7o6YGODjAYoK0NbScy9PXhwAEIDgZtbWhpkU7kcIDHg6YmaGiAxkZoaID6emZ9Pa26GsrKBM3NQslsNBphYmJgb29vb9/H3t7ewcHB3t6+d+/exsbGCv1zKBamsacrKipKTU1NS0tLTX2YlnY/IyOHxxMAgKEh08GBsLfn2duDnR2YmoKpKZiZgbExvPmJw8pKMDJ6i2KamqCkBCoqoLQUSkogLw9yc+l5eYz8fD6XKwIAIyO9fv083N093d3dPTw83NzcNDQ03vo1UxWmscdpbW1NSUlJSkq6detGYuKN0tIqALC2Zrm7C/v2FfbtC66u4OAAlLoJUSSC4mLIyoK0NEhLg9RU1pMnQg5HyGDQPT3dfH2H+/j4+Pr62tnZkV1pt2AaewShUHj37t2YmJjY2IvJyXd5PIGJCdPHRzhkiMjHBzw8QFeX7BLfkkgET5/CnTuQlASJiaz79wV8vsjMzHD06DFBQWMCAwNNTU3JrvGtYRpVWV1d3ZkzZy5cOH/5ckxNTYOFBSsoiD9qlNjXFxwcyC5OplpaICUFrl+H2Fj6zZtiPl/cr1+foKB3pk6dOnjwYGU524lpVEF1dXVnz549evSf2Ng4ghCNGkUEBQmDgsDVlezKFILDgYQEiImBixdZ2dk8a2vT6dPDQkJCqB9LTKPqEIvF8fHxu3b9fvr0aQBhUBAREiKcOFH59kJl6OFDOHYMjh1jZWXx7O2tFi36cP78+dTdiRUj5VddXf399987OdkBgK8vc+9eqKsDsRiH9uHBA/j4YzAwYDCZ9OnTp8TFxZG90jqBaVRulZWVX3zxhZ6elo4O47334P598t/3VB5aW+HoUQgIYBIEeHi4HT16VCQSkb0O22EalVV1dfXq1as1NdWMjZlbtkBDA/nvdSUa7tyBiRNpBAH9+7ufPXuW7JUphWlUPiKRaM+ePYaGeiYmzB9+gKYm8t/cSjo8eADTptEJAsaNC8rOziZ7xWIalU1qaqqPjzeDQfv4Y6K+nvw3tAoMCQnQty9TTY35+eef83g8ElcuplGZ7N27V12d5efHePhQ7u/RhQulx/mys8kPjLwHPh+2bwdNTbqv78DCwkKy1i/eUaUcWltbFyyYt3Dhgo8/5ickCPr1I7sg1cJgwIoVcPu2sL7+wYABfS9dukRKGZhGJdDS0jJp0vjTpw+dPQvffivueXcaKYirK9y+zR8zpnHChPGkPKEA00h1LS0tEyeOv3372qVLgnfeIbsaVaepCX/9JfrwQ+HMmTP++ecfBS8d00h177678P7961evCgYOlFmbM2YAQQBBQH09fP01ODiAmho4O8Mff7zmFx88gLAwsLMDdXWws4PZsyEvr/2nISFAEMBmAwDs2QMuLsBmg4MD/Pbb280DANXVsGoVODoCmw0GBjB+PCQnd9JIczPMnAk6OvDTT937izyHIGDHDli8WDx3bvjdu3dl1u6bIOsDK3oTx48fJwi4cEHGBy3mzZOufUmHNc/3x//nn688ipOUBJJ7CTU0oF8/UFMDANDRaZ9h7lzpr7ycjTNn3mKeykrpRe0sFgwaBObm0vHLlzs28skn0pHNm2X8JxKJYNw4uqOjTWNjo8JWN6aRuioqKvT1tZcuJWR+CLEtaSYm8PAhiERw6BBILqg2MwOhsPM0jhsnnSI5opuYKP128eKOzRobw5EjkJ8PK1ZIp4wY8RbzLFgAAMBgwJ07IBZDaysEBAAAODt3bMTAANasgSNHpHPKdiguhl69GB9/vFxhaxzTSF2ffPKJiQmTw5H9+6zt3fzDD+0Thw6VTrx3r/M0ZmZCcjLEx4NYDEIhcLnSTgCGDevY7JdfSqdwuaCvLw35G87D50u3wH5+7bWdPCn9Lcmlf22NLFsm+z/O88PvvwObzSwpKVHMGsfPjRQlFAr379/94Yd8uXY00ZZAAPD2lo4896TwF5iZwYULsHQpaGoCnQ5sNki6L+ZyO84ZGCgdYbHA0REAoLLyTefJzYXmZgCAmzelH24JAqZOlc6clvZCI5Mnv/Yldsv8+aCpKT506JB8F/MvfDQCRT18+LCsrLrtXSgnvXq1j0s2UADA4XQyp0gEAQFw+zYAwPjx4OEBbDZs3gz/9if+guf7wpH8NxG/dN/eq+ZpbJRONDEBT8+Ov9Xh7jAzs06WLkNsNkyYIIyOPr9mzRr5LgkAMI2U9ejRI3V1upubUK5Lqa1tH6+pkY5oaXUyZ0KCNIqhoXD4MAAAnw+bNsm+pLb+ePr1g+jo18z875M+5MjLS3zxYqrcFwMAeIaDsmpra/X16Z09sVuW2o7EAEBKinTE2bmTOfPzpSNtPRVfudLJFq/77O1BUxMAIC0NhPL9X/RGjIygpqZBMcvCNFKUhoZGc7NI3kvZulX6KfHoUbhxAwDA1hY67RncwkI6cvky8PmQlwfLl4Pkn0V5uSxLYjAgJAQAoLQUvv8eAEAohIULwcAAPDzaN+AKw+GAhgZbMcvCNFKUo6NjXZ2gtFS+S3F3h969QU8PZs6Ubui2boVO+44ZOhRsbAAArlwBHR3p+cC1awEA8vPB3R1keJ58yxbpstavB0NDMDKCvXuhthYiIsDAQGZLeUPp6eDoaK+YZWEaKWrgwIFqaszXfnDqpu3b4dNPQVsbWCzo1w8iI6XbpZepq0NUFAQFga4uaGpCRARcvw6rV0NwMJiaglgsvbxGJkxM4PZtWLYMbG2hsRHodAgIgHPnYNUqmS3izV28yBo2LEAxy8JeqqgrJGRqYeH55GS+zFtetAj+/BMAIDtbenYBdSohAUaMgOTk5EGDBilgcbhtpK516z69c0cgOYCJFE8ggLVrGSNHDlVMFAHTSGXe3t7Lli1dvJhRWEh2KT3S5s2Qlkb/7bfXXUovO5hGStuy5TsLC7uwMEanZ+SR/ERHw7ff0rZt+7FPnz4KWyh+bqS6J0+ejBw51NW18fx5vuREHJK3S5dg8mR6WFj4nj17Fdk9OW4bqc7V1fXy5YQnT7SDgxlVVWRX0wOcPg2TJ9NDQ+fs3v2ngp8UgGlUAu7u7pcvJxQUGA8YwExKIrsa1cXnw+rVMHUqLFjw7p49e2nyvhLqZYq5VQR1X3V19fjxY1ks2tatwOfL906iHjhkZcGQIQxNTbW//vqLrFWMaVQmIpHou+++U1Nj9u3LvHaN/HewagwcDnz2GbDZNE9PtydPnpC4fjGNyic7O3vcuCCCgDlz6Lm55L+blXcQCuHwYbC1Zerqau7YsYPP55O7ZjGNyurkyZOOjjYMBi0igpaRQf47W7kGPh8OHABnZyadTps7N7y0tJTs9SkWYxqVGp/P/+uvv/r0caDTiRkzaFeugEhE/hud4kN1Nfz4Izg4MBkM+ty5EZmZmWSvxnaYRqUnFAqPHDni4+MNAM7OrB9+gKoq8t/0FBxu3oSICEJNjaatrf7hhx/k5uaSveo6wrP/qiM9Pf3Agf27d/9eX9/k40OEhIjCwl7o8KJnevwYjh2DyEhWRgbP1dXpvfcWL1y4UKvTDg7IhmlUNRwO58SJE8eORcbGxorFwsBA2pQpgqAgsLIiuzIFEgggMRGiouD4cWZ2Nt/GxnT69NmzZs3ybuuKi5IwjSqrvr7+7Nmzx44duXz5cnMz18WFFRTECwyE4cM77/lGBWRlQWwsxMbSr1whGhsFDg5WU6bMDAkJGThwoIKvqukaTKPqEwgESUlJ58+fj4u7eO9eGo0Gzs4MLy++vz/4+YGra+c3+ysFPh9SU+HGDUhJoV+7xigo4Gpqqvn6DgkICAoICPDy8iK7wLeDaexZysrKrl27lpSUlJR07d69VC6Xb2jI9PQU9+0rcHeHfv3A1RXk2oNrNxUXw6NHkoF49Ij1+DFPIBCbmxv6+Pj7+vr5+voOHjyYwVDWnhAxjT0Xl8u9d+/e7du3U1NTU1PvPnmS2dzMpdEIOzuWg4PQ3l5gbw+Swda2vbdVxeDzobQU8vLaBiIvj5mVJaqtFQCApaVx376e/fr179+/v6+vr7W1tUKLkxtMI5ISCoV5eXkPHz7MyMjIzc3Ny8vMy8stKamUvEPU1GgmJgxzczA2FlhYiIyNQU8PdHRAWxv09EBXV9q5joZGewc5bQFubYWWFul4fT2IRNDQAI2N7V9ra6G6GsrLoayM8ewZvaJCVFEh7X9EQ4Ntb29tb+9sb+/o6Ojo7u7er18/fQX/b1AUTCP6L1wu9+nTpwUFBWX/Ki8vf/asoKKirL6+oQRypI4AACAASURBVKGB09TU8vpWXo1Op+noaOjp6RgYGJiaWpqYmJubmxsbG5uZmZmbm9vb25uamsrqtVAfphF1i0gkqq+vr6ura2hoEAgEjY2NAoEAAIRCYUNDw4cffjh+/PgpU6a0nd/T0tJiMplaWlo6Ojo6OjqaeAP1czCNSI6srKxWrFixcuVKsgtRDni3MUJUgWlEiCowjQhRBaYRIarANCJEFZhGhKgC04gQVWAaEaIKTCNCVIFpRIgqMI0IUQWmESGqwDQiRBWYRoSoAtOIEFVgGhGiCkwjQlSBaUSIKjCNCFEFphEhqsA0IkQVmEaEqALTiBBVYBoRogpMI0JUgWlEiCowjQhRBaYRIarANCJEFZhGhKgC04gQVWAaEaIKTCNCVIFpRIgqMI0IUQWmESGqwDQiRBWYRoSoAtOIEFVgGhGiCkwjQlSBaUSIKjCNCFEFphEhqsA0IkQVmEaEqALTiBBVYBoRogpMI0JUgWlEiCowjQhRBaYRIarANCJEFZhGhKgC04gQVWAaEaIKTCNCVIFpRIgqMI0IUQWmESGqwDQiRBWYRoSoAtOIEFVgGhGiCkwjQlSBaUSIKjCNCFEFphEhqsA0IkQVmEaEqALTiBBVYBoRogpMI0JUgWlEiCowjQhRBaYRIarANCJEFZhGhKgC04gQVWAaEaIKBtkFIJWSkZHB4XDavuXxeMXFxSkpKW1T7OzsDAwMyChNCRBisZjsGpDqWLJkyW+//fYfM+Tk5Dg4OCisHuWCe6pIlkJDQ1/1I4IgPD09MYr/AdOIZMnPz8/c3LzTH9Hp9Hnz5im2HCWDaUSyRBBEeHg4k8l8+UdCoTAkJETxJSkRTCOSsdDQUD6f32EijUYbPnz4qzabSALTiGTMw8PDycmpw0TJNpOUepQIphHJ3ss7qwRBTJ48max6lAWmEcleWFiYQCBo+5bBYAQHB+NpxtfCNCLZs7e379+/P0EQkm+FQuGcOXPILUkpYBqRXERERNDpdMm4mpra+PHjya1HKWAakVyEhoaKRCIAYDKZ06ZN09DQILsiJYBpRHJhbGw8bNgwGo3G5/P/4wId9DxMI5KX8PBwkUikp6cXGBhIdi3KAe/hQF1XVlZWWlpaVVVV86/GxsampibJ2f/q6moajdarV6/3339fcsJDW1tbTU3N4DmGhoY2Njbq6upkvxRKwHs40BspKip6/Pjxo0ePMjIyCgvzCgufFhaWtrbyJD+l0QgDA4aBAU1bG3R0RJLDN1pawjt3RE5OwGbTBQIaAHA4RHMzUVMjrqkRcjjCtsaNjPSsra2sre0dHHq7urq6u7u7urpqamqS8ULJhGlEnSspKUlMTExMTLx9++ajR2n19RwAsLBgubiIbG0F1tZgYwPW1mBuDkZGoK/feSO3boGvL/x7puMFXC7U1EBFBRQWQn4+FBZCYSGRk8NMTxe2tAgJgrCzM+/ff9CQIf4+Pj4DBgxQU1OT58ulBEwjaldZWRkTExMdHRUfH1tcXEGnE+7uTF9fnqcnuLmBm9srUydDQiHk5cGjR/D4Mdy5Q09OplVU8FkshpeXZ2Bg8NixYwcNGtR27kTFYBoRZGZmRkZGnj9/KiXlIZ1O+PvTR4/mDxkCAweClhbZxQHk5kJiIty4AdHRjIICgYGBdmDg2ClTpk2YMEHFTpxgGnuuoqKiI0eO/PPPwfv308zMmBMn8seOhdGjQVub7MpeLT0doqPh4kXG1atCdXW1iRMnhYbOHjt2LIOhEscjxajnuX79ekjIVAaDpq/PCA8nzp4FPh/EYmUaqqvhwAEICGDSaISpaa9169YVFRWR/XftLkxjD8Ln8/fu3evq6gQA/v7MyEjg8cjPVTeHp09h3Tro1YvBYjHmzAl78uQJ2X/mrsM09ggCgeDgwYOOjjZMJm3+fNr9++SnSLZDSwvs3Qtubkw6nTZnTmhWVhbZf/KuwDSqvoSEBDc3JwaDNm8eLTeX/OTIbxAK4fBh6NOHyWDQli1bWl9fT/bf/u1gGlVZVVXVggXzCIIIDmZkZZGfFsUMAgHs2QOGhgwLC6Pjx4+TvRLeAqZRZSUkJJiZGZqZMQ8cID8hih9qauCjj2g0GjF7dmhjYyPZa+ONYBpVkEgk+uabb+h02rRp9Lo68oNB4nDpEhgZMVxcHB8/fkz2ank9TKOq4fP5s2eHMpm0H38EkYj8PJA+FBWBnx9DW1sjISGB7JXzGnj2X6Xw+fywsFkXL545c0YYEEB2NZTB58Ps2bQLF5inT5+j8u1deH+jSpk3L+LSpbNRUaoTxTlzgCDg/PluNcJkwj//iKZP50+cOD4xMVFGpckeplF1bN++PTIy8uRJwbBhZJciI+XlcOyYbJqi02HfPlFgoHD69EllZWWyaVTmyN5VRrJx69YtBoO2datsPmvdvw+hoWBrC2pqYGsLYWHQ4URlUhIMHw4aGqCvDzNmQGkp+PgAADg6ts9TVQUrV4KDA7BYoK8PwcGQlNT+0+nTAQBYLBCLYfdu6NMHWCywt4dff5XOMHx4x/fquXPdfV11ddC7N3PECH+RSET2GusEplEViESiwYMHjB7NkMlhm6QkkNwaoaEB/fqB5L5CHR3IzpbOkJYGbfdOqKsDjQb9+oGdHQCAi4t0nspKkDyNisWCQYNA0uU/iwWXL0tnmDtX2sJPP3VM3ZkzIBZDWBgYGkqnmJmBgwNcuSKDV3f3LtBoxNGjR8leaZ3ANKqCyMhIGo2Q1fVu48ZJM/DwIYjF0PY5a/Fi6QzTpkmnrFwJPB5UV4Ofn3SKm5t0ngULAAAYDLhzB8RiaG0FyUdZZ2fpDAsXSn/F2BiOHIH8fFixQjplxAjpPD//LLOt4vNDRATNwcGay+WSvd46wjSqgmHDfGfMoMnqzZqZCcnJEB8PYjEIhcDlguR2pWHDpFMkXWRoa0Nzc/vm9Pk08vnSjaefX3uzJ09K55H812hL45dfSmfgcqV3M5uZyTeN+flAoxFnz54le711hEdxlF55efnNm8lhYSJZNWhmBhcuwNKloKkJdDqw2SDpxZ/LBQAoKQHJo8Td3aGtc6lBg+D5527k5kJzMwDAzZtAENJh6lTpT9PSXlhc2xkHFgscHQEAKitl9VI6Z2MDQ4Ywjh+X0QEi2VGJezR7tujoaDabCAqSTWsiEQQEwO3bAADjx4OHB7DZsHkztD1Wo6lJOvJ8rxwEAXp67SlqbJSOmJiAp2fHRejqvvCtkVH7uGSLKpb/KfDJk/lbtpyV+2LeEqZR6T19+tTWlqGuLnz9rG8gIUEaxdBQOHwYAIDPh02b2mdo68mtrq59olgMtbXt3+roSEf69YPoaJnUJWOurlBVVd/Y2KhNpZ4OcE9V6ZWUlFhayiaKAJCfLx1xd5eOSI5ktrGwkB5lffwYWlulE5OT4blnUoG9vTS0aWkglEVpIpnthktZWgIAFBcXy7jd7sE0Kj2CIGS4a2dhIR25fBn4fMjLg+XLgUYDACgvBwCg06Wf9OrrYeNG4PGgqgpWrXqhEQYDJM8ULy2F778HABAKYeFCMDAADw+oqXnTYtrOo8j8+hnJX4xGo9b7n1rVoC6wsLAoLpZZj4ZDh4KNDQDAlSugoyM9Z7h2LQBAfj64u8Pdu7BpE7BYAADbtoGODhgbg0AAxsYvtLNli7Sd9evB0BCMjGDvXqithYgIePPnOPbv396aoSH89lv3X59UUREAgEXb/x5qwDQqPTs7u/x8QUuLbFpTV4eoKAgKAl1d0NSEiAi4fh1Wr4bgYDA1BbEY2GwYMACiosDbG9hs0NWF+fMhKkq6M9l2ZNXEBG7fhmXLwNYWGhuBToeAADh3ruNW9L/17w9btoCpKbBYoK0NVlayeY0AkJEBRkZ6WlTooPI5eA+H0isvL7ewMD92TDRliuIWKhZDeTmYmEj7EW9uBi0t6ZUDFy8qrowu8/NjOjmF7tt3gOxCXoDbRqVnYmLi7+/7zz+KW5Vffgnq6mBmBgf+fTN/+630k9jYsQqrouvy8yEpSTBtWgjZhbyE7MsPkAwcPXqURiPu3ZPlBSv/MWRnt39KtLNrHx84sP3qHCoPc+bQHB1teDwe2eutI9xTVQVisdjPb7Ca2v3LlwWdPoJG5p4+hW3b4NIlePYMCAJ694bp02HVKqD+o9/u3AEfH+Lo0WPT2i63pQxMo4pITk4eOtRv82bhunVkl0JhdXUwcCDTxmZIXFw82bV0Aj83qojBgwdv3bptwwYiLo7sUqhKJILwcHpLi/6hQ5Fk19I53DaqlDlzws6ePXbunODlW3V7OIEA5s+nHT9Ov3r1mo/kzmjqwW2jStm378C4cZODg+kxMWSXQiU8HsyaRTt1inXu3EXKRhEwjSqGyWQePnxk+vTQd96hbd+uiJshqK+oCIYPZ8TGqkVFxQRQu/cuTKOqodPp+/cf3Lz523Xr6JLejXuyqCgYMIDR2GiXnJwydOhQsst5HXJPsCD5iY+Pl/T8f+QI+af4FD9UVMDcuTSCgPDw2U1NTWSvjTeCaVRl1dXVixYtJAhizBhGejr5CVHMwOfDH3+AgQHD0tLk5MmTZK+Et4BpVH3Xr1/v27cPnU5ERNBycshPi/wGoRD+/hucnJhMJv3jjz9uaGgg+2//djCNPYJQKPz777+dnOyYTNq8eTSFXUOnsKG5Gf78E1xdmXQ6LSJidk5ODtl/8q7ANPYgfD5/3759bm7OADBkCPPwYeByyQ9SN4fcXFi9GgwMGGw2Izx8dnp6Otl/5q7DNPZEd+/eDQ8PYzLpenqM8HDi7Fng8cjP1VsNVVWwaxf4+TEJAszMDNetW1dcXEz237W78FqcnqukpCQyMvKffw7evfvQxIQ5cSJ/zBgICOjYpxulpKVBdDRERTESEoQaGmqTJ0+bNSt0zJgxdLrMej8gEaYRQXZ2dmRk5IULp+/cuU8Q4OvLGD2aN2QIDB7c3vsbibKyIDERrl+HS5eYxcV8Q0PdwMCxU6dOHz9+vDr17xl5G5hG1K66ujouLi46Oio+PjY//xmdTri6Mn19ef37g4sLuLtDr15yr0EggNxcSEuDx4/hzh16UhKtqoqvpsby9u4fFDR+zJgx3t7eVOtdSlYwjahzpaWl0dHR69evNzbuVVRUWFfXBACmpixXV7GNDd/GBmxswNoaLCzA0BAMDODl+yp5PLh6FcaM6bz9lhaoqYHycigshIICKCiAwkIiN5eVns7nckU0GmFnZzFgwOAhQ/x9fHwGDBjAknSMpdIwjahzlZWVo0aNam1tjY+Pt7CwKCkpefz4cVpaWkZGRmFhXmHh0/z84pYWXtv8BgZMAwOari6oq4vV1MQAUFsrqKgQ+/hAQwNDKCQAoK6OxuVCTY24pkbY0tLe0aqpqYG1tbW1tYO9vYObm5ubm5uLi4tGW/+NPQamEXWirq5u9OjRVVVVCQkJtra2r5qtoqLi2bNnNTU1NTU11dXVNTU19fX1XC63ubkZAE6fPl1TUzNp0iRdXV0Gg0EQhJ6eHovFMviXvr6+kZGRtbW1mqS/5B4P04g6qqurCwwMLC8vT0hIsJM8lvHt5ebm9u7dWywWx8XFjR49WrYVqirV/DSMuqy+vn7MmDFlZWVXr17tchQBYOfOnQwGg8lknj9/XoblqTbcNqJ2DQ0NQUFB+fn58fHxffr06XI7PB7PxMSkrq4OACwtLYskPXuj18FtI5LicDgTJ058+vTplStXuhNFADhx4kR9fb1kvLi4OD09XRYFqj5MIwIAaG5ufuedd548eXLlyhVXV9dutvbrr7+2nRLEndU3h2lE0NLSMmHChMePH1++fNnNza2brWVmZt66dUv475PiBALBybaHjKP/hGns6Xg83vTp0+/duxcVFdW3b9/uN7hr1y4Go/0pvWKx+Pbt21VVVd1vWeVhGns0Ho83bdq0mzdvxsbGenl5db9BLpe7b98+Pp/fYXo0NR9xTDGYxp6Lx+OFhIRcv349JibG29tbJm0eO3as7fhNGxqNdvbsWZm0r9rwDEcPxefzZ8yYERsbGxUVJcPO1Hx9fe/cuSN86fHimpqaNTU1PeFa0+7AbWNPJBQKIyIiYmJiLly4IMMopqenJycnvxxFAOBwODdu3JDVglQVprHHEQqFc+fOPXv27Pnz54fL9AkBu3btetWP8DzHm8A09iwikWj+/PknT548d+7cyJEjZdtycnKyiYmJlpbWy3fi8/n8M2fOyHBxKonx+lmQqhCLxR9++GFkZOTJkydHjRol28ZpNFpiYmLbt3fu3Bk0aFBCQoKxsTGHw6mvr29paeHz+UwmU7bLVSWYxp5CLBYvXrx4//79x48fHz9+vLwX19jYCABubm69FNBfgKrANPYIYrF46dKlf/755/HjxydMmKCAJdbW1hIEoUvlHq+oB9Oo+sRi8fLly3ft2nXo0KGJEycqZqF1dXXa2trPX5SDXguP4qi+9evX//bbb3/99dfMmTMVttC6ujo9PT2FLU41YBpV3Keffrpt27YDBw6EhoYqcrn19fWYxreFOxKqbOPGjd99993+/ftnz56t4EXX1tZiGt8WplFlbdq06Ztvvtm5c2d4eLjil457ql2Ae6qqadu2bV999dWvv/763nvvkVJAXV2dvr4+KYtWXphGFbR9+/a1a9f+/PPPH374IVk14LaxC3BPVdXs2LFj1apVW7duXbJkCYll4OfGLsBto0r5+eefV65cuWXLljVr1pBbSV1dHZ76f1uYRtXx559/Ll++/Ouvv163bh3ZteDnxq7ANKqIvXv3vvfee1999dWnn35Kdi3A5/M5HA7uqb4tTKMqOHDgwLvvvrtx48bPPvuM7FoAACT9GmMa3xamUekdPXp04cKFK1eu3LRpE9m1SNXW1gKm8e1hGpXb8ePHZ8+e/dFHH33//fdk19IOt41dg2lUYidPngwNDV2yZMn27dvJruUFmMauwTQqq9OnT8+aNWv+/Pk//vgj2bV0VFlZyWKx8AzH28I0KqXo6OhZs2ZFRETs2rWLePkZ32SrrKw0NDSkYGEUh2lUPjExMVOmTAkLC/vjjz+o+Y6vrKw0MjIiuwrlg2lUMnFxcZMmTZo5c+aePXvangNFNRUVFcbGxmRXoXwoujpRp65fvz558uSJEydSOYqA28auou4aRR3cvHkzODh43Lhxhw4donh/M5jGrsE0KofExMRx48YFBQUdPnyY4lEETGNXYRqVQFJS0tixY/39/Q8fPqwUvQNjGrsG00h19+/fHz9+/JAhQ06dOsVms8ku5/V4PF59fT2msQswjZT24MGDgIAAb29vZYkiAJSWlorFYjMzM7ILUT6YRupKTU0NCAgYMGDA6dOn1dTUyC7nTZWWlgIAprELMI0UlZGRMWbMGBcXl9OnT6urq5NdzlsoLS0lCMLU1JTsQpQPppGKsrKyRo0a5eDgEBUVpampSXY5b+fZs2cGBgbKsl9NKZhGysnOzh45cqStrW1UVJSWlhbZ5by10tJSc3NzsqtQSphGasnJyRk5cqSpqemFCxe0tbXJLqcrSktL8UNj12AaKaSwsDAwMNDY2Dg2NlZ5u3h69uwZprFrMI1UUVRUNGLECF1d3djYWAMDA7LL6TrcU+0yTCMlFBcXjxw5UltbOy4uTtkfBlxaWooHVLsG00i+8vLywMBABoNx6dIlQ0NDssvpltbW1srKSmtra7ILUUpUv/5Y5VVUVIwaNUokEsXHx6vAJqWoqEgsFltZWZFdiFLCNJKpsrJy1KhRfD4/Pj5eNY58FBUVAQCmsWswjaSpqqoaPXp0U1NTQkKCyhz2KCoqYrPZeMl412AayVFXVzd27Nj6+vqEhAQbGxuyy5GZoqIiKysravbWQ32YRhLU19cHBQVVVFTEx8fb2tqSXY4sSdJIdhXKCtOoaA0NDUFBQaWlpfHx8fb29mSXI2OYxu7AMxwKxeFwJkyYUFBQEBMT4+DgQHY5sodp7A7cNipOc3PzO++8k5GRcfXqVRcXF7LLkQtMY3dgGhVEEsXHjx9fuXLF1dWV7HLkorGxsb6+HtPYZZhGRWhpaZk4ceKDBw/i4uLc3d3JLkde8GRjN2Ea5Y7H44WEhKSkpMTGxg4YMIDscuQI09hNmEb54vF406dPv3HjRmxsrLe3N9nlyFdRUZGWlhY+KK7LMI1yxOfzZ8yYkZCQEBMTM3DgQLLLkbuioiK8Xrw7MI3yIhQKw8PD4+LiLl68OHjwYLLLUQQ8oNpNeL5RLiRRPHfu3Pnz54cNG0Z2OQqCaewmTKPsCYXCefPmnTlz5vz58yNGjCC7HMXBNHYT7qnKmEgkWrBgwbFjx06dOjVy5Eiyy1GokpISTGN3YBplSSwWL168+MiRIydPnhw3bhzZ5ShUTU1NU1MTprE7cE9VZsRi8ZIlS/bt23f8+PHx48eTXY6i4cnG7sNto2yIxeJly5bt2bPn2LFjEyZMILscEkjSaGlpSXYhSgzTKBuffPLJzp07//7770mTJpFdCzmKiooMDAyU7jkFlIJ7qjKwfv36H3744eDBg7NmzSK7FtIUFBSoUicGpMA0dteGDRu+//77/fv3h4WFkV0LmQoKClSsHwPFwzR2y+eff/6///3v999/nzNnDtm1kCw/Px+3jd2Eaey6r7766uuvv/7999/fffddsmshH+6pdh+msYt++OGHTZs2/fLLL++//z7ZtZCPy+WWl5djGrsJ09gVP/7445o1a/7v//5v8eLFZNdCCYWFhSKRCD83dhOe4XhrP/3008qVK7/77rulS5eSXQtV5OfnAwBuG7sJt41vZ/fu3StWrPj222/Xrl1Ldi0UUlBQoK2trdQPuqMCTONb2Lt37wcffLB58+b169eTXQu14OkNmcA0vql9+/a9++67mzZt2rBhA9m1UA6e3pAJTOMbOXDgwKJFi1atWrVx40aya6Ei3DbKBKbx9Y4dO7Zo0aIVK1Zs3bqV7FooCreNMoFpbCcWi1+eeOLEibCwsKVLl27btk3xJSkFPp//7NkzTGP3YRqluFxucHBwZWXl8xNPnjwZGhq6ePHiH3/8kazCqK+4uFgoFGIauw/TKLVv377o6Ohhw4ZVVFRIpkRFRYWFhc2dO3fHjh3k1kZxkpON+Lmx+zCNAABCofDbb78lCCInJ8ff37+srOzSpUtTpkyZPXv2rl278Nmg/62goEBdXR2fZ9x9eC0OAMDRo0eLi4vFYrFAIMjPz/fx8amurp49e/bu3btpNPyH9RqSQzj4P6v78K0GACDZMErGJcckNDQ0NmzYgFF8E3h6Q1bw3QYXL15MS0sTiURtU/h8fk1NzdChQ/Py8kgsTFngvVSygmmEr7/+msHouMcuEAgqKir8/f1zc3NJqUqJ5OTkqORzmhWvp6fx1q1biYmJAoHg5R8JBILS0tKRI0dWVVUpvjBlweVyS0pKMI0y0dPT+M033zCZzJenMxgMgiCCg4NPnDhhaGio+MKURV5enkgkwjTKRI8+ppqenh4VFdXhEhwGgyEUCgMDA7/88sue8Ji3bpLsydvb25NdiCro0dvGDp8YWSwWQRCBgYF37ty5ePEiRvFN5OTkmJiYaGtrk12IKqD6tlEsFtfV1dXW1tbV1bW2tjY3N7dNlMzAZrM1NDQAgCAIPT09NTU1fX19PT09ycT/UFRUFBkZKRQKAYDFYvH5/ICAgM2bN6v2w8BlLjc319HRkewqVAQl0lhWVpaVlVVYWPjs2bOSkpKiooJnzwrLy8vq6hrr6pq61iaLxdDX1zYw0Dc3t7KwsLG0tDQ3N7eysrK1tXV2dmaz2Vu3bhUKhQwGQyQSTZ069fPPP3dxcZHt6+oJcnNz8UOjrJCQxrKysnv37j148CAzMzMjIzUzM7u+ngMALBbNzIxhaQmWlnxfX7GZGejrg56e9KueHrBY0LZDpKsLkjPzLS3Q2goAIBBAYyM0N0NdnWQQ1NXVVlfXlpTkPXtGf/CA8eyZqKqKDwA0GmFpaVpcXE4QhJeX17p168aPH89isRT/p1ABOTk5Pj4+ZFehIohObyOSrdbW1sTExOvXr6ek3ElJSS4pqQQAOzuWs7OwTx+hszNIBnNzeRcCra2QkwOZmfD771BQAJqazLw8cWOjgMVi9O3r4uXlO3jw4OHDh+M/+zckFAo1NDT27t07e/ZssmtRBfJKo1AoTEpKunz58tWrMUlJt1tb+ba2rIEDBV5eIi8v8PICfX15LPZN1dSApEclkQiysiAlRTIwU1JEHI7Q2tp0xIjAUaNGBwYGmivgn4TSevr0qb29fVJS0uDBg8muRRXIOI2tra03btw4d+7s0aOHyspqzMyY/v6CgABxYCDY2clwOfIiEMDDhxAXB3FxzJs3RS0tQldXp5CQ0AkTJnh5eZFdHeXExcUFBgZWVlbiKVnZEMuCUCi8dOnSrFkzNDTYNBoxZAjzu+8gMxPEYiUeWlrg/Hl4913CxIQJAE5Otl9//XVRUZFM/mKqYefOnTo6OmRXoTq6m8b8/PyNGzdaWZkCgJ8fc9cuKCsjP0iyHYRCuHkTli8HQ0MGnU4bM2Z0ZGQkj8eTyQpQamvWrBkwYADZVaiOrqfxwYMH4eGzGQyamRnzo48gNZX82Mh7EAggNhZCQhhMJs3U1PCLL76ora2V4cpQOlOmTAkJCSG7CtXRlTTeunVr9OgRAODpyTx0CPh88nOi4CE/H5YvBy0tur6+1meffVZfXy/zFaMU+vXrt379erKrUB1vl8bc3NwZM6YTBDFiBCM2lvxUkDvU1MA334CBAcPExGDnzp18Pl9OK4maRCKRlpbW7t27yS5EdbxpGrlc7oYNG9hsZp8+zLNnyU8CdYbqalixAlgsmpubU2JiolzXFqUUFxcDQHx8PNmFqI43SmNqaqqnp7uWFuOnn3rifumbDNnZMHYsncGgbdiwgcvlynu1UUFs2SGUBwAAIABJREFUbCwAlJeXk12I6nj9PRy///77wIFempoZDx4IPvoIXrpLHgEAODrCxYvCn38W/fTTFh8fr8LCQrIrkruMjAwDAwNjY2OyC1Ed/5VGkUi0evXqJUsWb9jAT0gQUPNysUWLgCCAICAnh+RKCAI++AAePBAKhZk+Pl4pKSkkFyRnmZmZffr0IbsKlfLKNAqFwtDQmb/88uOhQ7BxI9DpiqxKiTk4wPXr/L59a4cP95fsy6mqjIwMTKNsvTKNS5cuOX/+VEyMKDRUkfWoAh0duHBBOGkSb9q0Sffv3ye7HHnBNMpc52ncsmXL7t1/HD4sHDZMwfWoCAYD9u8XDR7Me+edMZJjjyqmqamppKQE0yhbnaQxNTV148YN27aJJ02SzTJCQoAggM2G5maYORN0dOCnn6Q/qq6GVavA0RHYbDAwgPHjITm5/RdnzJB+Jqyvh6+/BgcHUFMDZ2f444/XLPHBAwgLAzs7UFcHOzuYPRue7xi1rR4A2LMHXFyAzQYHB/jtN9m8XgkmE06cEOro1L3//iJZtksN6enpYrEY0yhjHY6xikSiIUMGDRrEEApldvR/7lzpsj75RDqyeTOIxVBZCZIjQywWDBokvb+RxYLLl6W/OG+edP5ZswAA1NXby/7zT+k8CxdKp2RnS6ckJYGkFw4NDejXD9TUAAB0dNpnaKun7Z9CmzNnZHzm4/p1IAg4ceIEGQfM5ejgwYOS7kvILkSldEzjqVOn6HTi4UNZviPbAmNgAGvWwJEjcOcOiMWwYAEAAIMh/ba1FQICAACcnTv+ookJPHwIIhEcOgSSHvrNzEDy/+LlNI4bJ50ieRWJidJvFy/u2KyxMRw5Avn5sGKFdMqIEbI/FTlvHq13b1uRSETKCpaTDRs2uLu7k12FqumYxsmTJ4wZw5Dt27Ht3b9sWftEPl+6BfPza5948qR0zvv3X/jFH35on2foUOnEe/c6T2NmJiQnQ3w8iMUgFAKXKz1HOmxYx3q+/FI6hcuV3v1sZib7NKalAQDcuHGDlBUsJ9OmTZs+fTrZVaiaFz431tbWXrwYNWdOJx1vy8Tkye3jubnQ3AwAcPOm9MMhQcDUqdKfSt7BbdoSCADe3tKRV51gNDODCxdg6VLQ1AQ6HdhskPQkzuV2nDMwUDrCYoGk37MXH6YqG25u4OnJ/Pvvv2TfNHnwgKo8vHBlTXp6Oo8nGDlSXgszM2sfb2yUjpiYgKdnxzl1dV/4tlev9vG2Ljw4nE4WIRJBQADcvg0AMH48eHgAmw2bN0NnXfvD808clGyoxfLpJGjsWP6FC1fl0jQZBAJBTk4OplHmXkhjYWEhk0kzMxO9au5uer6LfR0d6Ui/fhAd/ZpfrK1tH6+pkY5oaXUyZ0KCNIqhoXD4MAAAnw+bNnWpXNlxdISCAtU5z5GXl8flcjGNMtdxT1VXl66YZxba24OmJgBAWhoIha+Zue1IDAC0XXDm7NzJnPn50hF3d+nIlSvy2uK9OXV1aGl5aUdZaWVkZBAE4dzpCkDd8ELyLCwsqqv5LS2KWDCDASEhAAClpfD99wAAQiEsXAgGBuDh0b4BlNi6Vfop8ehRuHEDAMDWtj1vz7OwkI5cvgx8PuTlwfLl0p5Xy8vl9FJer6YG9PRUp2/89PR0S0tLrU53TlA3vJBGGxsbsRiePlXQsrdsAclDONevB0NDMDKCvXuhthYiIqTdK7Zxd4fevUFPD2bOlG7otm6FTp9sPXSotM0rV0BHR3o+c+1aAID8fHB3h7t35fmSXuHhQ1Cl/brU1NR+/fqRXYUKeiGNbm5upqa92k4zyJuJCdy+DcuWga0tNDYCnQ4BAXDuHKxa1XHO7dvh009BWxtYLOjXDyIjpdvVl6mrQ1QUBAWBri5oakJEBFy/DqtXQ3AwmJqCWCy9BEeR+Hw4c4Y5btxERS9Ybh49eoRplIsOZzxWrFjh5MQSiWR/2q07JyrbziUq43DmDNBoRGFhIRlnsGSPy+WyWKzDhw+TXYgK6njEZuHChbm5ggMHFP1PQVUJBPD558zg4DFWVlZk1yIbGRkZPB4Pt43y0DGNbm5uS5YsWb2agU/XlokdOyAjA7Zt20F2ITKTmprKYrF69+5NdiEqqJOzGZs3f62mZjB3Lr3TM+bozSUlweef0zds2KhKJwMePXrk4uKCj/SSh86fw3H37t0RI4aGhvJ275bXlQAq7+lT8PVlenuPPn36HEOFehMaN26ckZHRwYMHyS5EBXV+pt/b2/vw4ch9+2DFCkKEeXx7mZkQEMC0tnaLjDyuSlEEgNTU1L59+5JdhWp65XU3EydO/PvvQ7//zggJoUsu70Zv6No1GDKEYWLiceFCjKbkgiNVUVNT8+zZMzyEIyf/dRXcrFmz4uKuJCRoDRvGzMhQWElKTCSCHTsgKIg2evTEy5evGT1/WbpKePjwIQDgtlFOXnNNqr+/f1LSXTq974AB9B07APda/0NBAQQEMNaupW/c+FVk5HH157sqUBWPHj3q1asXPmFWTl5/hbijo+PNm8nr13+xdi192DCG5A4J9LzWVti6Ffr1Y1RU2Ccn39mwYQPR6WV7yg+vwpGrN7pfg8FgbNy4MTn5Do020McHZs+mFRTIuzDlIBbDkSPg4sL86iv2ihUb7t592L9/f7KLkiO8QlW+3vbindjYWDe33iwWLTycSEsj/7ozsgYeD44eBW9vFo1GhIRMz8/Pl8OVUtQiFAo1NTX37NlDdiEq663TKBaLeTzenj17+vRxIAiYMIF++TJQ5LpWxQzV1bBtG1hZMZlM+pw5oampqTJfK9SUnZ0NAElJSWQXorK6kkYJoVB4+vRpP79BAGBnx/zyS8jPJz8q8hsEArhwAUJCaGw2TUtL/eOPPy4oKJDhmqC+I0eO0Ol0DodDdiEqq+tpbPPo0aOVK1caG+vTaMSIEYyfflKpWPL5EBcHy5aBuTmTIIhhw4bs3bu3sbGx+383pbNu3bq+ffuSXYUq6/zKuC7g8/kXL16MjDxy8eK5+nrOgAGsiRN5gYEwcOAL3eEoi/JyuHoVLlwgLlyg19YKPDz6TJ48Mzw83IGaT+pSiKCgIHNz8/3795NdiMqSWRrb8Hi8+Pj406dPnzt3ori4QkuL7udHjBghGD4cPD2ByifhSkrg1i2Ij4f4eNaTJzwGg+7nN3jSpOmTJ0+2s7MjuzryGRkZffbZZ8uXLye7EJUl+zQ+LysrKz4+/urVK/HxcWVl1QwG4ebG9PbmeXmBlxc4O3fsqVGRRCIoLIRHjyAlBVJS6Hfv0srK+AwGfcCAviNHjhkxYoS/vz92/dKmsLDQxsbm2rVrQ5/v3BbJlHzT+Lzs7OyUlJS7d++mpCTfu3evoaEZAExNWX36iJ2c+M7OYGMDZmZgbQ2mpjJ+gnJjIxQVQUkJlJRATg5kZhJZWaysLH5rq4ggCEdHKy8vX2/vgV5eXl5eXtraqtOdlAydPn166tSpdXV1Om19byJZU1wanycWi/Py8jIzMzMyMrKysrKyHmdlZZaWVolEYgCg0QhTU6aJCWFgINLTE+jpifX0QE8PmEzQ1pYGVVMTJHfYNTZKey7mcIDHg8ZGqKuTDPT/b+++45q69/+BvzIJM2GErQRFARcgooILa7W2/bX22qq1w7bXLm21dljrba+2tUvrqN2uVq0dfm/tsrW1xdUiVCIIKiIqQ5lhJYFAAhnn90dSUEQFDTkH8n4+zoPHITnJeZ8kr5zPOTnnczQaQV0dr6TEpNPZuoh0dRX37x8WGTlk4MCogQMHRkVFRUVFyWQyx78CPc7y5ct37tx5mo5X7k7spLFDRqOxsrKypKSkrKysrKxMpVKp1Wq1Wq3R1Gg0tRqNxmQyaTT11oLr65vMZgsAd3eJWCwEIJG4uLpK3N3dvb19ZDI/mcxHJpN5e3uHhob6+PjMnTv30UcffdfaVyTpujvuuMPT0/Mra4/RpHtw6NQ7kUjUp0+fbuo/pqys7Lnnnps9e/bw4cO74/l7vaysrEWLFrFdRS/HoXVjt7JYLMnJyfX19UqlUtQTf3JhlUqlCgwMTElJmTRpEtu19GYO6eWfA/h8/ubNm8+cObN69Wq2a+l5MjMzAcRefvUiYlfOkkYAAwcOXL58+WuvvXbq1Cm2a+lhsrKyFAqF78WXCiPdwFlaqlZmszkxMVEgEKSmpgoEArbL6THuvvtuALt27WK7kF7OidaNAAQCwZYtW7Kysj788EO2a+lJsrKyevd5mxzhXGkEMHTo0KVLl/7nP/85d6VrI5NLqdXq8+fP075oB3CulqqVyWQaOXKkVCrdv39/b+0yw45SUlImT55cUVERGBjIdi29nNOtGwEIhcItW7YcPnx406ZNbNfSA2RlZQUHB1MUHcAZ0wggLi7u2WefXbx4cUlJCdu1cF1GRsaoUaPYrsIpOGkaAbz++ushISFPPvkk24VwXUZGRkJCAttVOAXnTaOLi8uWLVt+++23HTt2sF0Ld1mPHB45ciTbhTgF500jgMTExPnz5y9atEilUrFdC0cdOXKEx+PFx8ezXYhTcOo0AnjnnXekUunTTz/NdiEcpVQqIyMj6aQzx3D2NLq7u2/atGnXrl10oEmHlEolNVMdxtnTCOCmm2565JFH5s2bV11dzXYt3MIwTGZmJu3CcRhKIwCsXbvWxcXlhRdeYLsQbjl79mxtbS2tGx2G0ggAUqn0008/3b59+08//cR2LRySkZEhFotjYmLYLsRZUBptbr/99tmzZ8+bN0+j0bBdC1colcqYmBgXFxe2C3EWlMY2H3zwgclkWrp0KduFcAXtwnEwSmMbX1/f9evXb9iwISUlhe1a2Gc0GrOzs2kXjiM54zkcVzd9+vTs7Ozjx487edfGmZmZI0aMOHXqVHR0NNu1OAtaN7b30UcfaTSaZcuWsV0IyzIyMjw9PSMjI9kuxIlQGtsLCgpas2bN+vXrU1NT2a6FTUqlMiEhgc+nT4jj0GvdgUceeWTKlCmPPvqowWBguxbW0C4cx6M0dmzDhg3l5eVvvPEG24WwQ6fT5eXl0S4cB6M0dqxv375vv/32ypUrrV2JOpujR4+azWZKo4NRGq9o3rx5SUlJc+fONRqNbNfiaBkZGYGBgd10FQZyJZTGK+Lz+Vu3bj137tyqVavYrsXRlEol9b7heJTGqwkPD1++fPmKFStyc3PZrsWhrDtU2a7C6dCv/9dgsVjGjRtnNpsPHz7sJN2TV1VVBQQE/P7775MnT2a7FudC68ZrsF5OJycnZ/369WzX4iDW3jeoO2PHozReW3R09NKlS//73/86SffkSqVywIABdA0cx6OWaqeYTKZRo0Z5enoeOHCg13dPPnXqVLlc/sUXX7BdiNOhdWOnWLsnT0tL27BhA9u1dC+GYY4ePUq7cFhBaeys2NjY559/fsmSJRcuXGC7lm5UUFBQW1tLaWQFpbELXn311dDQ0Llz5/bi5n1GRoZIJKLLGLOC0tgF1u7JDxw40Iu3qZRK5bBhw1xdXdkuxBlRGrtm9OjRTz311DPPPFNWVsZ2Ld0iIyODTt1gC6Wxy95++21fX9958+axXYj9mUwm6n2DRZTGLnNzc9u0adPPP//8v//9j+1a7OzEiRNNTU2URrZQGq/HxIkTH3300fnz5/ey7skzMjI8PDyoIxy2UBqv0+rVq11dXZ977jm2C7EnpVI5YsQIJzkcl4MojdfJy8vr008/3bFjx48//sh2LXZDF05lF6Xx+t12223333///Pnz1Wo127XYQVNTE/W+wS5K4w1Zv3692WxesmQJ24XYwdGjR00mE/28wSJK4w3x9fV9//33N2/e/Pvvv7Ndy43KyMjw9/cPCwtjuxDnRedw2MHdd9+dlZV14sSJHt09+axZs/R6PV2li0W0brSDjz76qL6+/pVXXmG7kBti3aHKdhVOjdJoB4GBgWvXrv3ggw96bvfklZWVRUVFiYmJbBfi1KilajfTpk3Lz88/duxYTzzk+ttvv501a1ZdXZ1UKmW7FudF60a7+fjjj1Uq1euvv852IdcjPT196NChFEV2URrtJiQk5K233lq9evXRo0fZrqXL0tLSxowZw3YVzo5aqvbEMMyUKVOqqqqUSqVYLGa7nM5qbm6WSqWbN29+4IEH2K7FqdG60Z54PN7GjRsLCwtXrlzJdi1doFQqm5ubk5KS2C7E2VEa7Sw8PPy111574403Tp48yXYtnZWWlhYQENCvXz+2C3F21FK1P4vFMn78eKPRmJaW1iPOh7jrrrsEAsGuXbvYLsTZ0brR/qzdkx8/fnzdunVs19Ipf//9NzVTuYDS2C2ioqJefvnlV155JS8vj+1aruHs2bMqlYrSyAXUUu0uJpNp9OjRLi4uf/31F5/P3W+9bdu2PfHEE1qt1sXFhe1anB13PyU9nbV7cqVS+emnn7Jdy9Wkp6fHx8dTFLmA0tiNYmJiFi9e/OKLLxYWFrJdyxX99ddfY8eOZbsKAlBLtbs1NzfHx8cHBASkpKRw8HI61dXVAQEBP//882233cZ2LYTWjd3M2j35oUOHtm7dynYtHTh06BCfz6ddOBxBaex2o0aNWrBgwbPPPsvB7skPHToUFxcnk8nYLoQAlEbHePPNN+Vy+ZNPPsl2Ie0dPHhwwoQJbFdBbCiNjmDtnvyXX37ZuXMn27W0qaurO3XqFKWROyiNDpKcnPz4448//fTTVVVVbNdic+jQIQDjxo1juxBiQ2l0nFWrVrm6ui5atIjtQmwOHToUExNDG43cQWl0HC8vrw0bNnz99dc//PAD27UAwKFDh5KTk9mugrShNDrUrbfe+uCDD3Khe3KNRnPixAnaaOQUSqOjvffeewzDLF68mN0y/vzzT4vFQkfhcAql0dF8fHw2bNjw2Wef7d27l8UyDh06NHToUF9fXxZrIO1QGllw55133n333U888URDQwNbNRw8eJA2GrmG0siOTz75pKmp6eWXX2Zl7rW1tdnZ2ZMmTWJl7uRKKI3s8PPzW7NmzUcfffTXX385fu4pKSl8Pp924XANncPBprvuuisvLy87O9vB3ZM/9thjeXl5Pfc6Bb0VrRvZ9PHHH1dVVb366qsOnu++fftuvvlmB8+UXBOlkU3BwcErV65cs2aNUql02EzPnTtXVFREaeQgaqmyjGGYW265pbS09NixY47pDuOTTz5ZsmRJbW2tSCRywOxI59G6kWXW7slLSkrefvttx8wxJSUlOTmZoshBlEb2KRSKFStWvPnmm8eOHWt3l91bLmaz+eDBg9RM5SZqqXKCxWKZMGFCY2PjkSNHrGsto9G4du1aPp9v32PoMjIyRo0alZubO2jQIDs+LbELWjdygrV78ry8PGv35EqlMiYm5qWXXtqzZ499Z/THH38EBwdHR0fb92mJXQjZLoDYREZGLlu27NVXXy0oKNi8ebP1Ah7p6ektLS12vPhcSkrK5MmTOdh7HQGtGzll7NixXl5en332mcViMRqNAJqbm+14bdampqb09HTaaOQsSiMnNDU1vfTSS8nJyTU1NSaTqfV2sVhs7S/DLg4dOtTS0kKHp3IWpZF9f/zxR2Rk5Nq1ay0Wi9lsvvguo9GYkpJirxmlpKQMGTIkKCjIXk9I7IvSyL4jR46Ul5e3y6EVwzDp6enWVuuNS0lJoWYql1Ea2ffKK6/s27fP29u7w1/k9Xp9VlbWjc9FpVKdOHGC0shllEZOSE5OzsnJGT58+OXXQhaJRAcPHrzxWaSkpIhEovHjx9/4U5FuQmnkipCQkNTU1BdeeIHH4138C4TJZNq3b9+NP39KSkpiYqKHh8eNPxXpJpRGDhEKhe+8887333/v7u7e2mplGCY1NfXiHa3Xh86i4j5KI+dMmzYtKysrIiJCKLQdm6HX6y8/hLVLTp8+XVJSQmnkOEojFw0YMCArK2vOnDnWf0Ui0Q3+6vjHH3/IZLIRI0bYozrSXSiNHCWRSLZs2bJhwwaxWGw0Gm9w0zElJWXixImtK1vCTXQORzfS6/V6vV6j0ej1eoPBoFarDQaD9RaDwdDU1KTVai0WS1NTU3NzM8MwGo0GgMHQpNfrAGi1GovFXF/fUFRUarFYFIpgk8nc0KDrcF5qdce3e3q6CoUCg8EoEPBEorY0urpKJBLbyc0SicTaMY9M5svjQSyWuLt7AfD09BQKhQKBwMvLC4BUKnV1dXVzc5NKpRKJxN3d3cvLSyKReHh4eHp6SiQST09Pe758zofS2AU1NTVqtVpzRbUaTa1Wq9FotI2Neo2m43gAkEqFEgnf3Z3n6QmhEBIJ4+rKAJBKjXw+RCJYd3xa7xUI4OKC//0PEyciJARSKfgdNWg8PHD5r5UWC7TajmvQamGx2MYbG9HSAoaBRgMABgP0eus0QouFZzRCp+MDUKthMDB6PaPRGK/0qfHwcHV3l8hkUpnMWybzkcn8pFKpt7e37FLWG/38/Gh1fTFKIwCYzWaVSlVTU1NdXV1VVVXzj6oqVXV1RU1NVU1NbU2N1my2XPwomUwokwlkMshkFpnMLJNZZDJYB3d3SKVwc4NEApkMEglcXW0jbm7XUyHDoLoa/v72Wd4b19yMpibU18NggE6HhgYYDGhoQGMjdDpoNK0DT6sVqtV8jQYajUWjMZnNl3zefHw85XJfPz+5n1+gXB7g7+/v9w9/f39/f/+AgAA7nsLCcU6URrVaXV5eXlZWVlFRUVJSUllZWVp6oaKipLS0VKWqa02aQMDz8xP6+fH9/Cz+/ka5HHI5/Pzg5wd/f/j4QCaDVApvb3aXpqdqaLAFVa1GTQ1UKtTU2IbqamFVlaCmhqmpMTU3t33xBQR4BwYGhIYqgoJCQ0JCQkJCgoKCQkNDAwMDAwICetPZYb0wjSqVqqioqPgfRUVni4sLSkoq9PoW6wSuroLgYGFwsCU01BgYiD59EBiI0NC21BHWNTRApUJ1NSoqUFaG8nLrX2F5uaC01NzQYPv1VSQSBAf7KxTh4eEDFQpFeHi4QqFQKBQhISGXH9XEfT04jWaz+fz58/n5+Xl5eQUFBcXFhcXF54qKSvT6ZgBCIS80VKxQ8BUKvUKBsDAEBSEkBMHB8PFhu3RyYxobUVKCykqUlqK0FMXFKC7mFxcLiovN1pWqSCTo2zdIoVAoFJH9+/ePjIyMioqKiIjgeKO3x6SxoaEhPz/fmr0zZ/Lz80/k5xc2NxsBBAaKBwxgFApjeDgUCtvQpw9oB4GzYRhUVKCoyJpP6yA8d45/4YLRYmGEQoFCERwVNTQqalBkZGRkZGR0dLQfl9pCHE0jwzCFhYXZ2dk5OTk5OcdycjLPn68AIBbzIyJEUVHGyEhLZCSiozFwIOhS2eTq9HqcOYP8fOTnIy+Pd+aMOD/fpNOZAfj5SWNj42Ji4mNjY2NiYqKioljs25IraTSZTCdOnDh69GhOTk5OTubx4yfq6xsFAl5EhEtMTEtcnGXIEERFQaGgNR6xj5IS5OcjNxc5OcjJEZ08aW5psbi4CAcPHhgbOyomJjYuLm7EiBGOvEQKm2lsaGg4cuRIampqZubfqampGk2jWMyPiODHx5vi4xEfj7g4uLuzVR1xLiYT8vNx6hRyc5GZKVIqeSpVi1AoGDiw/9ixyWPGjBk3blx4eHi31uDoNFZWVv7++++pqalpaX/m5Z2xWJjISEliYnNSEpOUhOjojn/aJsTxzp/H4cNIT0damuj4cZPJxPTtGzBmTHJS0tjJkydHRkbafY6OSGNLS0tqaurevXv37t19/PhpFxd+QgI/Kck4ZgxGj4Zc3t3zJ+RG6XTIyLCGU5CWxtNqTQpF8C233HHLLbdMmjTJeuTgjevGNFZVVe3atWvPnt0HDhxobDRERkpuucUwdSomTLjO41EI4QKTCUeO4LffsHevKDPTxOfzkpJGTp165z333DNgwIAbemrG3urr67dv3z516s1CocDDQ/Cvfwk+/RRFRWCYnjrMnWt7rc6eZb8Ytgrr5Lx8fQFg8GD2XxzHVFJdja++wkMP8QICRABGjoxbt25deXn59WXHbltpFovl559/vvfeWQEB8kcffVgsPrhjh7mqyvzdd+YnnoBCYa/5EMIhfn6YPRtbtzJlZcbff8egQdnLl7/Qp0/o5MnJn3/+eVNTU5eezQ5p1Gq1q1evjogImzbtzsrK7957r7miwvLjj6ZZs+DY62cTwhqBAJMn4/PPGZXK/M03Fk/P1HnzHg0NDXzxxRcvXLjQySe5oTRqtdrXXntNoQhdsWLpHXeUnj7NHDxoevxxOvSMOC+JBPfcg+++M5eUWF54oeGrr94bMKD/E088fv78+Ws+9jrTyDDM9u3bBw4MX7NmxRNP6M6fN61fjxvcgr0RM2aAx4OLC5qaMGsWvLywfr3trtpaPP88IiLg4gIfH9x+O44caXvgzJng8cDjQavFG2+gf39IJIiMxMaN15hjdjbuuw/h4XB1RXg47r8fhYUd1ANg82ZER8PFBf374+OPu7ZcaWm4+26Eh0MiQVAQbrkFv/zSwWRC4dXm0tKCdeuQkABPT0gkiIjA00+jtLRtgrFjweO1P6zCzw88HoYMuVp5R4/ippvg7g5fX8yeDZUK13FCxdXfoE6+knapxL7kcvznPygqMm7aZNq/f9vAgRHPPPOMTnfFs16B69qLU1paOmHCGKGQv3AhT61mf3udYfDQQ7bFeekl28iKFbaN7P79AUAsxsiRCA62je/bZ3vgww/bpr/3XgCXNK23bLFNc/kOjL//tu0WdnPDsGGQSADAy6ttgtZ6Wr8UWv34Y2cXatcu20fKw+OSAwA//LB9YW+8ccW56PVo7UJVIGjbm+3nhxMnbNOMGWO79yp7QS5/EU6dQmt3kFIpRCLExNiaRZ3fd3LNN6gzr6RdKunWoaUFa9bAy0sYERF29OjRKyULXY1ienq6XO4dGSlDMObuAAAfgklEQVTKymJ/IVuH1s+Kjw8WL8Y330CpBMPg3/8GAKHQ9q/BAGu3aZGR7R8YEICcHFgs+PJLWwaCgmA2d/xBvPVW2y05OWAYpKfb/p0/v/3T+vvjm29QXIxnn7Xdkpzc2YVKTASAESPQ3AyGgcmEBx6Avz9uvhkWS2fnsnSp7ZZHHkFjI8xmbN5sW8D4eNs015fGmTNttyxbBrMZDQ2YONF2S+cz0Pk36CrLaJdKHDCUl2PyZKFYLNy6dasd0nj48GEPD9c77xQ2NLC/bBcPre/ZggVtNxqNtlXBmDFtN373nW3KY8cueeCaNW3TjBtnu9H6jXP5BzE/H0eO4OBBMAzMZjQ325p548e3r+e112y3NDfbTlAOCursQlkveRoRgXPnrrHUV5qL2WxbRXh5Qadre+DUqbYHHj8O5rrSaDbbDlqUyWxfFgyDzMyuZaBLb9BVlvHGK3HYYDZj6VLweLxNmzZdnq8ubDfW1NTMnPmvm25q+fZbE2d7rL7rrrbxggJY9zAfPmzbOOTxMH267d6TJy95YGsCAbR2dHjuXMdzCQrCL7/g6afh7m7rt8ba+XBzc/spJ0+2jYjFiIgAgOrqzi6L9bHnziEiAgMG4KGH8OWXaGi44pSXz6WwEHV1ABAbe8kRv6NG2UaysztbTDvl5WhsBIDBg9F6zmBcHLp0/mCX3qArLaNdKnEYPh9vvYVly5innpqnVCrb3duFEyJWr15tsai3bTOzd8bJtV18NbTWD25AAGJj208plV7yr3VVYNXay4b1bW7HYsHNNyMjAwBuvx0xMXBxwYoV6LA38IuP+7OuB5hOH/v0zjvQ6bB9O0wmnDuHc+ewfTtkMuzciSlTOjWX1i6qLl46oG0rq76+s8W007oz4uKXkceDVNqFr5suvUFXWka7VOJgy5bhzz/x0ksv7Nt3STe5XUjjjh2fzZtn5PjJhBd/U7QePDhsGH777RoPVKvbxq3rE1z0qb3YoUO2KM6eja++AgCjEa++el3lXpWrK7ZswapVSElBWhoOHUJODjQa3HUXzp/v1PG9rW9W6xJZVVXZRqzfO9bNSLMZLS22VYrJZOs/7kpa9wZdPJnFcsnLeE1deoO6tRIH4/OxZIlp6tQ/y8rKQkJC2m7v5OMNBkN5eU1MTPdU1z369bM1z06eREcXR7xE654YoG2ro8PD9IuLbSOte//37+/CGq+rfH0xaxbWr0d2Nt5/HwD0ehw/3qnHhofb1orHjuHiw0IOH7aNJCTYZmGVn28bOXjwGq9YSIht/3Nublv7/O+/O24gXEmX3qBurcTxrG2Bc5duC3U2jRKJxMNDUllp96q6kVCIGTMAoKIC774LAGYz5s6Fjw9iYtqvLlatsm0l/t//ITUVABSKjn9ta/0u27cPRiMKC/HMM7YTwVQquxVfWYmxYxEQgA8+aLux9dPWycsT8/l4/HEAqK/H88+jpQVmMz78EGlpADBliu0n4tYvncWLUVyMv//GvHnXOI7KeugJAK0Wr78OsxlqNRYv7sICootvULdW4ngVFQAgb9fC6fwO1enT70pMFFn3rXNtuNIxzZWVCAuz3eXr27ZBuHp1+wdaf7S4eNvj//6v4ydvamp7TusvjZGRbb9zDh4MpbLjeiZMAC7bdXmVwfpJBRAQgMGDERho+3fWrKstdbu56PVtO6gu7s01PBwlJbZpCgvb94z80EOIjweAqKgrzuvo0bY9JR4eEAgwerTta6L1UdccOv8GXWUZ7VKJg4eFC6FQhJhMpuv8hePo0aNCIX/tWvaX5PLhKmcYqFRYsAAKBcRi+Pnh5puxe3cHD8zLw3/+g9BQiMUYNgw7d17tyU+dwpQpkErh64s5c1BVhZoa3HYbAgMxaBCOH7dPGk0mrFyJESPg6QmBAEFBmDABX3wBk+lqS335XJqbsXo1hg+39bYcHY2lS1FXd8m89uxBbCzEYgQHY8kStLTgppsAoG/fq80rJQUjRsDFBf7++Pe/UVtra0306dOF966Tb9DVl9EulThsOHgQQiF/48aN7SKGzqeRYZiVK1cKBLxNm9hfHnsNnD1biobeOqSmQioVzpgx/fJ8da3LpxdffJFhmMcee0mp5L3/PuPi0qVHE+Lstm/HvHmCSZOmbN/+5eX3drkDtiVLlvTp02fevMfS0owffmi0thlIl+Tnt62Tr2TVKiQlOaSa7uEMy9glZ85gwQLhvn2Wl19+edmyZR13hd6llmqrgoKC22+fCmD6dIH1WM0eOlBLlYbuHsrKsHAhxGJ+TMyg1NTUq8QK15dGq59//jk2djCPh3/9S3DoEPuLTQMNnBry8vDkkzyJRBAaGvDJJ5+024Nq5zQyDGOxWL777rvExAQAcXHiDz+ESsX+q0ADDSwOOh2++gpTpgh4PERE9P3oo48MBkNn0oQbTGOrjIyMhx6a4+npJhTyp04Vbt+O+nr2XxcaaHDY0NKC3btx3308d3eBUCi47bYpP/30k9ls7nyI7NyDo16v/+mnn776asdvv/0mEDD/7/9h2jTz5MkcugwoIfal0+HAAfzyC779VlhXZ05KSrjvvodmzJjR/jibTuiu/lTVavW33377zTc7UlPTTCZzXJzolltapk5FYiJdSIP0eAyD48exdy9++014+LDFaGSGDx86Y8Z99957b1jrsUVd1+19jet0uv379+/du3fv3p8LCi5IpcJJk5gJE8xJSYiNpWSSHoNhcPo00tPx55+8338XVlQY/f1lkyffOnXqbVOmTPG3R/PPodfhOHfu3N69e/fu3XP4cGpdXb2bmyAhgT9mjDExEYmJ7c/BI4R1jY1QKm0d/qen8+rqTG5uLgkJIyZPvnXq1KlxcXF8u143hp1rVDEMc/r06fT09MOHD6enHzx9ughgIiPFCQnGmBgmNhaxsRROwgKdDidOICcHx44hM1Ock2M0mZjQUP8xY5ITE5OSkpJiY2O77wKPnLh+Y11dXXp6enp6+tGjR3JyjlVW1gLo08clJsYUE2O2hrN/f/b75CO9T3k5srORk4PsbF52tvDcOZPFwkilbjExw+LiRiUmJiYlJfXp08cxxXAije2o1erc3NzMzMzMzKOnTh07cSK/pcUkFvNDQwWDBpkGD2b69cOgQYiJgacn27WSnsNoREkJCguRm4tTp1BY6HLyJFNZ2QIgKMgvPn7k4MFDBw0aFB8fHx0dbd8maCdxMY3tNDU1nTx58uTJk2fOnMnPz8vLO1lYeN5oNAPo00cSGWkZOLBl4EAoFAgPh0IBO129i/RgBgOKi1FcjKIiFBTg9Gl+fr6guNhkMjF8Pq9v34DIyMGRkYOjoqKioqJiY2O9W0+sZFUPSOPljEZjUVHR6dOn8/Pz8/PzT58+UVBwrrLSdq64j49IoeCFhxsVCkahsKU0LKzjTm5Ij2YwoLQURUW27BUXo6hIXFyMiooW6wTe3p79+ikiI4dERUVHRkZGRkYOHDjQkRcP75IemcYONTc3l5WVFbYpKC8vrqgoKyqqtC6jRMIPDhYGBVmCg01BQQgORutfyiqXqdUoL0dFhe1vYSHKy4UVFcLycotKZbRYrG+uODg4oF+/Af36RfS7CEdWep3Ue9J4JQ0NDcXFxRcuXKioqCgrKysvLy8vLy0rK66oqFSp1K2LL5eL5HK+XG6Ry03+/oxcDj8/+PkhIACt4/TrqN1VV6OmxjaoVG3/qlTC6mpBTQ2jUhnNZtt75OvrFRQUEBISFhQUGhoaGhgYGBoaGhISolAo/Pz82F0Qu+j9abwKo9GoUqlKS0srKytLS0tramqqq6urqlTV1RU1NVU1NbXV1RrrV6+Vr6/I25svk0Ems3h7G6VSyGSQydA60jru4QGZzBl3Amu1aGyEVguNBhpN28hFg1Cr5Ws0PLWaqalpSxoAHx9PudzXz0/u5xfo7x/o7+/v5+cXFBQUEhISHBwcHBwssXZD1Hs5dRqviWGYmn9UV1erVCrNP9RqtVZbq9HUajQarbZeo9E1NxvbPdzFhe/mJvDy4ksk8PCAp6dFIrF4epo9PCCRwMsL7u4QiyES2drJHh4QicDn2zrLcnW19YJ1cbAlko57c/P07GDVzTAdd4va0nJJx8319TCbYTLZuhtubERLCywWW+fIej0MBtu/TU0wGKDRQK8XGAx8tZpvMECvh0ZjMRgsTU3tu2EUCPgymYdM5iWTyWQyH5nMTybzlkqlMplMJpPJ5fKAgAA/Pz8/Pz+5XC50+rYHpdFu9Hq9RqPRarUajUan02m1WoPB0NjYWF9fr9frrSMGg0Gn0zU0qPX6Jp2uvqGhwWQyNTe3NDXpAdTXN5nNFraXo41M5sHjQSwWu7u7AvD29nZ1dZNI3Ly95RKJxNXV1dvb2zoik8kkEombm5tMJnNzc2vNmwdtjncFpZFzTCZTQ0MDgMbGxpaWFovFom3twR+wBrjdQxiG0fyzEty2bdvZs2ff+Ocich4eHh0eO3Lx7g3rNHw+XyqVAnB1de31bUJucva2AQcJhUJrVK5vf2B6enpdXd2M1s5YSc/BwgEHhJAOURoJ4QpKIyFcQWkkhCsojYRwBaWREK6gNBLCFZRGQriC0kgIV1AaCeEKSiMhXEFpJIQrKI2EcAWlkRCuoDQSwhWURkK4gtJICFdQGgnhCkojIVxBaSSEKyiNhHAFpZEQrqA0EsIVlEZCuILSSAhXUBoJ4QpKIyFcQWkkhCsojYRwBaWREK6gNBLCFZRGQriC0kgIV1AaCeEKSiMhXEFpJIQrKI2EcAWlkRCuoDQSwhWURkK4gtJICFdQGgnhCkojIVxBaSSEKyiNhHAFpbEXOnv2LO8KPv30U7arI1ckZLsAYn99+vTZvXu3dXzGjBnDhg3773//a/23X79+7NVFroHS2AtJJJLRo0dbx11cXORyeeu/hMuopep0fHx8Pvjgg2nTprm5udXX13t4eKxevbr13kcffXTEiBHWcbPZvHz5coVC4eLiEhERsX79epZKdha0bnQ6YrH4s88+S0hI+PLLL11dXa8y5fLly9evX79ly5Zx48bt27fvscceE4vF8+bNc1ipzobS6HSEQqHZbN64cePVJ9PpdO+9997ixYtnzpwJ4IEHHkhLS1uzZg2lsftQS9UZjR079prTnD17trGxMTk5ufWWcePGFRQUVFdXd2Nlzo3Wjc7I29v7mtNUVFQAuDiNVtXV1XK5vDuqIpRGZ8Tj8TocB6DT6awjXl5eAH7//fe4uLiLJ+hMksn1oTQ6O5lMplarreNms1mpVFrzFhMT4+LicubMmcmTJ7NaoBOh7UZnl5CQ8PXXX2dmZp4+fXrevHl8vu0j4enpOW/evNdee+2HH36orKzMzs6+/fbbH374YVaL7eUojc7u3XffDQsLGz9+/E033RQRETF79myTyWS9a/Xq1fPnz1+0aFGfPn2mTp3q4+Pz1ltvsVtt78ZjGIbtGog9Pffcc7/++uvy5cu7bxbTpk27+g+V5PrQdmMvVFFR8cYbb3Tf80+cOJHS2B0ojb3QoEGD0tLS2K6CdBltNxLCFZRGQriC0kgIV1AaCeEKSiMhXEFpJIQrKI2EcAWlkRCuoDQSwhWURkK4gtJICFdQGgnhCkojIVxBaSSEKyiNhHAFpZEQrqA0EsIVlEZCuILSSAhXUBoJ4QpKIyFcQWkkhCsojYRwBaWREK6gNBLCFZRGQriC0kgIV1AaCeEKSiMhXEFpJIQrKI2EcAWlkRCuoDQSwhWURkK4gtJICFdQGgnhCkojIVxBaezxfvzxR95F1q1bl56efvEtS5cuZbtG0ik8hmHYroHckJaWFj8/v4aGhitNkJOTM2zYMEeWRK4PrRt7PLFYPHPmTJFI1OG9ERERFMWegtLYG9x3331Go/Hy20Ui0SOPPOL4esj1oZZqb2CxWAIDA6urqy+/6+zZsxEREY4viVwHWjf2Bnw+/4EHHmjXWOXxeCNGjKAo9iCUxl5i9uzZ7RqrAoFgzpw5bNVDrgO1VHuP8PDw4uLi1n/5fH5ZWVlgYCB7FZGuoXVj7/Hggw+2NlYFAkFycjJFsWehNPYe7fasPvjggywWQ64DtVR7lSFDhpw6dYphGJFIVFVVJZPJ2K6IdAGtG3uVOXPmCIVCgUBw6623UhR7HEpjrzJ79myTyWQ2m6mZ2hNRS7UHqK+vr6ioqK+v12q1Wq1Wp9M1NDTodDqNRmOxWLRabeuUarV6//79Go1m2rRpMplMKBRab3d3dxeLxR4eHp6enh4eHl5eXjKZzMPDQyqVyuVyuVzO0pKRS1AaOcFoNJaWlhYXFxcVFRUXF5eUlFRXVaoqyyorVdU16uaWS35IdHUReEj4nq58b3cAkLmZeTye9S4vF2NxNep0SOgPjV7IwHa7Vs+zWHj1eugMjM5gaWgyXfyEQqFA7iuT+/kFBof4BwSHhIQoLiKRSBzxEhBKIytUKtWpU6fy8vJyc3Pzco8XFhaUlqvMZgsAVxdBeICwr4/J39Ps74UgGeRekHsiyBtSV0jd4OUKwVU3L6rrcbwEkwZfowZNE3QGaJtQVY8KDarrUd2ACjWqdfySOlFxtVmjsyU2KMBXoVAMGhIbFRU1ePDg6OjosLCw1vwTO6I0OkJBQYFSqVQqlUcz0k+ePFmnaQDg7SmMDuYNDjb2D4BCDoUfFHIESNmu9R+aJhRXo7gaxTUoUCGvQphXzi+vbQHg7iaJjhoYn5A4cuTIhISEQYMGCQQCtuvtDSiN3aK5ufnIkSMHDhz4O/2wMiOjVq0VCnhDwlwSwgzD+iI6BINCENQDd3lqmnCqFKfKkFsKZZHgWDHT1Gxxd5MMj4sZlTguOTl5/Pjxnp6ebJfZU1Ea7cZisWRkZOzfv//A/j/S0tKb9M2KAPGYCGNCPyahH+IUcBWzXaK9mcw4VYaMAigLkV4gPnmhRcAXxA+PmThpysSJE8ePH0/bnF1CabxRBoMhNTV19+6f/rfzqwpVbYC3aPxA081DmDEDMTiU7eIcq7oef5/D4TNIyZNkFRgkLuJJk26+485pd911l7+/P9vV9QCUxutkNBr37NmzY8cXv/26p7FJP3KA+K64ljvjMSiE7cq4oUyN3Vn4IVNw4BRjsfAmjB87+/4HZ8yY4eXlxXZp3EVp7LLs7Oxt27Z9uWNbbZ1m4mDhPQnGO4cj2JvtsriqXo9fc/CdkvdTFo8vEE2ffvfDj/x74sSJfD4dedIepbGzzGbzrl27Vq96W5mZHREkfmhsy5xx6OvLdlk9h6YJ36Rj61+iI2eN4WGhi55bPHfuXHd3d7br4hBK47U1NTV9/vnna1evLL5QOj2Bv/AW89iBoN/brlteGT5OwWeHBK5ubvOeembBggW0VWlFabwai8WyY8eOl158vq6ubuZo5j93MlHBbNfUW2ibsPVPrNoj1jTxFixc9PLLL9NPI5TGK9q3b9/zzy7MPXX6iUnM8n8xctr70A30LXh/L976Segp9XnjrZVz5sxx5u1JSmMH9Hr9Sy+9+P77H948VLD2fvPQPmwX1D38nkCtDoNDcXIly5XU6vD697yP/+AlJMTv+PKbfv36sVwQS5z3e+hKMjIyhg2J/mrbhu+fxR8v9doocoqvB9Y/yGS8btGUZscPj9m5cyfbFbGD0niJ7du3jx2T1M+z7PhbxrtGsF2Nk4lTIHOF8b6RjbNn3/v88885YatNyHYBHPL+++8vWrTopTuYN2fSLlN2uIrx0cPMuEg89MF6dV3dps1bnOp4dEqjzbp1655//rlVs/HC7WyXcplaHd76ET9moqQW7i5IHIBl/8Kof3otnrEe32ZALETzNmw+gDV7UFiFUB88fxvmT257kqOFePFrHDkHiRhThuK9B7n7jXNvImRulrvf39HcbPjyq2/YLsdxBK+++irbNbAvNTX1gfvvW3kvw8Eo1jRg9HLsyUZjM2LDoG9BVjG2p2LMQIT7A8CebORcgNkCHw/M/xw1DTBboG7EnmwMD0dkEADklWHs6zhTAaMZYiGOX8De49DqoW+Bv9cloeWIiEAkRjBLN+Z5e/uMGjWK7XIchLYbUVtbO3vWPf9vOO/529gupSNLvkaBCkIBDi/HkddRuA43D0GLCfM/t00g/Kcp9+YP+GYBitfj2Vttt6zbYxt59TvoDACwbDrqNqJuI3w8UKdz5HJ02U2D8ep0y+IXnsvMzGS7FgehNGLFihVmQ91nj5k52HIzmfFNOgCM6o8R/QDARWRbleVXIPv8JRM/NRmzRiPMD+/cC2snHfkVAGBh8MsxAJC54eVp4PPgIcHq+x24GNdr6TQkDsDCp+ezXYiDOHsatVrtxg2f/OcOozcnj5csqEJTCwAcPgPe/bZh+jrbvSdLLpl48lDbiFiIiAAAqG4AgHI1GpsBYHAoxP/sKIgLaxvnLD4Pb880pf2dkZaWxnYtjsD5N6Sb7d6922I2PTiW7TquoEFvGwmQIjas/b1St0v+vfhoITcXALD+RmBto7abnseD1A3V9XYstluMjsDQMPHOnTuTkpLYrqXbOXsa09PT4/sJpG4WtgvpmJerbWRYX/y25DqfxJpMAJqmthstDNSNN1CZA02KbklNPch2FY7g7C3VstKScL8OrgrMEf384e4CACdLYL7eb4wQb1snILmlaP5nWf8+C5PZHiV2v3B/lJaWsl2FIzh7Gnk8HpcP+RAKMGMUAFRo8O7PAGC2YO5G+DyOmKWd3Skq4GPyEADQNuH1722/fyz+utuKtjeGgZMcSu4UC3kVIaF9imtF156OPe/cizA/AFi6E35PQP4kPjsEdSPmjIOPR2efZNl02z6bt36E7DHIn4TFYuu07rpXuQ5TVI2QEKfo4MTZ05iUlJRZaL54g4prAqTIWIEFt0AhR4MBAj5uHoLdL6BLv47Gh2PPixjRDy4iuInx0Dj8shi+HgBsu1u5bF+eOGlsMttVOIKzn1FVX18fFOj/9ozmhbewXQrpSPpZJL2KtLS0xMREtmvpds6+bvTy8nriyflv7xZx/MAU52Rh8NJO4dik0c4QRdC6EUBdXV3ssMFxQdU/LLLn4Tj5FZi78RoT1OqQNOBq06yajaSBdivpKpVws9QV3+PNn4Rp6UeGDx9u56fmJEojAKSlpSVPGL/iHvOSO9guhfzjjxO4dRXv/Q8+nD+fjoxzJklJSavXrF26Eyt3s10KAQDsyca0dYL77pvtPFEEHYvTauHChQKBYOHCBepGvD2L4eAR5M7j6zQ8tIE/Z86cDRs3sV2LQ1FL9RI7duz49yMPJw/ibX3cRN2HO15TC174kvfpPua55557993VznaVSEpje0ql8oH7ZtVVl278t/Ff1DWOA2UW4YFPRVU61083bp4xYwbb5bCAthvbS0hIyD6ee99DT0xfh8nvCI5fYLsgJ1CrwzNf8EYv5wcPSMw5keucUQSlsUOurq7r139w4MCBOuHg+Ff48z/nqbRs19RLNbXgnZ/Q71nhd8f9P9+6LWXfwdBQJ7vO3kWopXo1DMN8++23i59/prJSNXM0s/QOJtopjpd0hOp6fHYI638Xa/W8BQsXvfLKKx4enT7utpeiNF6bXq/funXr2tUrC4svTBvBXzjFPCGKux2ucd/JUnz8B2/rX3x3d4+nFix66qmn5HI520VxAqWxsywWy/fff7961dt/Z2T2CxQ9NMY4ZxwU9CnqNHUjvkrDtlSR8pyxf3jfZ59/8ZFHHnFzc7v2I50GpbHLTpw48fnnn3/5xbaaOvWEQcJ7RhjvjEeoD9tlcZWmCXuy8f1R/u4sCEWiu++e8fAj/05OTna2Xy86g9J4nYxG46+//rpjxxe/7vmlsUkf3992pXG6bofVhVrszsKPWYKDpywAf/y4sbPvf3DmzJl0WbiroDTeqObm5r/++mv37t27/vd1WUW1v0w0IdI0ZiAzNhLx4WwX51hV9TiUh9R8HC5wzSrQu0pcbrpp0oyZs+68806ZTMZ2dT0ApdFuLBZLZmbm/v37D+z/IzX1cGOToY9cPHaAMaEfk9APcQpbDze9idGMkyXIKMDRIqQXiE+VtAgEgpEjhk+cNGXixIljx451cel1y9ydKI3dwmg0Hjly5MCBA3+npyuVf1fXqAV83qC+4gRFy9A+zOAQRIf0yE3NOh1yy3CqFCdLcbRYlF1sNrRYPD1ch8fFjUocm5ycPG7cOPqh4rpRGh3h/PnzSqUyIyMjU3nkxInj1bUaAFJ34aBQ/uDglv7+UMgR5geF3NZXDRfU6VBcg/M1KK5GgQp5FcLcMp5KbQTg5ek2KDoqPiExISEhISEhKirKSXqR6m6URhbU1NTk5ubm5eXl5ubm5Z4oKDhbWq4ymcwAJGJ+mL+oj7c5UGqSeyJQhgAprCMyN3i7w0MC0Q1fQ83CQNuEej3q9VBpUalBdQNUWqi0qGoQlNQJi6vNDU0mADweLyjANzw8PHpwTHR09JAhQ6Kjo/v0oV1V3YLSyAkmk6msrKz4HyUlJVWqyuqqioqKiqrqWr2h5eKJXUR8D1eB1I3v5QYBD65ii+SfXu88xCaRgAHAMNAYbLeazGgw8AFomqAzMA1N5qbmS7pSFYuEcj9vf395YFCo3D8wNDQ0LCxMoVAoFIqwsDDa9nMYSmMPoNPpKisrtVqtWq3W6XQNDQ06na6+vl6r1VosFp1OZzTaOi223mIdl0ql1gYkj8ez7tKUSqUeHh6enp4eHh4ymczLy8vT0zMgIMDHpwduwvZGlEZCuII2vgnhCkojIVxBaSSEK/4/miVg5wRdZH0AAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "  \"input\": {\n",
    "    \"input\": \"good morning\"\n",
    "  },\n",
    "  \"plan\": [\n",
    "    {\n",
    "      \"key\": \"General_conv\",\n",
    "      \"value\": \"respond with 'good morning' to user\"\n",
    "    }\n",
    "  ],\n",
    "  \"past_steps\": [\n",
    "    \"respond with 'good morning' to user\",\n",
    "    \" Good morning there! I hope this new day brings you joy and positivity. Is there anything specific I can help you with today?\"\n",
    "  ],\n",
    "  \"response\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state['plan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial of 3 is 6\n"
     ]
    }
   ],
   "source": [
    "def factorial(x):\n",
    "    \"\"\"This is a recursive function\n",
    "    to find the factorial of an integer\"\"\"\n",
    "\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        k = (x * factorial(x-1))\n",
    "        return (x * factorial(x-1))\n",
    "\n",
    "\n",
    "num = 3\n",
    "print(\"The factorial of\", num, \"is\", factorial(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If 'exception_key' is specified then input must be a dictionary.However found a type of <class 'list'> for input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m runnable \u001b[38;5;241m=\u001b[39m RunnableGenerator(_generate_immediate_error)\u001b[38;5;241m.\u001b[39mwith_fallbacks(\n\u001b[1;32m     16\u001b[0m     [RunnableGenerator(_generate)],\n\u001b[1;32m     17\u001b[0m     exception_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# An error occurred.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/runnables/fallbacks.py:438\u001b[0m, in \u001b[0;36mRunnableWithFallbacks.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception_key\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is specified then input must be a dictionary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHowever found a type of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# setup callbacks\u001b[39;00m\n\u001b[1;32m    443\u001b[0m config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n",
      "\u001b[0;31mValueError\u001b[0m: If 'exception_key' is specified then input must be a dictionary.However found a type of <class 'list'> for input"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
    "    raise ValueError()\n",
    "    yield \"\"\n",
    "\n",
    "def _generate(input: Iterator) -> Iterator[str]:\n",
    "    try:\n",
    "        yield from input[\"exception\"].args[0]\n",
    "    except KeyError:\n",
    "        yield from \"An error occurred.\"\n",
    "\n",
    "runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
    "    [RunnableGenerator(_generate)],\n",
    "    exception_key=\"exception\"\n",
    ")\n",
    "print(''.join(runnable.stream([])))  # An error occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amaithi/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing is an exciting area.', 'Huge budget have been allocated for this.']\n",
      "['Natural', 'language', 'processing', 'is', 'an', 'exciting', 'area', '.', 'Huge', 'budget', 'have', 'been', 'allocated', 'for', 'this', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Natural language processing is an exciting area. Huge budget have been allocated for this.\"\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.output_parsers import (\n",
    "    BaseGenerationOutputParser,\n",
    "    BaseOutputParser,\n",
    "    JsonOutputParser,\n",
    ")\n",
    "from langchain_core.output_parsers.openai_functions import (\n",
    "    JsonOutputFunctionsParser,\n",
    "    PydanticAttrOutputFunctionsParser,\n",
    "    PydanticOutputFunctionsParser,\n",
    ")\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "from langchain.output_parsers import (\n",
    "    JsonOutputKeyToolsParser,\n",
    "    PydanticOutputParser,\n",
    "    PydanticToolsParser,\n",
    ")\n",
    "\n",
    "\n",
    "def create_openai_fn_runnable(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    enforce_single_function_usage: bool = True,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "    \"\"\"Create a runnable sequence that uses OpenAI functions.\n",
    "\n",
    "    Args:\n",
    "        functions: A sequence of either dictionaries, pydantic.BaseModels classes, or\n",
    "            Python functions. If dictionaries are passed in, they are assumed to\n",
    "            already be a valid OpenAI functions. If only a single\n",
    "            function is passed in, then it will be enforced that the model use that\n",
    "            function. pydantic.BaseModels and Python functions should have docstrings\n",
    "            describing what the function does. For best results, pydantic.BaseModels\n",
    "            should have descriptions of the parameters and Python functions should have\n",
    "            Google Python style args descriptions in the docstring. Additionally,\n",
    "            Python functions should only use primitive types (str, int, float, bool) or\n",
    "            pydantic.BaseModels for arguments.\n",
    "        llm: Language model to use, assumed to support the OpenAI function-calling API.\n",
    "        prompt: BasePromptTemplate to pass to the model.\n",
    "        enforce_single_function_usage: only used if a single function is passed in. If\n",
    "            True, then the model will be forced to use the given function. If False,\n",
    "            then the model will be given the option to use the given function or not.\n",
    "        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\n",
    "            will be inferred from the function types. If pydantic.BaseModels are passed\n",
    "            in, then the OutputParser will try to parse outputs using those. Otherwise\n",
    "            model outputs will simply be parsed as JSON. If multiple functions are\n",
    "            passed in and they are not pydantic.BaseModels, the chain output will\n",
    "            include both the name of the function that was returned and the arguments\n",
    "            to pass to the function.\n",
    "        **llm_kwargs: Additional named arguments to pass to the language model.\n",
    "\n",
    "    Returns:\n",
    "        A runnable sequence that will pass in the given functions to the model when run.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "                from typing import Optional\n",
    "\n",
    "                from langchain.chains.structured_output import create_openai_fn_runnable\n",
    "                from langchain_openai import ChatOpenAI\n",
    "                from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "                class RecordPerson(BaseModel):\n",
    "                    '''Record some identifying information about a person.'''\n",
    "\n",
    "                    name: str = Field(..., description=\"The person's name\")\n",
    "                    age: int = Field(..., description=\"The person's age\")\n",
    "                    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")\n",
    "\n",
    "\n",
    "                class RecordDog(BaseModel):\n",
    "                    '''Record some identifying information about a dog.'''\n",
    "\n",
    "                    name: str = Field(..., description=\"The dog's name\")\n",
    "                    color: str = Field(..., description=\"The dog's color\")\n",
    "                    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "\n",
    "                llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "                structured_llm = create_openai_fn_runnable([RecordPerson, RecordDog], llm)\n",
    "                structured_llm.invoke(\"Harry was a chubby brown beagle who loved chicken)\n",
    "                # -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\n",
    "    \"\"\"  # noqa: E501\n",
    "    if not functions:\n",
    "        raise ValueError(\"Need to pass in at least one function. Received zero.\")\n",
    "    openai_functions = [convert_to_openai_function(f) for f in functions]\n",
    "    llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions, **llm_kwargs}\n",
    "    if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "        llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "    output_parser = output_parser or get_openai_output_parser(functions)\n",
    "    if prompt:\n",
    "        return prompt | llm.bind(**llm_kwargs_) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs_) | output_parser\n",
    "\n",
    "\n",
    "def create_structured_output_runnable(\n",
    "    output_schema: Union[Dict[str, Any], Type[BaseModel]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    enforce_function_usage: bool = True,\n",
    "    return_single: bool = True,\n",
    "    mode: Literal[\n",
    "        \"openai-functions\", \"openai-tools\", \"openai-json\"\n",
    "    ] = \"openai-functions\",\n",
    "    **kwargs: Any,\n",
    ") -> Runnable:\n",
    "    \"\"\"Create a runnable for extracting structured outputs.\n",
    "\n",
    "    Args:\n",
    "        output_schema: Either a dictionary or pydantic.BaseModel class. If a dictionary\n",
    "            is passed in, it's assumed to already be a valid JsonSchema.\n",
    "            For best results, pydantic.BaseModels should have docstrings describing what\n",
    "            the schema represents and descriptions for the parameters.\n",
    "        llm: Language model to use. Assumed to support the OpenAI function-calling API \n",
    "            if mode is 'openai-function'. Assumed to support OpenAI response_format \n",
    "            parameter if mode is 'openai-json'.\n",
    "        prompt: BasePromptTemplate to pass to the model. If mode is 'openai-json' and \n",
    "            prompt has input variable 'output_schema' then the given output_schema \n",
    "            will be converted to a JsonSchema and inserted in the prompt.\n",
    "        output_parser: Output parser to use for parsing model outputs. By default\n",
    "            will be inferred from the function types. If pydantic.BaseModel is passed\n",
    "            in, then the OutputParser will try to parse outputs using the pydantic \n",
    "            class. Otherwise model outputs will be parsed as JSON.\n",
    "        mode: How structured outputs are extracted from the model. If 'openai-functions' \n",
    "            then OpenAI function calling is used with the deprecated 'functions', \n",
    "            'function_call' schema. If 'openai-tools' then OpenAI function \n",
    "            calling with the latest 'tools', 'tool_choice' schema is used. This is \n",
    "            recommended over 'openai-functions'. If 'openai-json' then OpenAI model \n",
    "            with response_format set to JSON is used.\n",
    "        enforce_function_usage: Only applies when mode is 'openai-tools' or \n",
    "            'openai-functions'. If True, then the model will be forced to use the given \n",
    "            output schema. If False, then the model can elect whether to use the output \n",
    "            schema.\n",
    "        return_single: Only applies when mode is 'openai-tools'. Whether to a list of \n",
    "            structured outputs or a single one. If True and model does not return any \n",
    "            structured outputs then chain output is None. If False and model does not \n",
    "            return any structured outputs then chain output is an empty list.\n",
    "        **kwargs: Additional named arguments.\n",
    "\n",
    "    Returns:\n",
    "        A runnable sequence that will return a structured output(s) matching the given \n",
    "            output_schema.\n",
    "    \n",
    "    OpenAI tools example with Pydantic schema (mode='openai-tools'):\n",
    "        .. code-block:: python\n",
    "        \n",
    "                from typing import Optional\n",
    "\n",
    "                from langchain.chains import create_structured_output_runnable\n",
    "                from langchain_openai import ChatOpenAI\n",
    "                from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "                class RecordDog(BaseModel):\n",
    "                    '''Record some identifying information about a dog.'''\n",
    "\n",
    "                    name: str = Field(..., description=\"The dog's name\")\n",
    "                    color: str = Field(..., description=\"The dog's color\")\n",
    "                    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "                llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", \"You are an extraction algorithm. Please extract every possible instance\"), \n",
    "                        ('human', '{input}')\n",
    "                    ]\n",
    "                )\n",
    "                structured_llm = create_structured_output_runnable(\n",
    "                    RecordDog, \n",
    "                    llm, \n",
    "                    mode=\"openai-tools\", \n",
    "                    enforce_function_usage=True, \n",
    "                    return_single=True\n",
    "                )\n",
    "                structured_llm.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "                # -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\n",
    "                \n",
    "    OpenAI tools example with dict schema (mode=\"openai-tools\"):\n",
    "        .. code-block:: python\n",
    "        \n",
    "                from typing import Optional\n",
    "\n",
    "                from langchain.chains import create_structured_output_runnable\n",
    "                from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "                dog_schema = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"record_dog\",\n",
    "                        \"description\": \"Record some identifying information about a dog.\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": {\n",
    "                                    \"description\": \"The dog's name\",\n",
    "                                    \"type\": \"string\"\n",
    "                                },\n",
    "                                \"color\": {\n",
    "                                    \"description\": \"The dog's color\",\n",
    "                                    \"type\": \"string\"\n",
    "                                },\n",
    "                                \"fav_food\": {\n",
    "                                    \"description\": \"The dog's favorite food\",\n",
    "                                    \"type\": \"string\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\"name\", \"color\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "\n",
    "\n",
    "                llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "                structured_llm = create_structured_output_runnable(\n",
    "                    doc_schema, \n",
    "                    llm, \n",
    "                    mode=\"openai-tools\", \n",
    "                    enforce_function_usage=True, \n",
    "                    return_single=True\n",
    "                )\n",
    "                structured_llm.invoke(\"Harry was a chubby brown beagle who loved chicken\")\n",
    "                # -> {'name': 'Harry', 'color': 'brown', 'fav_food': 'chicken'}\n",
    "    \n",
    "    OpenAI functions example (mode=\"openai-functions\"):\n",
    "        .. code-block:: python\n",
    "\n",
    "                from typing import Optional\n",
    "\n",
    "                from langchain.chains import create_structured_output_runnable\n",
    "                from langchain_openai import ChatOpenAI\n",
    "                from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "                class Dog(BaseModel):\n",
    "                    '''Identifying information about a dog.'''\n",
    "\n",
    "                    name: str = Field(..., description=\"The dog's name\")\n",
    "                    color: str = Field(..., description=\"The dog's color\")\n",
    "                    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "                llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "                structured_llm = create_structured_output_runnable(Dog, llm, mode=\"openai-functions\")\n",
    "                structured_llm.invoke(\"Harry was a chubby brown beagle who loved chicken\")\n",
    "                # -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\n",
    "                \n",
    "    OpenAI functions with prompt example:\n",
    "        .. code-block:: python\n",
    "\n",
    "                from typing import Optional\n",
    "\n",
    "                from langchain.chains import create_structured_output_runnable\n",
    "                from langchain_openai import ChatOpenAI\n",
    "                from langchain_core.prompts import ChatPromptTemplate\n",
    "                from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "                class Dog(BaseModel):\n",
    "                    '''Identifying information about a dog.'''\n",
    "\n",
    "                    name: str = Field(..., description=\"The dog's name\")\n",
    "                    color: str = Field(..., description=\"The dog's color\")\n",
    "                    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "                llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "                structured_llm = create_structured_output_runnable(Dog, llm, mode=\"openai-functions\")\n",
    "                system = '''Extract information about any dogs mentioned in the user input.'''\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [(\"system\", system), (\"human\", \"{input}\"),]\n",
    "                )\n",
    "                chain = prompt | structured_llm\n",
    "                chain.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "                # -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\n",
    "    OpenAI json response format example (mode=\"openai-json\"):\n",
    "        .. code-block:: python\n",
    "        \n",
    "                from typing import Optional\n",
    "\n",
    "                from langchain.chains import create_structured_output_runnable\n",
    "                from langchain_openai import ChatOpenAI\n",
    "                from langchain_core.prompts import ChatPromptTemplate\n",
    "                from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "                class Dog(BaseModel):\n",
    "                    '''Identifying information about a dog.'''\n",
    "\n",
    "                    name: str = Field(..., description=\"The dog's name\")\n",
    "                    color: str = Field(..., description=\"The dog's color\")\n",
    "                    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "                llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "                structured_llm = create_structured_output_runnable(Dog, llm, mode=\"openai-json\")\n",
    "                system = '''You are a world class assistant for extracting information in structured JSON formats. \\\n",
    "                \n",
    "                Extract a valid JSON blob from the user input that matches the following JSON Schema:\n",
    "                \n",
    "                {output_schema}'''\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [(\"system\", system), (\"human\", \"{input}\"),]\n",
    "                )\n",
    "                chain = prompt | structured_llm\n",
    "                chain.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "    \"\"\"  # noqa: E501\n",
    "    # for backwards compatibility\n",
    "    force_function_usage = kwargs.get(\n",
    "        \"enforce_single_function_usage\", enforce_function_usage\n",
    "    )\n",
    "\n",
    "    if mode == \"openai-tools\":\n",
    "        # Protect against typos in kwargs\n",
    "        keys_in_kwargs = set(kwargs.keys())\n",
    "        # Backwards compatibility keys\n",
    "        unrecognized_keys = keys_in_kwargs - {\"enforce_single_function_usage\"}\n",
    "        if unrecognized_keys:\n",
    "            raise TypeError(\n",
    "                f\"Got an unexpected keyword argument(s): {unrecognized_keys}.\"\n",
    "            )\n",
    "\n",
    "        return _create_openai_tools_runnable(\n",
    "            output_schema,\n",
    "            llm,\n",
    "            prompt=prompt,\n",
    "            output_parser=output_parser,\n",
    "            enforce_tool_usage=force_function_usage,\n",
    "            first_tool_only=return_single,\n",
    "        )\n",
    "\n",
    "    elif mode == \"openai-functions\":\n",
    "        return _create_openai_functions_structured_output_runnable(\n",
    "            output_schema,\n",
    "            llm,\n",
    "            prompt=prompt,\n",
    "            output_parser=output_parser,\n",
    "            enforce_single_function_usage=force_function_usage,\n",
    "            **kwargs,  # llm-specific kwargs\n",
    "        )\n",
    "    elif mode == \"openai-json\":\n",
    "        if force_function_usage:\n",
    "            raise ValueError(\n",
    "                \"enforce_single_function_usage is not supported for mode='openai-json'.\"\n",
    "            )\n",
    "        return _create_openai_json_runnable(\n",
    "            output_schema, llm, prompt=prompt, output_parser=output_parser, **kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid mode {mode}. Expected one of 'openai-tools', 'openai-functions', \"\n",
    "            f\"'openai-json'.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _create_openai_tools_runnable(\n",
    "    tool: Union[Dict[str, Any], Type[BaseModel], Callable],\n",
    "    llm: Runnable,\n",
    "    *,\n",
    "    prompt: Optional[BasePromptTemplate],\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]],\n",
    "    enforce_tool_usage: bool,\n",
    "    first_tool_only: bool,\n",
    ") -> Runnable:\n",
    "    oai_tool = convert_to_openai_tool(tool)\n",
    "    llm_kwargs: Dict[str, Any] = {\"tools\": [oai_tool]}\n",
    "    if enforce_tool_usage:\n",
    "        llm_kwargs[\"tool_choice\"] = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\"name\": oai_tool[\"function\"][\"name\"]},\n",
    "        }\n",
    "    output_parser = output_parser or _get_openai_tool_output_parser(\n",
    "        tool, first_tool_only=first_tool_only\n",
    "    )\n",
    "    if prompt:\n",
    "        return prompt | llm.bind(**llm_kwargs) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs) | output_parser\n",
    "\n",
    "\n",
    "def _get_openai_tool_output_parser(\n",
    "    tool: Union[Dict[str, Any], Type[BaseModel], Callable],\n",
    "    *,\n",
    "    first_tool_only: bool = False,\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    if isinstance(tool, type) and issubclass(tool, BaseModel):\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticToolsParser(tools=[tool], first_tool_only=first_tool_only)\n",
    "    else:\n",
    "        key_name = convert_to_openai_tool(tool)[\"function\"][\"name\"]\n",
    "        output_parser = JsonOutputKeyToolsParser(\n",
    "            first_tool_only=first_tool_only, key_name=key_name\n",
    "        )\n",
    "    return output_parser\n",
    "\n",
    "\n",
    "def get_openai_output_parser(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    \"\"\"Get the appropriate function output parser given the user functions.\n",
    "\n",
    "    Args:\n",
    "        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\n",
    "            or a Python function. If a dictionary is passed in, it is assumed to\n",
    "            already be a valid OpenAI function.\n",
    "\n",
    "    Returns:\n",
    "        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\n",
    "            a JsonOutputFunctionsParser. If there's only one function and it is\n",
    "            not a Pydantic class, then the output parser will automatically extract\n",
    "            only the function arguments and not the function name.\n",
    "    \"\"\"\n",
    "    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\n",
    "        if len(functions) > 1:\n",
    "            pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "        else:\n",
    "            pydantic_schema = functions[0]\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    else:\n",
    "        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\n",
    "    return output_parser\n",
    "\n",
    "\n",
    "def _create_openai_json_runnable(\n",
    "    output_schema: Union[Dict[str, Any], Type[BaseModel]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    ") -> Runnable:\n",
    "    \"\"\"\"\"\"\n",
    "    if isinstance(output_schema, type) and issubclass(output_schema, BaseModel):\n",
    "        output_parser = output_parser or PydanticOutputParser(\n",
    "            pydantic_object=output_schema,\n",
    "        )\n",
    "        schema_as_dict = convert_to_openai_function(output_schema)[\"parameters\"]\n",
    "    else:\n",
    "        output_parser = output_parser or JsonOutputParser()\n",
    "        schema_as_dict = output_schema\n",
    "\n",
    "    llm = llm.bind(response_format={\"type\": \"json_object\"})\n",
    "    if prompt:\n",
    "        if \"output_schema\" in prompt.input_variables:\n",
    "            prompt = prompt.partial(output_schema=json.dumps(schema_as_dict, indent=2))\n",
    "\n",
    "        return prompt | llm | output_parser\n",
    "    else:\n",
    "        return llm | output_parser\n",
    "\n",
    "\n",
    "def _create_openai_functions_structured_output_runnable(\n",
    "    output_schema: Union[Dict[str, Any], Type[BaseModel]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "    if isinstance(output_schema, dict):\n",
    "        function: Any = {\n",
    "            \"name\": \"output_formatter\",\n",
    "            \"description\": (\n",
    "                \"Output formatter. Should always be used to format your response to the\"\n",
    "                \" user.\"\n",
    "            ),\n",
    "            \"parameters\": output_schema,\n",
    "        }\n",
    "    else:\n",
    "\n",
    "        class _OutputFormatter(BaseModel):\n",
    "            \"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"  # noqa: E501\n",
    "\n",
    "            output: output_schema  # type: ignore\n",
    "\n",
    "        function = _OutputFormatter\n",
    "        output_parser = output_parser or PydanticAttrOutputFunctionsParser(\n",
    "            pydantic_schema=_OutputFormatter, attr_name=\"output\"\n",
    "        )\n",
    "    return create_openai_fn_runnable(\n",
    "        [function],\n",
    "        llm,\n",
    "        prompt=prompt,\n",
    "        output_parser=output_parser,\n",
    "        **llm_kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaithi/projects/weaviate/langgraph-final/.venv/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatGroq.with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing_extensions import List\n",
    "from typing import Literal\n",
    "\n",
    "class Step(BaseModel):\n",
    "    key: Literal[\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"] = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Step] \n",
    "    \n",
    "\n",
    "\n",
    "crews = [\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"]\n",
    "\n",
    "function = {'name': 'plan',\n",
    " 'description': '',\n",
    " \n",
    " 'parameters': {\n",
    "   'type': 'array',\n",
    "   'properties': {\n",
    "     'key': {\n",
    "       \"enum\": f\"{crews}\",\n",
    "       'description': 'the worker gonna handle this task/step',\n",
    "       'type': 'string'},\n",
    "     'value': {\n",
    "       'description': 'task/ step the worker need to do',\n",
    "      'type': 'string'}\n",
    "     },\n",
    "                    \n",
    "    'required': ['plan'],\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "\n",
    "    \n",
    "# from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given user input, come up with a simple step by step plan but don't provide answer coz you have tools to figure out things. if the input requires multiple steps(combintion of multiple tools) then create a list or else just give direct single step with proper input to the selected tool.\\\n",
    "        \n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "    \n",
    "    if the user asks anything related to food, receipies, and it's related stuffs use 'Food_crew' key,\n",
    "        if the user asks anything related to mediwave and it's related stuffs use 'Mediwave_rag' key,    \n",
    "        if the user makes conversation like hi, hello and something like that, jokes and funny conversations then use 'General_conv' key and don't provide answer to it just give what user asked,\n",
    "        if the user asks anything related to weather, time, wikipedia and it's related stuffs use 'General_other' key,\n",
    "        if the user asks anything related to travel, exploration, city tour and it's related stuffs use 'Travel_crew' key.\n",
    "    \n",
    "    always plan minimal steps. make multiple steps only if it is necessary. don't make unnecessary steps, make sure the steps satisfy the original user input.\n",
    "    \n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "only add the actionable step not the empty steps..\n",
    "\n",
    "user input : {objective}\"\"\"\n",
    ")\n",
    "\n",
    "# llm = ChatOllama(model=os.environ['LLM'], \n",
    "#                  stop= [\n",
    "#                    os.environ['LLM_START_PARAM'],\n",
    "#                    os.environ['LLM_STOP_PARAM']\n",
    "#                    ]\n",
    "#                  )\n",
    "\n",
    "llm = ChatGroq(\n",
    "  model=os.environ['LLM'],\n",
    "  api_key=os.environ['GROQ_API_KEY']\n",
    ")\n",
    "\n",
    "planner = planner_prompt | llm.with_structured_output(\n",
    "  schema=Plan,\n",
    "  method='function_calling'\n",
    ")\n",
    "\n",
    "\n",
    "# planner = create_structured_output_runnable(\n",
    "#     function, \n",
    "#     llm,\n",
    "#     planner_prompt,\n",
    "#     enforce_function_usage = True,\n",
    "# ).with_retry(\n",
    "#   retry_if_exception_type = (ValueError, KeyError),\n",
    "#   stop_after_attempt = 4\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = planner.invoke({\"objective\": \"good morning\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'key': 'General_conv', 'value': 'good morning'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan.dict()['steps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Groq Chat wrapper.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterator,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    TypedDict,\n",
    "    Union,\n",
    "    cast,\n",
    ")\n",
    "\n",
    "from langchain_core._api import beta\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.language_models.chat_models import (\n",
    "    BaseChatModel,\n",
    "    agenerate_from_stream,\n",
    "    generate_from_stream,\n",
    ")\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    BaseMessageChunk,\n",
    "    ChatMessage,\n",
    "    ChatMessageChunk,\n",
    "    FunctionMessage,\n",
    "    FunctionMessageChunk,\n",
    "    HumanMessage,\n",
    "    HumanMessageChunk,\n",
    "    SystemMessage,\n",
    "    SystemMessageChunk,\n",
    "    ToolMessage,\n",
    "    ToolMessageChunk,\n",
    ")\n",
    "from langchain_core.output_parsers import (\n",
    "    JsonOutputParser,\n",
    "    PydanticOutputParser,\n",
    ")\n",
    "from langchain_core.output_parsers.base import OutputParserLike\n",
    "from langchain_core.output_parsers.openai_tools import (\n",
    "    JsonOutputKeyToolsParser,\n",
    "    PydanticToolsParser,\n",
    "    make_invalid_tool_call,\n",
    "    parse_tool_call,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator\n",
    "from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils import (\n",
    "    convert_to_secret_str,\n",
    "    get_from_dict_or_env,\n",
    "    get_pydantic_field_names,\n",
    ")\n",
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "\n",
    "class ChatGroq(BaseChatModel):\n",
    "    \"\"\"`Groq` Chat large language models API.\n",
    "\n",
    "    To use, you should have the\n",
    "    environment variable ``GROQ_API_KEY`` set with your API key.\n",
    "\n",
    "    Any parameters that are valid to be passed to the groq.create call can be passed\n",
    "    in, even if not explicitly saved on this class.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain_groq import ChatGroq\n",
    "\n",
    "            model = ChatGroq(model_name=\"mixtral-8x7b-32768\")\n",
    "    \"\"\"\n",
    "\n",
    "    client: Any = Field(default=None, exclude=True)  #: :meta private:\n",
    "    async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n",
    "    model_name: str = Field(default=\"mixtral-8x7b-32768\", alias=\"model\")\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n",
    "    groq_api_key: Optional[SecretStr] = Field(default=None, alias=\"api_key\")\n",
    "    \"\"\"Automatically inferred from env var `groq_API_KEY` if not provided.\"\"\"\n",
    "    groq_api_base: Optional[str] = Field(default=None, alias=\"base_url\")\n",
    "    \"\"\"Base URL path for API requests, leave blank if not using a proxy or service\n",
    "        emulator.\"\"\"\n",
    "    # to support explicit proxy for Groq\n",
    "    groq_proxy: Optional[str] = None\n",
    "    request_timeout: Union[float, Tuple[float, float], Any, None] = Field(\n",
    "        default=None, alias=\"timeout\"\n",
    "    )\n",
    "    \"\"\"Timeout for requests to Groq completion API. Can be float, httpx.Timeout or\n",
    "        None.\"\"\"\n",
    "    max_retries: int = 2\n",
    "    \"\"\"Maximum number of retries to make when generating.\"\"\"\n",
    "    streaming: bool = False\n",
    "    \"\"\"Whether to stream the results or not.\"\"\"\n",
    "    n: int = 1\n",
    "    \"\"\"Number of chat completions to generate for each prompt.\"\"\"\n",
    "    max_tokens: Optional[int] = None\n",
    "    \"\"\"Maximum number of tokens to generate.\"\"\"\n",
    "    default_headers: Union[Mapping[str, str], None] = None\n",
    "    default_query: Union[Mapping[str, object], None] = None\n",
    "    # Configure a custom httpx client. See the\n",
    "    # [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n",
    "    http_client: Union[Any, None] = None\n",
    "    \"\"\"Optional httpx.Client.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\n",
    "        all_required_field_names = get_pydantic_field_names(cls)\n",
    "        extra = values.get(\"model_kwargs\", {})\n",
    "        for field_name in list(values):\n",
    "            if field_name in extra:\n",
    "                raise ValueError(f\"Found {field_name} supplied twice.\")\n",
    "            if field_name not in all_required_field_names:\n",
    "                warnings.warn(\n",
    "                    f\"\"\"WARNING! {field_name} is not default parameter.\n",
    "                    {field_name} was transferred to model_kwargs.\n",
    "                    Please confirm that {field_name} is what you intended.\"\"\"\n",
    "                )\n",
    "                extra[field_name] = values.pop(field_name)\n",
    "\n",
    "        invalid_model_kwargs = all_required_field_names.intersection(extra.keys())\n",
    "        if invalid_model_kwargs:\n",
    "            raise ValueError(\n",
    "                f\"Parameters {invalid_model_kwargs} should be specified explicitly. \"\n",
    "                f\"Instead they were passed in as part of `model_kwargs` parameter.\"\n",
    "            )\n",
    "\n",
    "        values[\"model_kwargs\"] = extra\n",
    "        return values\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n",
    "        if values[\"n\"] < 1:\n",
    "            raise ValueError(\"n must be at least 1.\")\n",
    "        if values[\"n\"] > 1 and values[\"streaming\"]:\n",
    "            raise ValueError(\"n must be 1 when streaming.\")\n",
    "\n",
    "        if values[\"temperature\"] == 0:\n",
    "            values[\"temperature\"] = 1e-8\n",
    "\n",
    "        values[\"groq_api_key\"] = convert_to_secret_str(\n",
    "            get_from_dict_or_env(values, \"groq_api_key\", \"GROQ_API_KEY\")\n",
    "        )\n",
    "        values[\"groq_api_base\"] = values[\"groq_api_base\"] or os.getenv(\"GROQ_API_BASE\")\n",
    "        values[\"groq_proxy\"] = values[\"groq_proxy\"] = os.getenv(\"GROQ_PROXY\")\n",
    "\n",
    "        client_params = {\n",
    "            \"api_key\": values[\"groq_api_key\"].get_secret_value(),\n",
    "            \"base_url\": values[\"groq_api_base\"],\n",
    "            \"timeout\": values[\"request_timeout\"],\n",
    "            \"max_retries\": values[\"max_retries\"],\n",
    "            \"default_headers\": values[\"default_headers\"],\n",
    "            \"default_query\": values[\"default_query\"],\n",
    "            \"http_client\": values[\"http_client\"],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            import groq\n",
    "\n",
    "            if not values.get(\"client\"):\n",
    "                values[\"client\"] = groq.Groq(**client_params).chat.completions\n",
    "            if not values.get(\"async_client\"):\n",
    "                values[\"async_client\"] = groq.AsyncGroq(\n",
    "                    **client_params\n",
    "                ).chat.completions\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Could not import groq python package. \"\n",
    "                \"Please install it with `pip install groq`.\"\n",
    "            )\n",
    "        return values\n",
    "\n",
    "    #\n",
    "    # Serializable class method overrides\n",
    "    #\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        return {\"groq_api_key\": \"GROQ_API_KEY\"}\n",
    "\n",
    "    @classmethod\n",
    "    def is_lc_serializable(cls) -> bool:\n",
    "        \"\"\"Return whether this model can be serialized by Langchain.\"\"\"\n",
    "        return True\n",
    "\n",
    "    #\n",
    "    # BaseChatModel method overrides\n",
    "    #\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of model.\"\"\"\n",
    "        return \"groq-chat\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        if self.streaming:\n",
    "            stream_iter = self._stream(\n",
    "                messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "            )\n",
    "            return generate_from_stream(stream_iter)\n",
    "        message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "        params = {\n",
    "            **params,\n",
    "            **kwargs,\n",
    "        }\n",
    "        response = self.client.create(messages=message_dicts, **params)\n",
    "        return self._create_chat_result(response)\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        if self.streaming:\n",
    "            stream_iter = self._astream(\n",
    "                messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "            )\n",
    "            return await agenerate_from_stream(stream_iter)\n",
    "\n",
    "        message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "        params = {\n",
    "            **params,\n",
    "            **kwargs,\n",
    "        }\n",
    "        response = await self.async_client.create(messages=message_dicts, **params)\n",
    "        return self._create_chat_result(response)\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "\n",
    "        # groq api does not support streaming with tools yet\n",
    "        if \"tools\" in kwargs:\n",
    "            response = self.client.create(\n",
    "                messages=message_dicts, **{**params, **kwargs}\n",
    "            )\n",
    "            chat_result = self._create_chat_result(response)\n",
    "            generation = chat_result.generations[0]\n",
    "            message = generation.message\n",
    "            tool_call_chunks = [\n",
    "                {\n",
    "                    \"name\": rtc[\"function\"].get(\"name\"),\n",
    "                    \"args\": rtc[\"function\"].get(\"arguments\"),\n",
    "                    \"id\": rtc.get(\"id\"),\n",
    "                    \"index\": rtc.get(\"index\"),\n",
    "                }\n",
    "                for rtc in message.additional_kwargs.get(\"tool_calls\", [])\n",
    "            ]\n",
    "            chunk_ = ChatGenerationChunk(\n",
    "                message=AIMessageChunk(\n",
    "                    content=message.content,\n",
    "                    additional_kwargs=message.additional_kwargs,\n",
    "                    tool_call_chunks=tool_call_chunks,\n",
    "                ),\n",
    "                generation_info=generation.generation_info,\n",
    "            )\n",
    "            if run_manager:\n",
    "                geninfo = chunk_.generation_info or {}\n",
    "                run_manager.on_llm_new_token(\n",
    "                    chunk_.text,\n",
    "                    chunk=chunk_,\n",
    "                    logprobs=geninfo.get(\"logprobs\"),\n",
    "                )\n",
    "            yield chunk_\n",
    "            return\n",
    "\n",
    "        params = {**params, **kwargs, \"stream\": True}\n",
    "\n",
    "        default_chunk_class = AIMessageChunk\n",
    "        for chunk in self.client.create(messages=message_dicts, **params):\n",
    "            if not isinstance(chunk, dict):\n",
    "                chunk = chunk.dict()\n",
    "            if len(chunk[\"choices\"]) == 0:\n",
    "                continue\n",
    "            choice = chunk[\"choices\"][0]\n",
    "            chunk = _convert_delta_to_message_chunk(\n",
    "                choice[\"delta\"], default_chunk_class\n",
    "            )\n",
    "            generation_info = {}\n",
    "            if finish_reason := choice.get(\"finish_reason\"):\n",
    "                generation_info[\"finish_reason\"] = finish_reason\n",
    "            logprobs = choice.get(\"logprobs\")\n",
    "            if logprobs:\n",
    "                generation_info[\"logprobs\"] = logprobs\n",
    "            default_chunk_class = chunk.__class__\n",
    "            chunk = ChatGenerationChunk(\n",
    "                message=chunk, generation_info=generation_info or None\n",
    "            )\n",
    "\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk, logprobs=logprobs)\n",
    "            yield chunk\n",
    "\n",
    "    async def _astream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AsyncIterator[ChatGenerationChunk]:\n",
    "        message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "\n",
    "        # groq api does not support streaming with tools yet\n",
    "        if \"tools\" in kwargs:\n",
    "            response = await self.async_client.create(\n",
    "                messages=message_dicts, **{**params, **kwargs}\n",
    "            )\n",
    "            chat_result = self._create_chat_result(response)\n",
    "            generation = chat_result.generations[0]\n",
    "            message = generation.message\n",
    "            tool_call_chunks = [\n",
    "                {\n",
    "                    \"name\": rtc[\"function\"].get(\"name\"),\n",
    "                    \"args\": rtc[\"function\"].get(\"arguments\"),\n",
    "                    \"id\": rtc.get(\"id\"),\n",
    "                    \"index\": rtc.get(\"index\"),\n",
    "                }\n",
    "                for rtc in message.additional_kwargs.get(\"tool_calls\", [])\n",
    "            ]\n",
    "            chunk_ = ChatGenerationChunk(\n",
    "                message=AIMessageChunk(\n",
    "                    content=message.content,\n",
    "                    additional_kwargs=message.additional_kwargs,\n",
    "                    tool_call_chunks=tool_call_chunks,\n",
    "                ),\n",
    "                generation_info=generation.generation_info,\n",
    "            )\n",
    "            if run_manager:\n",
    "                geninfo = chunk_.generation_info or {}\n",
    "                await run_manager.on_llm_new_token(\n",
    "                    chunk_.text,\n",
    "                    chunk=chunk_,\n",
    "                    logprobs=geninfo.get(\"logprobs\"),\n",
    "                )\n",
    "            yield chunk_\n",
    "            return\n",
    "\n",
    "        params = {**params, **kwargs, \"stream\": True}\n",
    "\n",
    "        default_chunk_class = AIMessageChunk\n",
    "        async for chunk in await self.async_client.create(\n",
    "            messages=message_dicts, **params\n",
    "        ):\n",
    "            if not isinstance(chunk, dict):\n",
    "                chunk = chunk.dict()\n",
    "            if len(chunk[\"choices\"]) == 0:\n",
    "                continue\n",
    "            choice = chunk[\"choices\"][0]\n",
    "            chunk = _convert_delta_to_message_chunk(\n",
    "                choice[\"delta\"], default_chunk_class\n",
    "            )\n",
    "            generation_info = {}\n",
    "            if finish_reason := choice.get(\"finish_reason\"):\n",
    "                generation_info[\"finish_reason\"] = finish_reason\n",
    "            logprobs = choice.get(\"logprobs\")\n",
    "            if logprobs:\n",
    "                generation_info[\"logprobs\"] = logprobs\n",
    "            default_chunk_class = chunk.__class__\n",
    "            chunk = ChatGenerationChunk(\n",
    "                message=chunk, generation_info=generation_info or None\n",
    "            )\n",
    "\n",
    "            if run_manager:\n",
    "                await run_manager.on_llm_new_token(\n",
    "                    token=chunk.text, chunk=chunk, logprobs=logprobs\n",
    "                )\n",
    "            yield chunk\n",
    "\n",
    "    #\n",
    "    # Internal methods\n",
    "    #\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the default parameters for calling Groq API.\"\"\"\n",
    "        params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"stream\": self.streaming,\n",
    "            \"n\": self.n,\n",
    "            \"temperature\": self.temperature,\n",
    "            **self.model_kwargs,\n",
    "        }\n",
    "        if self.max_tokens is not None:\n",
    "            params[\"max_tokens\"] = self.max_tokens\n",
    "        return params\n",
    "\n",
    "    def _create_chat_result(self, response: Union[dict, BaseModel]) -> ChatResult:\n",
    "        generations = []\n",
    "        if not isinstance(response, dict):\n",
    "            response = response.dict()\n",
    "        for res in response[\"choices\"]:\n",
    "            message = _convert_dict_to_message(res[\"message\"])\n",
    "            generation_info = dict(finish_reason=res.get(\"finish_reason\"))\n",
    "            if \"logprobs\" in res:\n",
    "                generation_info[\"logprobs\"] = res[\"logprobs\"]\n",
    "            gen = ChatGeneration(\n",
    "                message=message,\n",
    "                generation_info=generation_info,\n",
    "            )\n",
    "            generations.append(gen)\n",
    "        token_usage = response.get(\"usage\", {})\n",
    "        llm_output = {\n",
    "            \"token_usage\": token_usage,\n",
    "            \"model_name\": self.model_name,\n",
    "            \"system_fingerprint\": response.get(\"system_fingerprint\", \"\"),\n",
    "        }\n",
    "        return ChatResult(generations=generations, llm_output=llm_output)\n",
    "\n",
    "    def _create_message_dicts(\n",
    "        self, messages: List[BaseMessage], stop: Optional[List[str]]\n",
    "    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "        params = self._default_params\n",
    "        if stop is not None:\n",
    "            if \"stop\" in params:\n",
    "                raise ValueError(\"`stop` found in both the input and default params.\")\n",
    "            params[\"stop\"] = stop\n",
    "        message_dicts = [_convert_message_to_dict(m) for m in messages]\n",
    "        return message_dicts, params\n",
    "\n",
    "    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:\n",
    "        overall_token_usage: dict = {}\n",
    "        system_fingerprint = None\n",
    "        for output in llm_outputs:\n",
    "            if output is None:\n",
    "                # Happens in streaming\n",
    "                continue\n",
    "            token_usage = output[\"token_usage\"]\n",
    "            if token_usage is not None:\n",
    "                for k, v in token_usage.items():\n",
    "                    if k in overall_token_usage and v is not None:\n",
    "                        overall_token_usage[k] += v\n",
    "                    else:\n",
    "                        overall_token_usage[k] = v\n",
    "            if system_fingerprint is None:\n",
    "                system_fingerprint = output.get(\"system_fingerprint\")\n",
    "        combined = {\"token_usage\": overall_token_usage, \"model_name\": self.model_name}\n",
    "        if system_fingerprint:\n",
    "            combined[\"system_fingerprint\"] = system_fingerprint\n",
    "        return combined\n",
    "\n",
    "    def bind_functions(\n",
    "        self,\n",
    "        functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        function_call: Optional[\n",
    "            Union[_FunctionCall, str, Literal[\"auto\", \"none\"]]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind functions (and other objects) to this chat model.\n",
    "\n",
    "        Model is compatible with OpenAI function-calling API.\n",
    "\n",
    "        NOTE: Using bind_tools is recommended instead, as the `functions` and\n",
    "            `function_call` request parameters are officially deprecated.\n",
    "\n",
    "        Args:\n",
    "            functions: A list of function definitions to bind to this chat model.\n",
    "                Can be  a dictionary, pydantic model, or callable. Pydantic\n",
    "                models and callables will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            function_call: Which function to require the model to call.\n",
    "                Must be the name of the single provided function or\n",
    "                \"auto\" to automatically determine which function to call\n",
    "                (if any).\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "\n",
    "        formatted_functions = [convert_to_openai_function(fn) for fn in functions]\n",
    "        if function_call is not None:\n",
    "            function_call = (\n",
    "                {\"name\": function_call}\n",
    "                if isinstance(function_call, str)\n",
    "                and function_call not in (\"auto\", \"none\")\n",
    "                else function_call\n",
    "            )\n",
    "            if isinstance(function_call, dict) and len(formatted_functions) != 1:\n",
    "                raise ValueError(\n",
    "                    \"When specifying `function_call`, you must provide exactly one \"\n",
    "                    \"function.\"\n",
    "                )\n",
    "            if (\n",
    "                isinstance(function_call, dict)\n",
    "                and formatted_functions[0][\"name\"] != function_call[\"name\"]\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"Function call {function_call} was specified, but the only \"\n",
    "                    f\"provided function was {formatted_functions[0]['name']}.\"\n",
    "                )\n",
    "            kwargs = {**kwargs, \"function_call\": function_call}\n",
    "        return super().bind(\n",
    "            functions=formatted_functions,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[dict, str, Literal[\"auto\", \"any\", \"none\"], bool]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Must be the name of the single provided function,\n",
    "                \"auto\" to automatically determine which function to call\n",
    "                with the option to not call any function, \"any\" to enforce that some\n",
    "                function is called, or a dict of the form:\n",
    "                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "\n",
    "        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "        if tool_choice is not None and tool_choice:\n",
    "            if isinstance(tool_choice, str) and (\n",
    "                tool_choice not in (\"auto\", \"any\", \"none\")\n",
    "            ):\n",
    "                tool_choice = {\"type\": \"function\", \"function\": {\"name\": tool_choice}}\n",
    "            if isinstance(tool_choice, dict) and (len(formatted_tools) != 1):\n",
    "                raise ValueError(\n",
    "                    \"When specifying `tool_choice`, you must provide exactly one \"\n",
    "                    f\"tool. Received {len(formatted_tools)} tools.\"\n",
    "                )\n",
    "            if isinstance(tool_choice, dict) and (\n",
    "                formatted_tools[0][\"function\"][\"name\"]\n",
    "                != tool_choice[\"function\"][\"name\"]\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"Tool choice {tool_choice} was specified, but the only \"\n",
    "                    f\"provided tool was {formatted_tools[0]['function']['name']}.\"\n",
    "                )\n",
    "            if isinstance(tool_choice, bool):\n",
    "                if len(tools) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"tool_choice can only be True when there is one tool. Received \"\n",
    "                        f\"{len(tools)} tools.\"\n",
    "                    )\n",
    "                tool_name = formatted_tools[0][\"function\"][\"name\"]\n",
    "                tool_choice = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": tool_name},\n",
    "                }\n",
    "\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        return super().bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "    @beta()\n",
    "    def with_structured_output(\n",
    "        self,\n",
    "        schema: Optional[Union[Dict, Type[BaseModel]]] = None,\n",
    "        *,\n",
    "        method: Literal[\"function_calling\", \"json_mode\"] = \"function_calling\",\n",
    "        include_raw: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n",
    "        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n",
    "\n",
    "        Args:\n",
    "            schema: The output schema as a dict or a Pydantic class. If a Pydantic class\n",
    "                then the model output will be an object of that class. If a dict then\n",
    "                the model output will be a dict. With a Pydantic class the returned\n",
    "                attributes will be validated, whereas with a dict they will not be. If\n",
    "                `method` is \"function_calling\" and `schema` is a dict, then the dict\n",
    "                must match the OpenAI function-calling spec.\n",
    "            method: The method for steering model generation, either \"function_calling\"\n",
    "                or \"json_mode\". If \"function_calling\" then the schema will be converted\n",
    "                to a OpenAI function and the returned model will make use of the\n",
    "                function-calling API. If \"json_mode\" then Groq's JSON mode will be\n",
    "                used. Note that if using \"json_mode\" then you must include instructions\n",
    "                for formatting the output into the desired schema into the model call.\n",
    "            include_raw: If False then only the parsed structured output is returned. If\n",
    "                an error occurs during model output parsing it will be raised. If True\n",
    "                then both the raw model response (a BaseMessage) and the parsed model\n",
    "                response will be returned. If an error occurs during output parsing it\n",
    "                will be caught and returned as well. The final output is always a dict\n",
    "                with keys \"raw\", \"parsed\", and \"parsing_error\".\n",
    "\n",
    "        Returns:\n",
    "            A Runnable that takes any ChatModel input and returns as output:\n",
    "\n",
    "                If include_raw is True then a dict with keys:\n",
    "                    raw: BaseMessage\n",
    "                    parsed: Optional[_DictOrPydantic]\n",
    "                    parsing_error: Optional[BaseException]\n",
    "\n",
    "                If include_raw is False then just _DictOrPydantic is returned,\n",
    "                where _DictOrPydantic depends on the schema:\n",
    "\n",
    "                If schema is a Pydantic class then _DictOrPydantic is the Pydantic\n",
    "                    class.\n",
    "\n",
    "                If schema is a dict then _DictOrPydantic is a dict.\n",
    "\n",
    "        Example: Function-calling, Pydantic schema (method=\"function_calling\", include_raw=False):\n",
    "            .. code-block:: python\n",
    "\n",
    "                from langchain_groq import ChatGroq\n",
    "                from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "                class AnswerWithJustification(BaseModel):\n",
    "                    '''An answer to the user question along with justification for the answer.'''\n",
    "                    answer: str\n",
    "                    justification: str\n",
    "\n",
    "                llm = ChatGroq(temperature=0)\n",
    "                structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
    "                # -> AnswerWithJustification(\n",
    "                #     answer='A pound of bricks and a pound of feathers weigh the same.'\n",
    "                #     justification=\"Both a pound of bricks and a pound of feathers have been defined to have the same weight. The 'pound' is a unit of weight, so any two things that are described as weighing a pound will weigh the same.\"\n",
    "                # )\n",
    "\n",
    "        Example: Function-calling, Pydantic schema (method=\"function_calling\", include_raw=True):\n",
    "            .. code-block:: python\n",
    "\n",
    "                from langchain_groq import ChatGroq\n",
    "                from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "                class AnswerWithJustification(BaseModel):\n",
    "                    '''An answer to the user question along with justification for the answer.'''\n",
    "                    answer: str\n",
    "                    justification: str\n",
    "\n",
    "                llm = ChatGroq(temperature=0)\n",
    "                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n",
    "\n",
    "                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
    "                # -> {\n",
    "                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_01htjn3cspevxbqc1d7nkk8wab', 'function': {'arguments': '{\"answer\": \"A pound of bricks and a pound of feathers weigh the same.\", \"justification\": \"Both a pound of bricks and a pound of feathers have been defined to have the same weight. The \\'pound\\' is a unit of weight, so any two things that are described as weighing a pound will weigh the same.\", \"unit\": \"pounds\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}, id='run-456beee6-65f6-4e80-88af-a6065480822c-0'),\n",
    "                #     'parsed': AnswerWithJustification(answer='A pound of bricks and a pound of feathers weigh the same.', justification=\"Both a pound of bricks and a pound of feathers have been defined to have the same weight. The 'pound' is a unit of weight, so any two things that are described as weighing a pound will weigh the same.\"),\n",
    "                #     'parsing_error': None\n",
    "                # }\n",
    "\n",
    "        Example: Function-calling, dict schema (method=\"function_calling\", include_raw=False):\n",
    "            .. code-block:: python\n",
    "\n",
    "                from langchain_groq import ChatGroq\n",
    "                from langchain_core.pydantic_v1 import BaseModel\n",
    "                from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "                class AnswerWithJustification(BaseModel):\n",
    "                    '''An answer to the user question along with justification for the answer.'''\n",
    "                    answer: str\n",
    "                    justification: str\n",
    "\n",
    "                dict_schema = convert_to_openai_tool(AnswerWithJustification)\n",
    "                llm = ChatGroq(temperature=0)\n",
    "                structured_llm = llm.with_structured_output(dict_schema)\n",
    "\n",
    "                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
    "                # -> {\n",
    "                #     'answer': 'A pound of bricks and a pound of feathers weigh the same.',\n",
    "                #     'justification': \"Both a pound of bricks and a pound of feathers have been defined to have the same weight. The 'pound' is a unit of weight, so any two things that are described as weighing a pound will weigh the same.\", 'unit': 'pounds'}\n",
    "                # }\n",
    "\n",
    "        Example: JSON mode, Pydantic schema (method=\"json_mode\", include_raw=True):\n",
    "            .. code-block::\n",
    "\n",
    "                from langchain_groq import ChatGroq\n",
    "                from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "                class AnswerWithJustification(BaseModel):\n",
    "                    answer: str\n",
    "                    justification: str\n",
    "\n",
    "                llm = ChatGroq(temperature=0)\n",
    "                structured_llm = llm.with_structured_output(\n",
    "                    AnswerWithJustification,\n",
    "                    method=\"json_mode\",\n",
    "                    include_raw=True\n",
    "                )\n",
    "\n",
    "                structured_llm.invoke(\n",
    "                    \"Answer the following question. \"\n",
    "                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
    "                    \"What's heavier a pound of bricks or a pound of feathers?\"\n",
    "                )\n",
    "                # -> {\n",
    "                #     'raw': AIMessage(content='{\\n  \"answer\": \"A pound of bricks is the same weight as a pound of feathers.\",\\n  \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The material being weighed does not affect the weight, only the volume or number of items being weighed.\"\\n}', id='run-e5453bc5-5025-4833-95f9-4967bf6d5c4f-0'),\n",
    "                #     'parsed': AnswerWithJustification(answer='A pound of bricks is the same weight as a pound of feathers.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The material being weighed does not affect the weight, only the volume or number of items being weighed.'),\n",
    "                #     'parsing_error': None\n",
    "                # }\n",
    "\n",
    "        Example: JSON mode, no schema (schema=None, method=\"json_mode\", include_raw=True):\n",
    "            .. code-block::\n",
    "\n",
    "                from langchain_groq import ChatGroq\n",
    "\n",
    "                llm = ChatGroq(temperature=0)\n",
    "                structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n",
    "\n",
    "                structured_llm.invoke(\n",
    "                    \"Answer the following question. \"\n",
    "                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n\"\n",
    "                    \"What's heavier a pound of bricks or a pound of feathers?\"\n",
    "                )\n",
    "                # -> {\n",
    "                #     'raw': AIMessage(content='{\\n  \"answer\": \"A pound of bricks is the same weight as a pound of feathers.\",\\n  \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The material doesn\\'t change the weight, only the volume or space that the material takes up.\"\\n}', id='run-a4abbdb6-c20e-456f-bfff-da906a7e76b5-0'),\n",
    "                #     'parsed': {\n",
    "                #         'answer': 'A pound of bricks is the same weight as a pound of feathers.',\n",
    "                #         'justification': \"Both a pound of bricks and a pound of feathers weigh one pound. The material doesn't change the weight, only the volume or space that the material takes up.\"},\n",
    "                #     'parsing_error': None\n",
    "                # }\n",
    "\n",
    "\n",
    "        \"\"\"  # noqa: E501\n",
    "        if kwargs:\n",
    "            raise ValueError(f\"Received unsupported arguments {kwargs}\")\n",
    "        is_pydantic_schema = _is_pydantic_class(schema)\n",
    "        if method == \"function_calling\":\n",
    "            if schema is None:\n",
    "                raise ValueError(\n",
    "                    \"schema must be specified when method is 'function_calling'. \"\n",
    "                    \"Received None.\"\n",
    "                )\n",
    "            llm = self.bind_tools([schema], tool_choice=True)\n",
    "            if is_pydantic_schema:\n",
    "                output_parser: OutputParserLike = PydanticToolsParser(\n",
    "                    tools=[schema], first_tool_only=True\n",
    "                )\n",
    "            else:\n",
    "                key_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n",
    "                output_parser = JsonOutputKeyToolsParser(\n",
    "                    key_name=key_name, first_tool_only=True\n",
    "                )\n",
    "        elif method == \"json_mode\":\n",
    "            llm = self.bind(response_format={\"type\": \"json_object\"})\n",
    "            output_parser = (\n",
    "                PydanticOutputParser(pydantic_object=schema)\n",
    "                if is_pydantic_schema\n",
    "                else JsonOutputParser()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized method argument. Expected one of 'function_calling' or \"\n",
    "                f\"'json_format'. Received: '{method}'\"\n",
    "            )\n",
    "\n",
    "        if include_raw:\n",
    "            parser_assign = RunnablePassthrough.assign(\n",
    "                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n",
    "            )\n",
    "            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n",
    "            parser_with_fallback = parser_assign.with_fallbacks(\n",
    "                [parser_none], exception_key=\"parsing_error\"\n",
    "            )\n",
    "            return RunnableMap(raw=llm) | parser_with_fallback\n",
    "        else:\n",
    "            return llm | output_parser\n",
    "\n",
    "\n",
    "def _is_pydantic_class(obj: Any) -> bool:\n",
    "    return isinstance(obj, type) and issubclass(obj, BaseModel)\n",
    "\n",
    "\n",
    "class _FunctionCall(TypedDict):\n",
    "    name: str\n",
    "\n",
    "\n",
    "#\n",
    "# Type conversion helpers\n",
    "#\n",
    "def _convert_message_to_dict(message: BaseMessage) -> dict:\n",
    "    \"\"\"Convert a LangChain message to a dictionary.\n",
    "\n",
    "    Args:\n",
    "        message: The LangChain message.\n",
    "\n",
    "    Returns:\n",
    "        The dictionary.\n",
    "    \"\"\"\n",
    "    message_dict: Dict[str, Any]\n",
    "    if isinstance(message, ChatMessage):\n",
    "        message_dict = {\"role\": message.role, \"content\": message.content}\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        message_dict = {\"role\": \"user\", \"content\": message.content}\n",
    "    elif isinstance(message, AIMessage):\n",
    "        message_dict = {\"role\": \"assistant\", \"content\": message.content}\n",
    "        if \"function_call\" in message.additional_kwargs:\n",
    "            message_dict[\"function_call\"] = message.additional_kwargs[\"function_call\"]\n",
    "            # If function call only, content is None not empty string\n",
    "            if message_dict[\"content\"] == \"\":\n",
    "                message_dict[\"content\"] = None\n",
    "        if \"tool_calls\" in message.additional_kwargs:\n",
    "            message_dict[\"tool_calls\"] = message.additional_kwargs[\"tool_calls\"]\n",
    "            # If tool calls only, content is None not empty string\n",
    "            if message_dict[\"content\"] == \"\":\n",
    "                message_dict[\"content\"] = None\n",
    "    elif isinstance(message, SystemMessage):\n",
    "        message_dict = {\"role\": \"system\", \"content\": message.content}\n",
    "    elif isinstance(message, FunctionMessage):\n",
    "        message_dict = {\n",
    "            \"role\": \"function\",\n",
    "            \"content\": message.content,\n",
    "            \"name\": message.name,\n",
    "        }\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        message_dict = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": message.content,\n",
    "            \"tool_call_id\": message.tool_call_id,\n",
    "        }\n",
    "    else:\n",
    "        raise TypeError(f\"Got unknown type {message}\")\n",
    "    if \"name\" in message.additional_kwargs:\n",
    "        message_dict[\"name\"] = message.additional_kwargs[\"name\"]\n",
    "    return message_dict\n",
    "\n",
    "\n",
    "def _convert_delta_to_message_chunk(\n",
    "    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]\n",
    ") -> BaseMessageChunk:\n",
    "    role = cast(str, _dict.get(\"role\"))\n",
    "    content = cast(str, _dict.get(\"content\") or \"\")\n",
    "    additional_kwargs: Dict = {}\n",
    "    if _dict.get(\"function_call\"):\n",
    "        function_call = dict(_dict[\"function_call\"])\n",
    "        if \"name\" in function_call and function_call[\"name\"] is None:\n",
    "            function_call[\"name\"] = \"\"\n",
    "        additional_kwargs[\"function_call\"] = function_call\n",
    "    if _dict.get(\"tool_calls\"):\n",
    "        additional_kwargs[\"tool_calls\"] = _dict[\"tool_calls\"]\n",
    "\n",
    "    if role == \"user\" or default_class == HumanMessageChunk:\n",
    "        return HumanMessageChunk(content=content)\n",
    "    elif role == \"assistant\" or default_class == AIMessageChunk:\n",
    "        return AIMessageChunk(content=content, additional_kwargs=additional_kwargs)\n",
    "    elif role == \"system\" or default_class == SystemMessageChunk:\n",
    "        return SystemMessageChunk(content=content)\n",
    "    elif role == \"function\" or default_class == FunctionMessageChunk:\n",
    "        return FunctionMessageChunk(content=content, name=_dict[\"name\"])\n",
    "    elif role == \"tool\" or default_class == ToolMessageChunk:\n",
    "        return ToolMessageChunk(content=content, tool_call_id=_dict[\"tool_call_id\"])\n",
    "    elif role or default_class == ChatMessageChunk:\n",
    "        return ChatMessageChunk(content=content, role=role)\n",
    "    else:\n",
    "        return default_class(content=content)  # type: ignore\n",
    "\n",
    "\n",
    "def _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n",
    "    \"\"\"Convert a dictionary to a LangChain message.\n",
    "\n",
    "    Args:\n",
    "        _dict: The dictionary.\n",
    "\n",
    "    Returns:\n",
    "        The LangChain message.\n",
    "    \"\"\"\n",
    "    id_ = _dict.get(\"id\")\n",
    "    role = _dict.get(\"role\")\n",
    "    if role == \"user\":\n",
    "        return HumanMessage(content=_dict.get(\"content\", \"\"))\n",
    "    elif role == \"assistant\":\n",
    "        content = _dict.get(\"content\", \"\") or \"\"\n",
    "        additional_kwargs: Dict = {}\n",
    "        if function_call := _dict.get(\"function_call\"):\n",
    "            additional_kwargs[\"function_call\"] = dict(function_call)\n",
    "        tool_calls = []\n",
    "        invalid_tool_calls = []\n",
    "        if raw_tool_calls := _dict.get(\"tool_calls\"):\n",
    "            additional_kwargs[\"tool_calls\"] = raw_tool_calls\n",
    "            for raw_tool_call in raw_tool_calls:\n",
    "                try:\n",
    "                    tool_calls.append(parse_tool_call(raw_tool_call, return_id=True))\n",
    "                except Exception as e:\n",
    "                    invalid_tool_calls.append(\n",
    "                        make_invalid_tool_call(raw_tool_call, str(e))\n",
    "                    )\n",
    "        return AIMessage(\n",
    "            content=content,\n",
    "            id=id_,\n",
    "            additional_kwargs=additional_kwargs,\n",
    "            tool_calls=tool_calls,\n",
    "            invalid_tool_calls=invalid_tool_calls,\n",
    "        )\n",
    "    elif role == \"system\":\n",
    "        return SystemMessage(content=_dict.get(\"content\", \"\"))\n",
    "    elif role == \"function\":\n",
    "        return FunctionMessage(content=_dict.get(\"content\", \"\"), name=_dict.get(\"name\"))\n",
    "    elif role == \"tool\":\n",
    "        additional_kwargs = {}\n",
    "        if \"name\" in _dict:\n",
    "            additional_kwargs[\"name\"] = _dict[\"name\"]\n",
    "        return ToolMessage(\n",
    "            content=_dict.get(\"content\", \"\"),\n",
    "            tool_call_id=_dict.get(\"tool_call_id\"),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        return ChatMessage(content=_dict.get(\"content\", \"\"), role=role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "  \"input\": {\n",
    "    \"input\": \"good morning\"\n",
    "  },\n",
    "  \"plan\": [\n",
    "    {\n",
    "      \"key\": \"General_conv\",\n",
    "      \"value\": \"respond with 'good morning' to user\"\n",
    "    }\n",
    "  ],\n",
    "  \"past_steps\": [\n",
    "    \"respond with 'good morning' to user\",\n",
    "    \" Good morning there! I hope this new day brings you joy and positivity. Is there anything specific I can help you with today?\"\n",
    "  ],\n",
    "  \"response\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing_extensions import List\n",
    "from typing import Literal\n",
    "\n",
    "class Step(BaseModel):\n",
    "    key: Literal[\"Food_crew\", \"General_conv\", \"General_other\", \"Mediwave_rag\", \"Travel_crew\"] = Field(description='the worker gonna handle this task/step')\n",
    "    value: str = Field(description='task/ step the worker need to do')\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[Step] \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from plan_and_execute.planner import crews\n",
    "import os\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "\n",
    "set_verbose=True \n",
    "set_debug=True\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "# response = {'name': 'Response',\n",
    "#  'description': 'Response to user.',\n",
    "#  'parameters': {'type': 'object',\n",
    "#   'properties': {'response': {'type': 'string'}},\n",
    "#   'required': ['response']}}\n",
    "\n",
    "\n",
    "# function1 = {'name': 'plan',\n",
    "#  'description': 'replanner',\n",
    " \n",
    "#  'parameters': {\n",
    "#    'type': 'array',\n",
    "#    'properties': {\n",
    "#      'key': {\n",
    "#        \"enum\": f\"{crews}\",\n",
    "#        'description': 'the worker gonna handle this task/step'\n",
    "       \n",
    "# ,\n",
    "        \n",
    "#        'type': 'string'},\n",
    "#      'value': {\n",
    "#        'description': 'task/ step the worker need to do',\n",
    "#       'type': 'string'}\n",
    "#      },\n",
    "                    \n",
    "#     'required': ['plan'],\n",
    "#   }\n",
    "# }\n",
    "\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "   \"\"\"\n",
    "you are a planning expert following are the user input, and the plan as well as the performed steps and it's outcomes you're job is to analyse the currently executed steps and its outcome to summarize the final answer to the user question, while providing response make sure the user input is satisfied with the response refer the follow steps to gather the necessary informations for the final response..\n",
    "\n",
    "user input was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "\\n incase you need more information/ execute all steps in the plan to summarize the final answer then do the following.\n",
    "\n",
    "Update your plan accordingly(remove the completed step). If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done and don't create unnecessary new steps that are not in the original plan. Do not return previously done steps as part of the plan.\n",
    "\n",
    "only provide the final answer after make sure the user requirement has been satisfied completely.\n",
    "\n",
    "don't use ```json``` while giving the response\n",
    "\n",
    "make sure the tool name is either 'Plan' for replan or 'Response' for final answer.\n",
    "\n",
    "don't make unneccesary steps unrelavant to the original user input.. remove the satisfied step in the given plan. \n",
    "\n",
    "don't add any notes to the output.\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatGroq(\n",
    "  model=os.environ['LLM'],\n",
    "  api_key=os.environ['GROQ_API_KEY']\n",
    ")\n",
    "\n",
    "parser = PydanticToolsParser(tools=[Plan, Response], first_tool_only=True)\n",
    "\n",
    "llm_func = llm.bind_tools([Plan, Response])\n",
    "\n",
    "replanner = replanner_prompt | llm_func | parser\n",
    "\n",
    "\n",
    "# replanner = create_openai_fn_runnable(\n",
    "    \n",
    "#     [Plan, Response],\n",
    "#     llm,\n",
    "#     replanner_prompt,\n",
    "# ).with_retry(\n",
    "#   retry_if_exception_type=(ValueError,KeyError),\n",
    "#   stop_after_attempt=4,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_state_ = state.copy()\n",
    "\n",
    "plan_step = []\n",
    "\n",
    "for item in state['plan']:\n",
    "    plan_step.append(item['value'])\n",
    "\n",
    "super_state_ |= {'plan': plan_step}\n",
    "\n",
    "output =replanner.invoke(super_state_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good morning there! I hope this new day brings you joy and positivity. Is there anything specific I can help you with today?'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(output, Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Good morning there! I hope this new day brings you joy and positivity. Is there anything specific I can help you with today?')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output.tool_calls[0].get('args')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AIMessage' object has no attribute 'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AIMessage' object has no attribute 'response'"
     ]
    }
   ],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(output, Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "\n",
    "PydanticToolsParser(strict=True, tools=[Plan, Response])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
